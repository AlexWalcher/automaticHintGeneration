{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "cxaEE5DFfr_v",
        "WckjgVs-fZto",
        "s5rwSDisuN9s",
        "du5Un8VufnlG",
        "_lhmuuUNQ__8",
        "0Yf6Fi_34ye5",
        "0LORwHLog-t_"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexWalcher/optimizationOfHintGeneration/blob/Test/optimizationOfHintGeneration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5rwSDisuN9s"
      },
      "source": [
        "# All imports"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "%rm -r optimizationOfHintGeneration\n",
        "!git clone https://github.com/AlexWalcher/optimizationOfHintGeneration.git\n",
        "%cd /content/"
      ],
      "metadata": {
        "id": "b_jGbTrbz8x2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24922100-6068-4788-91e1-e65cd3498c29"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "rm: cannot remove 'optimizationOfHintGeneration': No such file or directory\n",
            "Cloning into 'optimizationOfHintGeneration'...\n",
            "remote: Enumerating objects: 65, done.\u001b[K\n",
            "remote: Counting objects: 100% (65/65), done.\u001b[K\n",
            "remote: Compressing objects: 100% (63/63), done.\u001b[K\n",
            "remote: Total 65 (delta 33), reused 6 (delta 1), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (65/65), 6.96 MiB | 4.49 MiB/s, done.\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install selenium\n",
        "!apt-get update \n",
        "!apt-get install firefox\n",
        "!apt install firefox-geckodriver"
      ],
      "metadata": {
        "id": "98Zh7IC7mSWI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a84f8c74-06c9-4100-ea21-9764fb8736dc"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: selenium in /usr/local/lib/python3.9/dist-packages (4.8.3)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.9/dist-packages (from selenium) (0.10.2)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.9/dist-packages (from selenium) (2022.12.7)\n",
            "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.9/dist-packages (from selenium) (0.22.0)\n",
            "Requirement already satisfied: urllib3[socks]~=1.26 in /usr/local/lib/python3.9/dist-packages (from selenium) (1.26.15)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in /usr/local/lib/python3.9/dist-packages (from trio~=0.17->selenium) (1.1.1)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.9/dist-packages (from trio~=0.17->selenium) (3.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.9/dist-packages (from trio~=0.17->selenium) (1.3.0)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.9/dist-packages (from trio~=0.17->selenium) (1.2.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.9/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.9/dist-packages (from trio~=0.17->selenium) (22.2.0)\n",
            "Requirement already satisfied: async-generator>=1.9 in /usr/local/lib/python3.9/dist-packages (from trio~=0.17->selenium) (1.10)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.9/dist-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
            "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.9/dist-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.9/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
            "Get:1 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "Hit:3 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease\n",
            "Get:4 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
            "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
            "Hit:6 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n",
            "Get:7 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
            "Hit:8 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "Hit:9 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
            "Hit:10 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "Hit:11 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3,075 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1,324 kB]\n",
            "Fetched 4,736 kB in 2s (2,345 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "firefox is already the newest version (112.0+build2-0ubuntu0.20.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 31 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "firefox-geckodriver is already the newest version (112.0+build2-0ubuntu0.20.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 31 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__DepwTLww00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d3c79e8-2031-4542-f448-d60f029d7385"
      },
      "source": [
        "!pip install sparqlwrapper"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sparqlwrapper in /usr/local/lib/python3.9/dist-packages (2.0.0)\n",
            "Requirement already satisfied: rdflib>=6.1.1 in /usr/local/lib/python3.9/dist-packages (from sparqlwrapper) (6.3.2)\n",
            "Requirement already satisfied: isodate<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from rdflib>=6.1.1->sparqlwrapper) (0.6.1)\n",
            "Requirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.9/dist-packages (from rdflib>=6.1.1->sparqlwrapper) (3.0.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from isodate<0.7.0,>=0.6.0->rdflib>=6.1.1->sparqlwrapper) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pageviewapi"
      ],
      "metadata": {
        "id": "66w0wd3eu-_z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cff5aeeb-32c8-4153-aaed-fad1f39f6d22"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pageviewapi in /usr/local/lib/python3.9/dist-packages (0.4.0)\n",
            "Requirement already satisfied: attrdict in /usr/local/lib/python3.9/dist-packages (from pageviewapi) (2.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from pageviewapi) (2.27.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from attrdict->pageviewapi) (1.16.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->pageviewapi) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->pageviewapi) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->pageviewapi) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->pageviewapi) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "id": "LMeyegyfgdd_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f624b953-02ce-4ef0-f951-0ba4e51ac9ff"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.9/dist-packages (2.2.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (0.13.4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (0.1.98)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (1.10.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (2.0.0+cu118)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (3.8.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (4.27.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (1.22.4)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (0.15.1+cu118)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.27.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.11.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence-transformers) (1.11.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence-transformers) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (16.0.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.10.31)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk->sentence-transformers) (8.1.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->sentence-transformers) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.2)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.12.7)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia"
      ],
      "metadata": {
        "id": "_KjIV0guhBIT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebec9f85-cd14-42c4-fb00-5153d8ed7a2b"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wikipedia in /usr/local/lib/python3.9/dist-packages (1.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from wikipedia) (2.27.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from wikipedia) (4.11.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.26.15)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4->wikipedia) (2.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COpv1aim8jLJ"
      },
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth',1000)"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7j16mqC9L83"
      },
      "source": [
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "\n",
        "sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests"
      ],
      "metadata": {
        "id": "n07hKh23g9dG"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4\n",
        "from bs4 import BeautifulSoup, NavigableString, Tag"
      ],
      "metadata": {
        "id": "PyfC4F2Ya6sh"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "pQIQvzqoaqbS"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFQ0BQ8dnv50"
      },
      "source": [
        "import wikipedia"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random"
      ],
      "metadata": {
        "id": "-6NVlM8dhEdI"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3KDOw-yx4JR"
      },
      "source": [
        "import pageviewapi"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QT41X2h85r7j"
      },
      "source": [
        "#!pip install -U pip setuptools wheel\n",
        "#!pip install -U spacy\n",
        "#!python -m spacy download en_core_web_sm\n",
        "#!python -m spacy link en_core_web_sm en\n",
        "import spacy"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#new imports\n",
        "\n",
        "import time\n",
        "import requests\n",
        "import re\n",
        "import difflib\n",
        "\n",
        "from selenium import webdriver\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "chGS_8doxwwE"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load File"
      ],
      "metadata": {
        "id": "WckjgVs-fZto"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_K8U-ZpsRss_"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.ExcelFile(\"./optimizationOfHintGeneration/testSet.xlsx\").parse(\"Sheet1\")\n",
        "x = []\n",
        "x.append(df[\"Answer\"])\n",
        "\n",
        "dataPerson = []\n",
        "dataYear = []\n",
        "dataLocation = []\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "  if(row[\"Category\"] == \"Person\"):\n",
        "    dataPerson.append([row[\"Question\"], row[\"Answer\"]])\n",
        "  elif(row[\"Category\"] == \"Year\"):\n",
        "    dataYear.append([row[\"Question\"], row[\"Answer\"]])\n",
        "  elif(row[\"Category\"] == \"Location\"):\n",
        "    dataLocation.append([row[\"Question\"], row[\"Answer\"]])\n",
        "\n",
        "person_df = pd.DataFrame(dataPerson, columns=[\"Question\", \"Answer\"])\n",
        "year_df = pd.DataFrame(dataYear, columns=[\"Question\", \"Answer\"])\n",
        "location_df = pd.DataFrame(dataLocation, columns=[\"Question\", \"Answer\"])"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **New prediction method for years**"
      ],
      "metadata": {
        "id": "mKh1YgDQt3BC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dictionary for thumbcaption part of a year\n",
        "\n"
      ],
      "metadata": {
        "id": "oq8Zj9_w-aS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import time\n",
        "#import requests\n",
        "#import re\n",
        "#import difflib\n",
        "\n",
        "#from selenium import webdriver\n",
        "#from bs4 import BeautifulSoup\n",
        "\n",
        "def get_table_info(url):\n",
        "    \"\"\"\n",
        "    Given a URL, this function opens the url and retrieves the information stored in a table.\n",
        "    \"\"\"\n",
        "    options = webdriver.FirefoxOptions()\n",
        "    #options.headless = True\n",
        "    options.add_argument('--headless')\n",
        "\n",
        "    driver = webdriver.Firefox(options=options)\n",
        "    driver.get(url)\n",
        "    time.sleep(5) # Wait for the page to load completely\n",
        "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "    driver.quit()\n",
        "    table = soup.find('table')\n",
        "    rows = table.find_all('tr')\n",
        "    headers = [header.text.strip() for header in rows[0].find_all('th')]\n",
        "    data = []\n",
        "    for row in rows[1:]:\n",
        "        data.append([cell.text.strip() for cell in row.find_all('td')])\n",
        "    return (headers, data)\n",
        "\n",
        "def get_table_info_requests(url):\n",
        "    \"\"\"\n",
        "    Given a URL, this function opens the url and retrieves the information stored in a table.\n",
        "    \"\"\"\n",
        "    headers = []\n",
        "    data = []\n",
        "\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    table = soup.find('table')\n",
        "    rows = table.find_all('tr')\n",
        "    headers = [header.text.strip() for header in rows[0].find_all('th')]\n",
        "    data = []\n",
        "    for row in rows[1:]:\n",
        "        data.append([cell.text.strip() for cell in row.find_all('td')])\n",
        "    return (headers, data)\n",
        "\n",
        "\n",
        "def get_links_in_section(wikipedia_url, section_heading):\n",
        "    \"\"\"\n",
        "    Given a Wikipedia URL, and a section of this article, returns an array of dictionaries containing the href, title, and description of each link on that section of the page.\n",
        "    \"\"\"\n",
        "    response = requests.get(wikipedia_url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    section = soup.find('span', {'id': section_heading})\n",
        "    if section is None:\n",
        "        return None\n",
        "    section_content = section.parent.find_next_sibling('ul')\n",
        "    if section_content is None:\n",
        "        return None\n",
        "    links = []\n",
        "    for item in section_content.find_all('li'):\n",
        "        link = item.find('a')\n",
        "        if link is not None and link.has_attr('href') and link['href'].startswith('/wiki/'):\n",
        "            title = link.get('title', '')\n",
        "            description = item.text.strip()\n",
        "            url = f\"https://en.wikipedia.org{link['href']}\"\n",
        "            links.append({'title': title, 'description': description, 'url': url})\n",
        "    print(links)\n",
        "    return links\n",
        "\n",
        "\n",
        "def get_links_in_section_with_sublinks(wikipedia_url, section_title):\n",
        "    \"\"\"\n",
        "    Given a Wikipedia URL, and a section of this article, returns an array of dictionaries containing the href, title, and description of each link  with its sublinks as well.\n",
        "    \"\"\"\n",
        "    response = requests.get(wikipedia_url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        section_heading = soup.find('span', {'id': section_title})\n",
        "        if section_heading is None:\n",
        "            return None\n",
        "        section = section_heading.parent\n",
        "        section_content = section.find_next_sibling('ul')\n",
        "        if section_content is None:\n",
        "            return None\n",
        "        links = []\n",
        "        for item in section_content.find_all('li'):\n",
        "            link = item.find('a')\n",
        "            if link is not None:\n",
        "                link_title = link.get('title')\n",
        "                link_url = 'https://en.wikipedia.org' + link.get('href')\n",
        "                link_description = item.text.replace(link_title, '').strip()\n",
        "                links.append({'title': link_title, 'url': link_url, 'description': link_description})\n",
        "                for sublink in item.find_all('a', href=True, recursive=False):\n",
        "                    sublink_title = sublink.get('title')\n",
        "                    sublink_url = 'https://en.wikipedia.org' + sublink.get('href')\n",
        "                    sublink_description = sublink.parent.text.replace(sublink_title, '').replace(link_title, '').strip()\n",
        "                    links.append({'title': sublink_title, 'url': sublink_url, 'description': sublink_description})\n",
        "        return links\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\"\"\"\n",
        "Returns all backlinks located in the thumbcaption section\n",
        "\"\"\"\n",
        "def get_wikipedia_backlinks_thumbcaption(url):\n",
        "    # Load the Wikipedia page HTML\n",
        "    page_html = requests.get(url).text\n",
        "    soup = BeautifulSoup(page_html, 'html.parser')\n",
        "\n",
        "    # Find the image caption on the page\n",
        "    caption = soup.find('div', class_='thumbcaption')\n",
        "\n",
        "    if caption is not None:\n",
        "      # Extract the caption text and any backlinks\n",
        "      backlink_sentences = {}\n",
        "      sentences = caption.text.strip().split('.')\n",
        "      for sentence in sentences:\n",
        "        links = caption.find_all('a', href=True, string=re.compile(sentence))\n",
        "        if len(links) > 0:\n",
        "            backlinks = []\n",
        "            for link in links:\n",
        "                backlink_url = 'https://en.wikipedia.org' + link['href']\n",
        "                backlink_title = link.get('title', '')\n",
        "                backlinks.append((backlink_url, backlink_title))\n",
        "            backlink_sentences[sentence] = backlinks\n",
        "    return backlink_sentences\n",
        "\n",
        "\"\"\"\n",
        "This function correctly prunes the links to only include the string after the last / character\n",
        "\"\"\"\n",
        "def prune_links(links):\n",
        "    pruned_links = []\n",
        "    for url, title in links:\n",
        "        pruned_url = url.split('/')[-1]\n",
        "        pruned_links.append((pruned_url, title))\n",
        "    return pruned_links\n",
        "\n",
        "\"\"\"\n",
        "This function first initializes an empty list combined_list that will hold the combined strings. Then, it uses a for loop to iterate through the input list in increments of 10 tuples at a time.\n",
        "For each sub-list of up to 10 tuples, it uses the list comprehension and join() method as before to combine the first elements of each tuple into a single string separated by '|'. \n",
        "Finally, the function appends each combined string to the combined_list and returns it at the end.\n",
        "\"\"\"\n",
        "def combine_first_elements(my_list):\n",
        "    combined_list = []\n",
        "    num_tuples = len(my_list)\n",
        "    for i in range(0, num_tuples, 10):\n",
        "        sub_list = my_list[i:i+10]\n",
        "        combined_str = '|'.join([tup[0] for tup in sub_list])\n",
        "        combined_list.append(combined_str)\n",
        "    return combined_list\n",
        "\n",
        "\"\"\"\n",
        "This function first initializes an empty list url_list that will hold the modified URLs. Then, it uses a for loop to iterate through each combined string in the input list. \n",
        "For each combined string, it concatenates the base URL with a forward slash and the combined string, and appends the resulting URL to the url_list. Finally, the function returns the url_list at the end.\n",
        "\"\"\"\n",
        "def add_combined_strings_to_url(base_url, combined_strings):\n",
        "    url_list = []\n",
        "    for string in combined_strings:\n",
        "        url_list.append(base_url + '/' + string)\n",
        "    return url_list\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "This will output a list of all the sentences in the thumbcaption, split by ';'.\n",
        "\"\"\"\n",
        "def get_thumbcaption_sentences(url):\n",
        "    # Get the HTML content of the page\n",
        "    page = requests.get(url)\n",
        "    soup = BeautifulSoup(page.content, 'html.parser')\n",
        "\n",
        "    # Find the thumbcaption element\n",
        "    thumbcaption = soup.find('div', class_='thumbcaption')\n",
        "\n",
        "    # Get all the sentences in the thumbcaption\n",
        "    sentences = thumbcaption.text.split('; ')\n",
        "\n",
        "    return sentences\n",
        "\n",
        "\"\"\"\n",
        "This function converts a list into a dict in the way that is needed.\n",
        "\"\"\"\n",
        "def list_to_dict(lst):\n",
        "    result = {}\n",
        "    for sublist in lst:\n",
        "        if len(sublist) >= 4:\n",
        "            key = sublist[1]\n",
        "            value = int(sublist[3].replace(',', ''))\n",
        "            result[key] = value\n",
        "    return result\n",
        "\n",
        "\"\"\"\n",
        "This function takes a list of links, opens each link to get the data, discards the header and converts the data into a dictionary using the list_to_dict() function, \n",
        "and then updates a combined dictionary with the resulting dictionary from each link. Finally, it returns the combined dictionary.\n",
        "\"\"\"\n",
        "def combine_dicts_from_links(link_list):\n",
        "    combined_dict = {}\n",
        "    for link in link_list:\n",
        "        header, data = get_table_info(link)\n",
        "        #print(data)\n",
        "        link_dict = list_to_dict(data)\n",
        "        combined_dict.update(link_dict)\n",
        "    return combined_dict\n",
        "\n",
        "\"\"\"\n",
        "This sorts the items in the dictionary based on the integer value of the second element in each key-value tuple (i.e. item[1]), in descending order (reverse=True).\n",
        "\"\"\"\n",
        "def sort_dict_desc(d):\n",
        "    return {k: v for k, v in sorted(d.items(), key=lambda item: int(item[1]), reverse=True)}\n",
        "\n",
        "\"\"\"\n",
        "This function takes in the ord dictionary and the sentences list as arguments.\n",
        "It initializes an empty list called result that we will append the found sentences to. It then iterates over each key in the ord dictionary and for each key, it iterates over each sentence in the sentences list. \n",
        "If the key is found in the sentence, the sentence is appended to the result list\n",
        "\"\"\"\n",
        "def find_sentences(ord, sentences):\n",
        "    result = []\n",
        "    for key in ord:\n",
        "        for sentence in sentences:\n",
        "            if key in sentence:\n",
        "                result.append(sentence)\n",
        "    return result\n",
        "\n",
        "\"\"\"\n",
        "This function takes a list of sentences and a keyword, removes the keyword from each sentence in the list, and returns the updated list.\n",
        "\"\"\"\n",
        "def remove_keyword(sentences, keyword):\n",
        "    updated_sentences = []\n",
        "    for sentence in sentences:\n",
        "        updated_sentence = sentence.replace(keyword, \"\")\n",
        "        updated_sentences.append(updated_sentence)\n",
        "    return updated_sentences\n",
        "\n",
        "\"\"\"\n",
        "This function takes a list of sentences and a list of keyword, removes the keyword from each sentence in the list, and returns the updated list. MAYBE NOT WORKING\n",
        "\"\"\"\n",
        "def remove_keywords(sentences, keywords):\n",
        "    \"\"\"\n",
        "    Removes one or more keywords from each sentence in the list of sentences.\n",
        "    \"\"\"\n",
        "    result = sentences\n",
        "    for entry in keywords:\n",
        "        result = remove_keyword(result, entry)\n",
        "    \n",
        "    return result\n",
        "\"\"\"\n",
        "This function prepends a given string to each sentence in a list.\n",
        "\"\"\"\n",
        "def prepend_string(sentences, prepend_str):\n",
        "    new_sentences = []\n",
        "    for sentence in sentences:\n",
        "        new_sentence = f\"{prepend_str}{sentence}\"\n",
        "        new_sentences.append(new_sentence)\n",
        "    return new_sentences\n",
        "\n",
        "\"\"\"\n",
        "This function creates a new list new_sentences and loops over each sentence in the input sentences list. \n",
        "It then uses a list comprehension and the difflib.SequenceMatcher class to compare the sentence to each sentence already in new_sentences. \n",
        "If the ratio of similarity between the two sentences is greater than 0.8 (adjust this threshold as needed), it considers the sentence to be similar and skips it. Otherwise, it adds the sentence to new_sentences. \n",
        "Finally, it returns the new list of unique sentences.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def remove_similar(sentences):\n",
        "    new_sentences = []\n",
        "    for sentence in sentences:\n",
        "        if not any(difflib.SequenceMatcher(None, sentence, s).ratio() > 0.8 for s in new_sentences):\n",
        "            new_sentences.append(sentence)\n",
        "    return new_sentences\n"
      ],
      "metadata": {
        "id": "wzL3elbnOlYl"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#IMPORTANT\n",
        "def thumbcaption_hints_per_year(years_list):\n",
        "  thumbcaption_hints = {}\n",
        "  wiki_base_link= 'https://en.wikipedia.org/wiki/'\n",
        "  pageviews_range_url = 'https://pageviews.wmcloud.org/?project=en.wikipedia.org&platform=all-access&agent=user&redirects=0&range=all-time&pages='\n",
        "  \n",
        "  for y in years_list:\n",
        "    years_key = str(y)\n",
        "    test_link = wiki_base_link + years_key\n",
        "    sentences_of_thumbcaption = get_thumbcaption_sentences(test_link) #just gets the sentences from the thumbcaption section of a wikipedi years page \n",
        "    thumbcaption = get_wikipedia_backlinks_thumbcaption(test_link) #get all backlinks of the thumbcapture of the year (those are the most known events)\n",
        "    thumbcaption_key = next(iter(thumbcaption))\n",
        "    thumbcaption_val = thumbcaption[thumbcaption_key]\n",
        "    pruned = prune_links(thumbcaption_val) #prune those backlinks such that only the important part remains\n",
        "    com = combine_first_elements(pruned) #combine up to 10 of these links to create a request to pageview\n",
        "    url_list = add_combined_strings_to_url(pageviews_range_url, com) #now we have a list of pageview links with all the backlinks of the thumbcaption part of the wiki page\n",
        "    data=combine_dicts_from_links(url_list) #now we called the links and retreieved the pageviews; saved them as a dictionary\n",
        "    ord = sort_dict_desc(data) #now the list is ordered in ascending order\n",
        "    tmp = find_sentences(ord,sentences_of_thumbcaption) #search the corresponding sentence to the keyword (USA and school shooting for example)\n",
        "    #remove the years number from the hints OBVIOUSNESS\n",
        "    keywords_list = [years_key, 'clockwise ', 'Clockwise ', 'From top left', 'from top-left', 'from top left', 'From top-left', 'from top-left: ', ':', 'from left, clockwise'] #list of keywords that should be removed from the sentences\n",
        "    t4=remove_keywords(tmp, keywords_list)\n",
        "    prepend_str = 'In the same year, '\n",
        "    hints = prepend_string(t4, prepend_str)\n",
        "\n",
        "    final_hints = remove_similar(hints) #before adjusting the sentences\n",
        "    thumbcaption_hints[y] = final_hints\n",
        "\n",
        "  return thumbcaption_hints\n",
        "\n",
        "def get_year_thumbcaption_hints():\n",
        "  file_years_list = []\n",
        "  for index, row in year_df.iterrows():\n",
        "    file_years_list.append(row[\"Answer\"])\n",
        "\n",
        "  pop_thumb_hints = thumbcaption_hints_per_year(file_years_list)\n",
        "  return pop_thumb_hints\n",
        "  #import pprint\n",
        "  #pprint.pprint(pop_thumb_hints, indent=4)\n",
        "\n",
        "#get_year_thumbcaption_hints()"
      ],
      "metadata": {
        "id": "cvcHIpCheNpL"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dictionary for popular sports events of a year"
      ],
      "metadata": {
        "id": "O37okmzfPCcM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import requests\n",
        "#from bs4 import BeautifulSoup\n",
        "\n",
        "def get_all_tables(url):\n",
        "    \"\"\"\n",
        "    Given a URL, this function opens the url and retrieves all tables on the page.\n",
        "    \"\"\"\n",
        "    options = webdriver.FirefoxOptions()\n",
        "    #options.headless = True\n",
        "    options.add_argument('--headless')\n",
        "\n",
        "\n",
        "    driver = webdriver.Firefox(options=options)\n",
        "    driver.get(url)\n",
        "    time.sleep(1) # Wait for the page to load completely\n",
        "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "    driver.quit()\n",
        "\n",
        "    tables = soup.find_all('table')\n",
        "    all_tables = []\n",
        "    for table in tables:\n",
        "        rows = table.find_all('tr')\n",
        "        headers = [header.text.strip() for header in rows[0].find_all('th')]\n",
        "        data = []\n",
        "        for row in rows[1:]:\n",
        "            data.append([cell.text.strip() for cell in row.find_all('td')])\n",
        "        all_tables.append({'headers': headers, 'data': data})\n",
        "\n",
        "    return all_tables\n",
        "\n",
        "def get_first_elements(data_dict):\n",
        "    \"\"\"\n",
        "    Extracts the first element from each sublist in the 'data' list of lists in a dictionary and returns them in a separate list.\n",
        "    \n",
        "    Args:\n",
        "    - data_dict (dict): A dictionary containing a 'data' key with a list of lists as its value.\n",
        "    \n",
        "    Returns:\n",
        "    - A list containing the first element of each sublist in the 'data' list of lists.\n",
        "    \"\"\"\n",
        "    data_list = data_dict['data']\n",
        "    result = []\n",
        "    for sublist in data_list:\n",
        "        if len(sublist) > 0:\n",
        "            if len(sublist[0]) > 0:\n",
        "              result.append(sublist[0])\n",
        "    return result\n",
        "\n",
        "#prune the list of the hole wiki page down to the important table\n",
        "def prune_dict_list(dict_list, keyw):\n",
        "    for d in dict_list:\n",
        "        if 'headers' in d and d['headers'] == [keyw]:\n",
        "            return d\n",
        "    return None  # if no dict with the desired key-value pair is found\n",
        "\n",
        "#replace the \\n and create a list of lists \n",
        "def create_list_from_list_of_lists_key(lst, keyw):\n",
        "    result = []\n",
        "    for sublst in lst:\n",
        "        if sublst:\n",
        "          new_test = [item.replace(keyw, '') for item in sublst[0].split(keyw)]\n",
        "          result.append(new_test)\n",
        "    return result\n",
        "\n",
        "#replace the : and create a dict; {'year': 'CL-winner'}\n",
        "def create_dict_from_list_of_lists(lst):\n",
        "    result_dict = {}\n",
        "    for sublist in lst:\n",
        "        for item in sublist:\n",
        "            if \":\" in item:\n",
        "                key, value = item.split(\":\")\n",
        "                key = key.strip().replace(\"–\", \"-\")\n",
        "                value = value.strip()\n",
        "                result_dict[key] = value\n",
        "            else:\n",
        "                continue\n",
        "    return result_dict\n",
        "\n",
        "#split on the : and creates a dict\n",
        "def create_dict_from_list(lst):\n",
        "    result = {}\n",
        "    for item in lst:\n",
        "        parts = item.split(\":\")\n",
        "       # parts = parts.strip().replace(\":\", \"\")\n",
        "        result[parts[0]] = parts[1]\n",
        "    return result\n",
        "\n",
        "#splits on [ and creates a dict 1950: 'Farina'\n",
        "def get_year_with_driver(race_results):\n",
        "    results_dict = {}\n",
        "    for result in race_results:\n",
        "        if result:\n",
        "            results_dict[result[0].split(\"[\")[0]] = result[1]\n",
        "    return results_dict\n",
        "\n",
        "#get rid of the links ('Alberto Ascari[20]' => 'Alberto Ascari')\n",
        "def clean_driver_names(results_dict):\n",
        "    for year, driver in results_dict.items():\n",
        "        results_dict[year] = driver.split('[')[0].strip()\n",
        "    return results_dict\n",
        "\n",
        "#deletes the : from the key\n",
        "def clean_dict_keys(dict_to_clean):\n",
        "    cleaned_dict = {}\n",
        "    for key, value in dict_to_clean.items():\n",
        "        cleaned_dict[key.rstrip(':')] = value\n",
        "    return cleaned_dict\n",
        "\n",
        "#function to split on \\n but that lets the city names stay together\n",
        "def create_city_dict(city_list):\n",
        "    city_dict = {}\n",
        "    cities = city_list[0].split('\\n')\n",
        "    for city in cities:\n",
        "        if city:\n",
        "            year, *city_name = city.strip().split()\n",
        "            city_dict[year] = ' '.join(city_name)\n",
        "    return city_dict\n",
        "\n",
        "\n",
        "#get the dict of all the champions league winners \n",
        "def champions_league_winners_list():\n",
        "    champions_league_url = 'https://en.wikipedia.org/wiki/List_of_European_Cup_and_UEFA_Champions_League_finals#List_of_finals'\n",
        "    all = get_all_tables(champions_league_url) #gets all tables of wiki page\n",
        "\n",
        "    #first prune of that huge dict\n",
        "    keyw= 'showvteEuropean Cup and UEFA Champions League winners'\n",
        "    pruned_dict = prune_dict_list(all,keyw)\n",
        "\n",
        "    #second prune\n",
        "    inter = pruned_dict.get('data') \n",
        "    cl_list = inter[2:5] + inter[7:11]\n",
        "    tmp1 = create_list_from_list_of_lists_key(cl_list, '\\n')\n",
        "    tmp2 = create_dict_from_list_of_lists(tmp1)\n",
        "\n",
        "    return tmp2\n",
        "\n",
        "#get the dict of all the euros winners \n",
        "def uefa_euros_winners_list():\n",
        "    euros_url = 'https://en.wikipedia.org/wiki/List_of_UEFA_European_Championship_finals#List_of_finals'\n",
        "    all = get_all_tables(euros_url) #gets all tables of wiki page\n",
        "\n",
        "    #first prune of that huge dict\n",
        "    keyw= 'showvteUEFA European Championship winners'\n",
        "    pruned_dict = prune_dict_list(all,keyw)\n",
        "\n",
        "    #second prune\n",
        "    inter = pruned_dict.get('data') \n",
        "    tmp1 = create_list_from_list_of_lists_key(inter, '\\n')\n",
        "    tmp2 = create_dict_from_list_of_lists(tmp1)\n",
        "\n",
        "    return tmp2\n",
        "\n",
        "#get the dict of all the wold cup winners \n",
        "def uefa_worlds_winners_list():\n",
        "    worlds_url = 'https://en.wikipedia.org/wiki/List_of_FIFA_World_Cup_finals#List_of_final_matches'\n",
        "    all = get_all_tables(worlds_url) #gets all tables of wiki page\n",
        "\n",
        "    tmp3=get_first_elements(all[3])\n",
        "\n",
        "    #first prune of that huge dict\n",
        "    keyw= 'showvteFIFA World Cup'\n",
        "    pruned_dict = prune_dict_list(all,keyw)\n",
        "  \n",
        "    #second prune\n",
        "    inter = pruned_dict.get('data') \n",
        "    inter = inter[2]\n",
        "\n",
        "    years = [s for s in inter[0].split('\\n')]\n",
        "\n",
        "    my_dict = dict(zip(years, tmp3))\n",
        "\n",
        "    return my_dict\n",
        "\n",
        "#get the dict of all the F1 drivers world champions\n",
        "def f1_winners_list():\n",
        "    euros_url = 'https://en.wikipedia.org/wiki/List_of_Formula_One_World_Drivers%27_Champions#By_season'\n",
        "    all = get_all_tables(euros_url) #gets all tables of wiki page\n",
        "\n",
        "    lst = all[2].get('data')\n",
        "\n",
        "    tmp2 = get_year_with_driver(lst)\n",
        "    tmp2 = clean_driver_names(tmp2)\n",
        "\n",
        "    return tmp2\n",
        "  \n",
        "#get the dict of all summer olympics host cities\n",
        "def summer_olympics_hosts_list():\n",
        "    summerO_url = 'https://en.wikipedia.org/wiki/Summer_Olympic_Games#List_of_Summer_Olympic_Games'\n",
        "    all = get_all_tables(summerO_url) #gets all tables of wiki page\n",
        "    lst = all[10].get('data')\n",
        "\n",
        "    tmp1 = create_list_from_list_of_lists_key(lst, '\\n')\n",
        "    tmp1 = tmp1[0]\n",
        "    tmp1 = create_dict_from_list(tmp1)\n",
        "    tmp1 = clean_dict_keys(tmp1)\n",
        "    tmp1 = clean_driver_names(tmp1)\n",
        "\n",
        "    return tmp1\n",
        "\n",
        "\n",
        "#get the dict of all winter olympics host cities\n",
        "def winter_olympics_hosts_list():\n",
        "    winterO_url = 'https://en.wikipedia.org/wiki/Winter_Olympic_Games#List_of_Winter_Olympic_Games'\n",
        "    all = get_all_tables(winterO_url) #gets all tables of wiki page\n",
        "    lst = all[8].get('data')\n",
        "\n",
        "    tmp1 = create_list_from_list_of_lists_key(lst, '\\n')\n",
        "    tmp1 = tmp1[0]\n",
        "    tmp1 = create_dict_from_list(tmp1)\n",
        "    tmp1 = clean_dict_keys(tmp1)\n",
        "    tmp1 = clean_driver_names(tmp1)\n",
        "\n",
        "    return tmp1\n"
      ],
      "metadata": {
        "id": "ARMx75Z0QkVh"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the hint for years question\n",
        "'''\n",
        "takes a list of years and then creates a dict of dicts, \n",
        "where (if available) the most popular sports events of that year are saved as hints.\n",
        "'''\n",
        "def popular_sports_per_year(years_list):\n",
        "  #For the Champions league\n",
        "  cl_all = champions_league_winners_list()\n",
        "  cl_sentence = ['In the same year, ', ' has won the UEFA Champions League.']\n",
        "\n",
        "  #For Football-Euros\n",
        "  euro_all = uefa_euros_winners_list()\n",
        "  euros_sentence = ['In the same year, ', ' has won the UEFA Euro Football Championship.']\n",
        "\n",
        "  #For Football-Worlds\n",
        "  worlds_all = uefa_worlds_winners_list()\n",
        "  worlds_sentence = ['In the same year, ', ' has won the FIFA World Cup.']\n",
        "\n",
        "  #For Fromula1\n",
        "  f1_all = f1_winners_list()\n",
        "  f1_sentence = ['In the same year, ', ' has won the F1 Drivers World Championship.']\n",
        "\n",
        "  #For OlympicSummerGames \n",
        "  summer_olympics_all = summer_olympics_hosts_list()\n",
        "  summer_olympics_sentence = ['In the same year, the Summer Olympics were held in ']\n",
        "\n",
        "  #For OlympicWinterGames \n",
        "  winter_olympics_all = winter_olympics_hosts_list()\n",
        "  winter_olympics_sentence = ['In the same year, the Winter Olympics were held in ']\n",
        "\n",
        "  pop_sport_hints_year = {}\n",
        "  for index in years_list:\n",
        "    year = index\n",
        "    year_s = str(year)\n",
        "\n",
        "    year_dict = {\n",
        "        'cl': '',\n",
        "        'euros': '',\n",
        "        'worlds': '',\n",
        "        'f1': '',\n",
        "        'summer': '',\n",
        "        'winter': '', \n",
        "        }\n",
        "\n",
        "    for key in cl_all:\n",
        "      if int(key.split('-')[1]) == year % 100:\n",
        "        result = cl_all[key]\n",
        "        break\n",
        "    if result:\n",
        "      year_dict['cl'] = cl_sentence[0] + result + cl_sentence[1] \n",
        "\n",
        "    for d in euro_all:\n",
        "      if year_s in d:\n",
        "        year_dict['euros'] = euros_sentence[0] + euro_all[year_s] + euros_sentence[1]\n",
        "\n",
        "    for d in worlds_all:\n",
        "      if year_s in d:\n",
        "        year_dict['worlds'] = worlds_sentence[0] + worlds_all[year_s] + worlds_sentence[1]\n",
        "\n",
        "    for d in f1_all:\n",
        "      if year_s in d:\n",
        "        year_dict['f1'] = f1_sentence[0] + f1_all[year_s] + f1_sentence[1]\n",
        "\n",
        "    for d in summer_olympics_all:\n",
        "      if year_s in d:\n",
        "        year_dict['summer'] = summer_olympics_sentence[0] + summer_olympics_all[year_s] \n",
        "\n",
        "    for d in winter_olympics_all:\n",
        "      if year_s in d:\n",
        "        year_dict['winter'] = winter_olympics_sentence[0] + winter_olympics_all[year_s] \n",
        "    \n",
        "    pop_sport_hints_year[year] = year_dict\n",
        "  \n",
        "  return pop_sport_hints_year"
      ],
      "metadata": {
        "id": "kuWB4hrxPCcV"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CALL FUNCTION\n",
        "#returns a dictionary of the years that occured in the xls file, with its corresponding hints from the most popular sports events of that year.\n",
        "def get_year_sports_hints():\n",
        "\n",
        "  file_years_list = []\n",
        "\n",
        "  for index, row in year_df.iterrows():\n",
        "    file_years_list.append(row[\"Answer\"])\n",
        "    \n",
        "  pop_sport_hints = popular_sports_per_year(file_years_list)\n",
        "  return pop_sport_hints\n",
        "  #import pprint\n",
        "  #pprint.pprint(pop_sport_hints, indent=4)"
      ],
      "metadata": {
        "id": "OO9_r-fCfKKz"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test it!"
      ],
      "metadata": {
        "id": "6vIkujY8y8Dj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint\n",
        "\n",
        "pop_year_hints = get_year_sports_hints()\n",
        "pop_thumb_hints = get_year_thumbcaption_hints()\n",
        "\n",
        "years_hints = {}\n",
        "\n",
        "for y in pop_year_hints:\n",
        "  year_dict = {\n",
        "      'sports': pop_year_hints[y],\n",
        "      'thumbcaption': pop_thumb_hints[y]\n",
        "  }\n",
        "\n",
        "  years_hints[y] = year_dict\n",
        "\n",
        "pprint.pprint(years_hints, indent=4)\n",
        "\n",
        "#pprint.pprint(pop_thumb_hints, indent=4)\n",
        "#pprint.pprint(pop_year_hints, indent=4)"
      ],
      "metadata": {
        "id": "-Pcx5uKity9M",
        "outputId": "55612b37-4269-48a9-a1c7-0d2e661be4cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{   1994: {   'sports': {   'cl': 'In the same year, AC Milan has won the UEFA '\n",
            "                                  'Champions League.',\n",
            "                            'euros': '',\n",
            "                            'f1': 'In the same year, Michael Schumacher has '\n",
            "                                  'won the F1 Drivers World Championship.',\n",
            "                            'summer': '',\n",
            "                            'winter': 'In the same year, the Winter Olympics '\n",
            "                                      'were held in Lillehammer',\n",
            "                            'worlds': 'In the same year, Brazil has won the '\n",
            "                                      'FIFA World Cup.'},\n",
            "              'thumbcaption': [   'In the same year, NAFTA, which was signed '\n",
            "                                  'in 1992, comes into effect in Canada, the '\n",
            "                                  'United States, and Mexico',\n",
            "                                  'In the same year, The  FIFA World Cup is '\n",
            "                                  'held in the United States',\n",
            "                                  'In the same year, Nelson Mandela casts his '\n",
            "                                  'vote in the  South African general '\n",
            "                                  'election, in which he was elected South '\n",
            "                                  \"Africa's first president, and which \"\n",
            "                                  'effectively brought Apartheid to an end',\n",
            "                                  'In the same year, From left, clockwise The  '\n",
            "                                  'Winter Olympics are held in Lillehammer, '\n",
            "                                  'Norway',\n",
            "                                  'In the same year, Skulls from the Rwandan '\n",
            "                                  'genocide, in which over half a million '\n",
            "                                  'Tutsi people were massacred by Hutus.',\n",
            "                                  'In the same year, A model of the MS '\n",
            "                                  'Estonia, which sank in the Baltic Sea',\n",
            "                                  'In the same year, The Kaiser Permanente '\n",
            "                                  'building after the  Northridge earthquake',\n",
            "                                  'In the same year, The first passenger rail '\n",
            "                                  'service to utilize the newly-opened Channel '\n",
            "                                  'tunnel']},\n",
            "    1995: {   'sports': {   'cl': 'In the same year, Ajax has won the UEFA '\n",
            "                                  'Champions League.',\n",
            "                            'euros': '',\n",
            "                            'f1': 'In the same year, Michael Schumacher has '\n",
            "                                  'won the F1 Drivers World Championship.',\n",
            "                            'summer': '',\n",
            "                            'winter': '',\n",
            "                            'worlds': ''},\n",
            "              'thumbcaption': []},\n",
            "    1999: {   'sports': {   'cl': 'In the same year, Manchester United has won '\n",
            "                                  'the UEFA Champions League.',\n",
            "                            'euros': '',\n",
            "                            'f1': 'In the same year, Mika Häkkinen has won the '\n",
            "                                  'F1 Drivers World Championship.',\n",
            "                            'summer': '',\n",
            "                            'winter': '',\n",
            "                            'worlds': ''},\n",
            "              'thumbcaption': [   'In the same year, the Columbine High School '\n",
            "                                  'massacre, one of the first major school '\n",
            "                                  'shootings in the United States',\n",
            "                                  'In the same year, the  İzmit earthquake '\n",
            "                                  'kills over 17,000 people in Turkey',\n",
            "                                  'In the same year, the Millennium Dome opens '\n",
            "                                  'in London',\n",
            "                                  'In the same year, NASA loses both the Mars '\n",
            "                                  'Climate Orbiter and the Mars Polar Lander',\n",
            "                                  'In the same year, the Year 2000 problem '\n",
            "                                  '(\"Y2K\"), perceived as a major concern in '\n",
            "                                  'the lead-up to the year 2000',\n",
            "                                  'In the same year, , clockwise The funeral '\n",
            "                                  'procession of King Hussein of Jordan in '\n",
            "                                  'Amman',\n",
            "                                  'In the same year, online music downloading '\n",
            "                                  'platform Napster is launched, soon a source '\n",
            "                                  'of online piracy',\n",
            "                                  'In the same year, a destroyed T-55 tank '\n",
            "                                  'near Prizren during the Kosovo War.']},\n",
            "    2000: {   'sports': {   'cl': 'In the same year, Manchester United has won '\n",
            "                                  'the UEFA Champions League.',\n",
            "                            'euros': 'In the same year, France has won the '\n",
            "                                     'UEFA Euro Football Championship.',\n",
            "                            'f1': 'In the same year, Michael Schumacher has '\n",
            "                                  'won the F1 Drivers World Championship.',\n",
            "                            'summer': 'In the same year, the Summer Olympics '\n",
            "                                      'were held in Sydney',\n",
            "                            'winter': '',\n",
            "                            'worlds': ''},\n",
            "              'thumbcaption': []},\n",
            "    2001: {   'sports': {   'cl': 'In the same year, Bayern Munich has won the '\n",
            "                                  'UEFA Champions League.',\n",
            "                            'euros': '',\n",
            "                            'f1': 'In the same year, Michael Schumacher has '\n",
            "                                  'won the F1 Drivers World Championship.',\n",
            "                            'summer': '',\n",
            "                            'winter': '',\n",
            "                            'worlds': ''},\n",
            "              'thumbcaption': [   'In the same year, About 585 of the deaths '\n",
            "                                  'are caused by landslides in Santa Tecla and '\n",
            "                                  'Comasagua.']},\n",
            "    2002: {   'sports': {   'cl': 'In the same year, Real Madrid has won the '\n",
            "                                  'UEFA Champions League.',\n",
            "                            'euros': '',\n",
            "                            'f1': 'In the same year, Michael Schumacher has '\n",
            "                                  'won the F1 Drivers World Championship.',\n",
            "                            'summer': '',\n",
            "                            'winter': 'In the same year, the Winter Olympics '\n",
            "                                      'were held in Salt Lake City',\n",
            "                            'worlds': 'In the same year, Brazil has won the '\n",
            "                                      'FIFA World Cup.'},\n",
            "              'thumbcaption': [   'In the same year, Queen Elizabeth The Queen '\n",
            "                                  'Mother and her daughter Princess Margaret, '\n",
            "                                  'Countess of Snowdon die',\n",
            "                                  'In the same year, an Armenian postage stamp '\n",
            "                                  'depicts the  FIFA World Cup, which was held '\n",
            "                                  'in South Korea and Japan',\n",
            "                                  'In the same year, FBI agents investigate a '\n",
            "                                  'crime scene related to the D.C. sniper '\n",
            "                                  'attacks']},\n",
            "    2003: {   'sports': {   'cl': 'In the same year, AC Milan has won the UEFA '\n",
            "                                  'Champions League.',\n",
            "                            'euros': '',\n",
            "                            'f1': 'In the same year, Michael Schumacher has '\n",
            "                                  'won the F1 Drivers World Championship.',\n",
            "                            'summer': '',\n",
            "                            'winter': '',\n",
            "                            'worlds': ''},\n",
            "              'thumbcaption': [   'In the same year, a statue of Saddam '\n",
            "                                  'Hussein is toppled in Baghdad after his '\n",
            "                                  'regime was deposed during the Iraq War.',\n",
            "                                  'In the same year, , clockwise The crew of '\n",
            "                                  'STS-107 perished when the Space Shuttle '\n",
            "                                  'Columbia disintegrated during reentry into '\n",
            "                                  \"Earth's atmosphere\",\n",
            "                                  'In the same year, SARS became an epidemic '\n",
            "                                  'in China, and was a precursor to SARS-CoV-2',\n",
            "                                  'In the same year, A U.S. Army M1 Abrams '\n",
            "                                  'tank patrols the streets of Baghdad after '\n",
            "                                  'the city fell to U.S.-led forces']},\n",
            "    2004: {   'sports': {   'cl': 'In the same year, Porto has won the UEFA '\n",
            "                                  'Champions League.',\n",
            "                            'euros': 'In the same year, Greece has won the '\n",
            "                                     'UEFA Euro Football Championship.',\n",
            "                            'f1': 'In the same year, Michael Schumacher has '\n",
            "                                  'won the F1 Drivers World Championship.',\n",
            "                            'summer': 'In the same year, the Summer Olympics '\n",
            "                                      'were held in Athens',\n",
            "                            'winter': '',\n",
            "                            'worlds': ''},\n",
            "              'thumbcaption': [   'In the same year,  Facebook, originally '\n",
            "                                  'called TheFacebook, is launched by Mark '\n",
            "                                  'Zuckerberg',\n",
            "                                  'In the same year, the European Union adds '\n",
            "                                  '10 new member-states',\n",
            "                                  'In the same year, NASA lands the '\n",
            "                                  'Opportunity Rover on Mars',\n",
            "                                  'In the same year, Al-Qaeda bombs multiple '\n",
            "                                  'trains in Madrid, killing 193 people',\n",
            "                                  'In the same year, the  Summer Olympics are '\n",
            "                                  'held in Athens',\n",
            "                                  'In the same year, 333 people are killed in '\n",
            "                                  'the Beslan school siege, carried out by '\n",
            "                                  'Chechen terrorists',\n",
            "                                  'In the same year, a massive megathrust '\n",
            "                                  'earthquake off the coast of Sumatra and the '\n",
            "                                  'resultant tsunami kill over 227,000 '\n",
            "                                  'people—one of the worst natural disasters '\n",
            "                                  'in recorded history.',\n",
            "                                  'In the same year, the  transit of Venus, '\n",
            "                                  'the first such occurrence since 1882']},\n",
            "    2006: {   'sports': {   'cl': 'In the same year, Barcelona has won the '\n",
            "                                  'UEFA Champions League.',\n",
            "                            'euros': '',\n",
            "                            'f1': 'In the same year, Fernando Alonso has won '\n",
            "                                  'the F1 Drivers World Championship.',\n",
            "                            'summer': '',\n",
            "                            'winter': 'In the same year, the Winter Olympics '\n",
            "                                      'were held in Turin',\n",
            "                            'worlds': 'In the same year, Italy has won the '\n",
            "                                      'FIFA World Cup.'},\n",
            "              'thumbcaption': [   'In the same year, The  FIFA World Cup in '\n",
            "                                  'Germany is won by Italy',\n",
            "                                  'In the same year, Twitter is founded and '\n",
            "                                  'launched by Jack Dorsey',\n",
            "                                  'In the same year, Montenegro votes to '\n",
            "                                  'declare independence from Serbia',\n",
            "                                  'In the same year, The IAU votes on the '\n",
            "                                  'definition of \"planet\", which demotes Pluto '\n",
            "                                  'and other Kuiper belt objects and redefines '\n",
            "                                  'them as \"dwarf planets\".',\n",
            "                                  'In the same year, Gol Transportes Aéreos '\n",
            "                                  'Flight 1907 crashes in the Amazon '\n",
            "                                  'rainforest after a mid-air collision with '\n",
            "                                  'an Embraer Legacy 600 business jet',\n",
            "                                  'In the same year, The Wii is released',\n",
            "                                  'In the same year, , clockwise The  Winter '\n",
            "                                  'Olympics open in Turin',\n",
            "                                  'In the same year, The  Yogyakarta '\n",
            "                                  'earthquake kills over 5,700 people']},\n",
            "    2021: {   'sports': {   'cl': 'In the same year, Chelsea has won the UEFA '\n",
            "                                  'Champions League.',\n",
            "                            'euros': '',\n",
            "                            'f1': 'In the same year, Max Verstappen has won '\n",
            "                                  'the F1 Drivers World Championship.',\n",
            "                            'summer': '',\n",
            "                            'winter': '',\n",
            "                            'worlds': ''},\n",
            "              'thumbcaption': [   'In the same year, A scene from the opening '\n",
            "                                  'ceremony of the 2020 Summer Olympics in '\n",
            "                                  'Tokyo, Japan',\n",
            "                                  'In the same year,  Taliban fighters in '\n",
            "                                  'Kabul on a captured Humvee following the  '\n",
            "                                  'fall of Kabul at the end of the War in '\n",
            "                                  'Afghanistan.',\n",
            "                                  'In the same year, The container ship Ever '\n",
            "                                  'Given gets stuck in the Suez Canal, '\n",
            "                                  'blocking international shipping for six '\n",
            "                                  'days',\n",
            "                                  'In the same year, Crowd shortly after the '\n",
            "                                  'January 6 United States Capitol attack']}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Old predictions methods**"
      ],
      "metadata": {
        "id": "bp5WmYAKw6Mn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "du5Un8VufnlG"
      },
      "source": [
        "## Functions for reuse"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8o8t6UzdAaKj"
      },
      "source": [
        "#get Wikidata ID\n",
        "def getQitemOfName(entity):\n",
        "  \n",
        "  searched = \"'\"+entity+\"'\"\n",
        "  sparql.setQuery('''\n",
        "  SELECT ?item ?itemLabel WHERE {\n",
        "    ?item rdfs:label '''+searched+'''@en.\n",
        "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
        "\n",
        "  }\n",
        "  ''')\n",
        "  \n",
        "  sparql.setReturnFormat(JSON)\n",
        "  entities = sparql.query().convert()\n",
        "  entities_df = pd.json_normalize(entities['results']['bindings'])\n",
        "  words = entities_df[[\"item.value\"]].iloc[0].str.split(\"/\")[0]\n",
        "  return words[-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4tNPxs6Cp8f"
      },
      "source": [
        "#get name of entity from Wikidata ID\n",
        "\n",
        "def getLabelOfQitems(entities):\n",
        "\n",
        "  finalLabels = []\n",
        "  for item in entities:\n",
        "    #searched = \"'\"+item+\"'\"\n",
        "    sparql.setQuery('''\n",
        "    SELECT * WHERE {\n",
        "        wd:'''+item+ ''' rdfs:label ?label .\n",
        "        FILTER (langMatches( lang(?label), \"EN\" ) )\n",
        "    } LIMIT 1\n",
        "    ''')\n",
        "    sparql.setReturnFormat(JSON)\n",
        "    entities = sparql.query().convert()\n",
        "    entities_df = pd.json_normalize(entities['results']['bindings'])\n",
        "    label = entities_df[\"label.value\"].iloc[0]\n",
        "    finalLabels.append(label)\n",
        "\n",
        "  return finalLabels  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gK0zkvbXxRpU"
      },
      "source": [
        "#function to get all backlinks to wikipage and saves backlinks >= 1000 in array\n",
        "#source: https://www.mediawiki.org/wiki/API:Backlinks#Response\n",
        "\n",
        "import requests\n",
        "\n",
        "def getBacklinks(entities):\n",
        "  S = requests.Session()\n",
        "\n",
        "  URL = \"https://en.wikipedia.org/w/api.php\"\n",
        "\n",
        "  listWithBacklink500 = []\n",
        "  dictWithAllEntries = {}\n",
        "\n",
        "  for val in entities:\n",
        "    #print(row[\"itemLabel.value\"])\n",
        "    PARAMS = {\n",
        "        \"action\": \"query\",\n",
        "        \"format\": \"json\",\n",
        "        \"list\": \"backlinks\",\n",
        "        \"bltitle\": val,\n",
        "        \"bllimit\": \"max\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "      R = S.get(url=URL, params=PARAMS)\n",
        "      DATA = R.json()\n",
        "   \n",
        "\n",
        "      BACKLINKS = DATA[\"query\"][\"backlinks\"]\n",
        "\n",
        "      count = 0\n",
        "      for links in BACKLINKS:\n",
        "        count +=1\n",
        "\n",
        "      \n",
        "      while \"continue\" in DATA:\n",
        "        if (count >= 1000):\n",
        "          break;\n",
        "        blcontinue = DATA[\"continue\"][\"blcontinue\"]\n",
        "        PARAMS[\"blcontinue\"] = blcontinue\n",
        "\n",
        "        R = S.get(url=URL, params=PARAMS)\n",
        "        DATA = R.json()\n",
        "        BACKLINKS = DATA[\"query\"][\"backlinks\"]\n",
        "\n",
        "        for links in BACKLINKS:\n",
        "          count += 1\n",
        "      \n",
        "      dictWithAllEntries[val] = count\n",
        "      #print(val + \": \" + str(count))\n",
        "\n",
        "    except ValueError:\n",
        "      print(\"Value Error\")\n",
        "\n",
        "    if (count >= 1000):\n",
        "      listWithBacklink500.append(val) \n",
        "\n",
        "  if (len(listWithBacklink500) > 0):\n",
        "    #return getPageViews(listWithBacklink500)\n",
        "    return listWithBacklink500\n",
        "\n",
        "  else:\n",
        "    listMostBacklinks = []\n",
        "    sortedDict = dict(sorted(dictWithAllEntries.items(), key=lambda item:item[1]))\n",
        "\n",
        "    for k, v in sortedDict.items():\n",
        "      listMostBacklinks.append(k)\n",
        "    \n",
        "    numberToPass = len(listMostBacklinks)//3\n",
        "        \n",
        "    return listMostBacklinks[-numberToPass:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-0fPfmxUZSW"
      },
      "source": [
        "#function to get all backlinks to wikipage and saves backlink == 500 in array\n",
        "#source: https://www.mediawiki.org/wiki/API:Backlinks#Response\n",
        "\n",
        "import requests\n",
        "\n",
        "def getBacklinksDict(entities):\n",
        "  S = requests.Session()\n",
        "\n",
        "  URL = \"https://en.wikipedia.org/w/api.php\"\n",
        "\n",
        "  dictWithBacklink500 = {}\n",
        "  dictWithAllEntries = {}\n",
        "\n",
        "  for key, val in entities.items():\n",
        "    print(key, val)\n",
        "\n",
        "    PARAMS = {\n",
        "        \"action\": \"query\",\n",
        "        \"format\": \"json\",\n",
        "        \"list\": \"backlinks\",\n",
        "        \"bltitle\": key,\n",
        "        \"bllimit\": \"max\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "      R = S.get(url=URL, params=PARAMS)\n",
        "      DATA = R.json()\n",
        "   \n",
        "\n",
        "      BACKLINKS = DATA[\"query\"][\"backlinks\"]\n",
        "\n",
        "      count = 0\n",
        "      for links in BACKLINKS:\n",
        "        count +=1\n",
        "\n",
        "      \n",
        "      while \"continue\" in DATA:\n",
        "        if (count >= 1000):\n",
        "          break;\n",
        "        blcontinue = DATA[\"continue\"][\"blcontinue\"]\n",
        "        PARAMS[\"blcontinue\"] = blcontinue\n",
        "\n",
        "        R = S.get(url=URL, params=PARAMS)\n",
        "        DATA = R.json()\n",
        "        BACKLINKS = DATA[\"query\"][\"backlinks\"]\n",
        "\n",
        "        for links in BACKLINKS:\n",
        "          count += 1\n",
        "      \n",
        "      dictWithAllEntries[key] = count\n",
        "      #print(val + \": \" + str(count))\n",
        "\n",
        "    except ValueError:\n",
        "      print(\"Value Error\")\n",
        "    except KeyError:\n",
        "      print(\"Key Error\")\n",
        "\n",
        "    if (count >= 1000):\n",
        "      dictWithBacklink500.update({key: val}) \n",
        "\n",
        "  if (len(dictWithBacklink500) > 0):\n",
        "    #return getPageViews(listWithBacklink500)\n",
        "    return dictWithBacklink500\n",
        "\n",
        "    #print(listMostBacklinks[-numberToPass:])\n",
        "\n",
        "    \n",
        "    return listMostBacklinks[-numberToPass:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hz4SZXwl1j7d"
      },
      "source": [
        "#get avg pageViews of entities\n",
        "\n",
        "def getPageViewsList(listEntities):\n",
        "  avgDict = {}\n",
        "  \n",
        "  for val in listEntities:\n",
        "    try:\n",
        "      resp = pageviewapi.per_article('en.wikipedia', val, '20191106', '20201120', access='all-access', agent='all-agents', granularity='monthly')\n",
        "    #print(resp)\n",
        "      avgViews = 0\n",
        "      count = 0\n",
        "      for i in resp.get(\"items\"):\n",
        "        #print(i.get(\"views\"))\n",
        "        avgViews += i.get(\"views\")\n",
        "        count += 1\n",
        "      avgDict.update({val: avgViews//count})\n",
        "    except:\n",
        "      print(\"Page not found to check pageViews for page: \" + val)\n",
        "      \n",
        "  #return (max(avgDict, key=avgDict.get))\n",
        "  return avgDict\n",
        "\n",
        "\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhL5jdCN-NdY"
      },
      "source": [
        "#get avg pageViews of entity\n",
        "import pageviewapi\n",
        "\n",
        "\n",
        "def getPageViews(entity):\n",
        "  avgDict = {}\n",
        "  \n",
        "  try:\n",
        "    resp = pageviewapi.per_article('en.wikipedia', entity, '20191106', '20201120', access='all-access', agent='all-agents', granularity='monthly')\n",
        "\n",
        "    avgViews = 0\n",
        "    count = 0\n",
        "    for i in resp.get(\"items\"):\n",
        "      #print(i.get(\"views\"))\n",
        "      avgViews += i.get(\"views\")\n",
        "      count += 1\n",
        "\n",
        "    pageViews = avgViews//count\n",
        "    #avgDict.update({val: avgViews//count})\n",
        "\n",
        "    #return (max(avgDict, key=avgDict.get))\n",
        "    return pageViews\n",
        "  #print(resp)\n",
        "  except:\n",
        "    print(\"Page not found to check pageViews for page: \" + entity)\n",
        "    return -1\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check if entity is a person\n",
        "#return list of persons\n",
        "\n",
        "def checkIfHuman(entity):\n",
        "\n",
        "  #listHumans = []\n",
        "\n",
        "  #for test in entity:\n",
        "    #print(item)\n",
        "  searched = \"'\"+entity+\"'\"\n",
        "\n",
        "  #print(searched)\n",
        "\n",
        "  try:\n",
        "\n",
        "    sparql.setQuery('''\n",
        "        SELECT distinct ?item ?itemLabel \n",
        "        WHERE{\n",
        "          ?item ?label '''+searched+'''.  \n",
        "          ?item wdt:P31 wd:Q5.\n",
        "          ?article schema:about ?item .\n",
        "          ?article schema:inLanguage \"en\" .\n",
        "          ?article schema:isPartOf <https://en.wikipedia.org/>.\t\n",
        "          SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }    \n",
        "        }\n",
        "    ''')\n",
        "    sparql.setReturnFormat(JSON)\n",
        "    entities = sparql.query().convert()\n",
        "    entities_df = pd.json_normalize(entities['results']['bindings'])\n",
        "\n",
        "    #print(entities)\n",
        "    #print(entities_df[\"itemLabel.value\"].item())\n",
        "\n",
        "    #for index, row in entities_df.iterrows():\n",
        "    return entities_df[\"itemLabel.value\"].item()\n",
        "    \n",
        "    \n",
        "  except:\n",
        "    return None\n"
      ],
      "metadata": {
        "id": "QHdp21OCwtgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOJJtksOgQYk"
      },
      "source": [
        "#if parantheses in entity -> remove\n",
        "\n",
        "def removeParanthesesDict(inputDict):\n",
        "\n",
        "  newDict = {}\n",
        "  for key, val in inputDict.items():\n",
        "    if ('(' in key):\n",
        "      newElem = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", key).rstrip()\n",
        "      newDict.update({newElem: val})\n",
        "    else:\n",
        "      newDict.update({key: val})\n",
        "\n",
        "  return newDict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lhmuuUNQ__8"
      },
      "source": [
        "##Hints if answer is a Location"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-T2fLcTRD54"
      },
      "source": [
        "#Query with all properties to get data\n",
        "\n",
        "def allHintsLocation(entity):\n",
        "  sparql.setQuery('''\n",
        "  SELECT ?itemLabel ?property\n",
        "  WHERE\n",
        "  {  \n",
        "    VALUES ?property {wdt:P35 wdt:P30 wdt:P36 wdt:P610 wdt:P1082 wdt:P421 wdt:P38 wdt:P17 wdt:P1376 wdt:P131 wdt:P6 wdt:P1830 wdt:P47 wdt:P206 wdt:P37 wdt:P463 wdt:793}\n",
        "    wd:'''+entity+''' ?property ?item.\n",
        "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
        "  }\n",
        "  ''')\n",
        "  sparql.setReturnFormat(JSON)\n",
        "  hintsLocation = sparql.query().convert()\n",
        "\n",
        "  hintsLocation_df = pd.json_normalize(hintsLocation['results']['bindings'])\n",
        "\n",
        "  return hintsLocation_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "co1A0iK-sery"
      },
      "source": [
        "#create template based hints\n",
        "\n",
        "def getFormatedHintsLocation(queryResults_df):\n",
        "  timeZone = []\n",
        "  isLocatedIn = []\n",
        "  ownerOf = []\n",
        "  shareBorder = []\n",
        "  bodyOfWater = []\n",
        "  languages = []\n",
        "  memberOf = []\n",
        "  significantEvents = []\n",
        "\n",
        "  listHintsLocation = []\n",
        "\n",
        "  searchString = \"http://www.wikidata.org/prop/direct/\"\n",
        "  \n",
        "  for index, row in queryResults_df.iterrows():\n",
        "    if(not location in row[\"itemLabel.value\"]):\n",
        "      if(row[\"property.value\"] == searchString + \"P35\"):\n",
        "        listHintsLocation.append(\"Head of state of searched country is \" + row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P30\"):\n",
        "        listHintsLocation.append(\"The searched location is on continent \" + row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P36\"):\n",
        "        listHintsLocation.append(\"The capital of searched country is \" + row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"\"):\n",
        "        listHintsLocation.append(\"The highest point in searched location is \" + row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P1082\"):\n",
        "        listHintsLocation.append(\"The searched location has a population of \" + row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P38\"):\n",
        "        listHintsLocation.append(\"Currency in searched location is \" + row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P17\"):\n",
        "        if(not location in row[\"itemLabel.value\"]):\n",
        "          listHintsLocation.append(\"The searched location is in country \"+ row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P1376\"):\n",
        "        listHintsLocation.append(\"The searched city is capital of \" + row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P131\"):\n",
        "        listHintsLocation.append(\"The searched location is located in \" + row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P6\"):\n",
        "        listHintsLocation.append(\"Head of government of searched location is \" + row[\"itemLabel.value\"])\n",
        "      \n",
        "      #if get multiple values save all in list and pick one afterwards\n",
        "      elif(row[\"property.value\"] == searchString + \"P1830\"):\n",
        "        if(not location in row[\"itemLabel.value\"]):\n",
        "          ownerOf.append(row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P47\"):\n",
        "        shareBorder.append(row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P206\"):\n",
        "        bodyOfWater.append(row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P37\"):\n",
        "        if(not location in row[\"itemLabel.value\"]):\n",
        "          languages.append(row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P421\"):\n",
        "        timeZone.append(row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P463\"):\n",
        "        memberOf.append(row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P793\"):\n",
        "        if(not location in row[\"itemLabel.value\"]):\n",
        "          significantEvents.append(row[\"itemLabel.value\"])\n",
        "\n",
        "  #getting more than one result with one query, pick one for output\n",
        "  if(len(ownerOf) > 0):\n",
        "    listHintsLocation.append(\"The searched location is owner of \" + ownerOf[0])\n",
        "  if(len(shareBorder) > 0):\n",
        "    listHintsLocation.append(\"The searched location shares border with \" + shareBorder[0])\n",
        "  if(len(bodyOfWater) > 0):\n",
        "    listHintsLocation.append(\"The next body of water of searched location is \" + bodyOfWater[0])\n",
        "  if(len(memberOf) > 0):\n",
        "    listBacklinks = getBacklinks(memberOf)\n",
        "    dictPageViews = getPageViewsList(listBacklinks)\n",
        "    listHintsLocation.append(\"Searched location is member of \" + (max(dictPageViews, key=dictPageViews.get)))\n",
        "  if(len(significantEvents) > 0):\n",
        "    listBacklinks = getBacklinks(significantEvents)\n",
        "    dictPageViews = getPageViewsList(listBacklinks)\n",
        "    listHintsLocation.append(\"A significant event happened in this location was \" + (max(dictPageViews, key=dictPageViews.get)))\n",
        "\n",
        "  #if getting more than one language concatenate all\n",
        "  commaCounter = len(languages)-1\n",
        "  stringLanguages = \"\"\n",
        "  for language in languages:\n",
        "    if(commaCounter > 0):\n",
        "      stringLanguages += language + \", \"\n",
        "      commaCounter -= 1\n",
        "    else:\n",
        "      stringLanguages += language\n",
        "  if(stringLanguages != \"\"):\n",
        "    listHintsLocation.append(\"Spoken language(s) in searched location is/are \" + stringLanguages)\n",
        "  if(len(timeZone) > 0):\n",
        "    listHintsLocation.append(\"The searched location is in time Zone \" + timeZone[0])\n",
        "\n",
        "  \n",
        "  return listHintsLocation\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g65hUdVTRbXU"
      },
      "source": [
        "def writeLocationHintsInFile(listHintsLocation):\n",
        "  randomHintsList = random.sample(listHintsLocation, len(listHintsLocation))\n",
        "  \n",
        "  for item in randomHintsList:\n",
        "    with open(\"hintsLocation.txt\", 'a') as writefile:\n",
        "      writefile.write(item + \"\\n\")\n",
        "      writefile.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Yf6Fi_34ye5"
      },
      "source": [
        "##Hints if answer is a Person"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Query with all properties to get data\n",
        "\n",
        "def allHintsPerson(entity):\n",
        "  sparql.setQuery('''\n",
        "  SELECT ?itemLabel ?property\n",
        "  WHERE\n",
        "  {  \n",
        "    VALUES ?property {wdt:P21 wdt:P27 wdt:P569 wdt:P19 wdt:P1971 wdt:P106 wdt:P1340 wdt:P1884 wdt:P2048 wdt:P39 wdt:P69 wdt:P512 wdt:P102 wdt:P3602 wdt:P800 wdt:P166 wdt:P3373 wdt:P1412 wdt:P413 wdt:P118 wdt:P54}\n",
        "    wd:'''+entity+''' ?property ?item.\n",
        "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
        "  }\n",
        "  ''')\n",
        "  sparql.setReturnFormat(JSON)\n",
        "  hintsPerson = sparql.query().convert()\n",
        "\n",
        "  hintsPerson_df = pd.json_normalize(hintsPerson['results']['bindings'])\n",
        "  \n",
        "  return hintsPerson_df"
      ],
      "metadata": {
        "id": "Lm8zYu5C3xSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qS1ZtKwL5hM-"
      },
      "source": [
        "#create temolate based hints\n",
        "\n",
        "def getFormatedHintsPerson(queryResults_df):\n",
        "\n",
        "  listOccupation = [\"Searched person was occuupied as \"]\n",
        "  listPositionHeld = [\"Searched person held the position(s) \"]\n",
        "  listEducatedAt = [\"Searched person was educated at \"]\n",
        "  listAcademicDegrees = [\"Searched person has academic degrees \"]\n",
        "  listPoliticParty = [\"Searched was/is member of politic party \"]\n",
        "  listCandidacyElection = [\"Searched person was candidacy at election(s) \"]\n",
        "  listNotableWork = [\"Searched person did notable work \"]\n",
        "  listAwardsReceived = [\"Searched person received awards \"]\n",
        "  listLanguages = [\"Searched person speaks language(s) \"]\n",
        "  listPlayedPositions = [\"Searched person played position \"]\n",
        "  listLeague = [\"Searched person played in leagues \"]\n",
        "  listTeams = [\"Searched person played in teams \"]\n",
        "  listCitizenship = [\"Searched person has following citizenships \"]\n",
        "  listSiblings = []\n",
        "  listHair = []\n",
        "\n",
        "  listHintsPerson = []\n",
        "\n",
        "  for index, row in queryResults_df.iterrows():\n",
        "    if(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P21\"):\n",
        "      listHintsPerson.append(\"Gender of searched person is \" + row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P569\"):\n",
        "      listHintsPerson.append(\"Searched person was born in \" + row[\"itemLabel.value\"][:4])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P19\"):\n",
        "      listHintsPerson.append(\"Searched person was born in \" + row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P1971\"):\n",
        "      listHintsPerson.append(\"Number of children of seared person is \" + row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P1340\"):\n",
        "      listHintsPerson.append(\"Searched person has \" + row[\"itemLabel.value\"] + \" eyes\")    \n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P2048\"):\n",
        "      listHintsPerson.append(\"Searched persons height is \" + row[\"itemLabel.value\"])    \n",
        "\n",
        "    \n",
        "    #if get multiple values save all in list and pick one afterwards\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P27\"):\n",
        "      listCitizenship.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P106\"):\n",
        "      listOccupation.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P39\"):\n",
        "      listPositionHeld.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P69\"):\n",
        "      listEducatedAt.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P512\"):\n",
        "      listAcademicDegrees.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P102\"):\n",
        "      listPoliticParty.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P3602\"):\n",
        "      listCandidacyElection.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P800\"):\n",
        "      listNotableWork.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P166\"):\n",
        "      listAwardsReceived.append(row[\"itemLabel.value\"])\n",
        "\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P1412\"):\n",
        "      listLanguages.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P413\"):\n",
        "      listPlayedPositions.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P118\"):\n",
        "      listLeague.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P54\"):\n",
        "      listTeams.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P3373\"):\n",
        "      listSiblings.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P1884\"):\n",
        "      listHair.append(row[\"itemLabel.value\"])\n",
        "\n",
        "  #if too many results could be return check\n",
        "  #if more thane 3 -> cut\n",
        "  listResultsCut = [listPositionHeld, listAcademicDegrees, listPoliticParty, \n",
        "                 listCandidacyElection, listNotableWork, listLanguages, \n",
        "                 listPlayedPositions, listLeague, listTeams, listCitizenship, listAwardsReceived, listEducatedAt, listOccupation]\n",
        "  for singleList in listResultsCut:\n",
        "    cutResults(singleList, listHintsPerson)\n",
        "\n",
        "  #just get one hair color\n",
        "  if(len(listHair) > 0):\n",
        "    listHintsPerson.append(\"The searched person has \" + listHair[0])\n",
        "\n",
        "  #get number of siblings\n",
        "  if(len(listSiblings) > 0):\n",
        "    listHintsPerson.append(\"The searched person has \" + str(len(listSiblings)) + \" siblings\")  \n",
        "  \n",
        "  return listHintsPerson"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#if more results than 3 -> print 3 results + \"and x others\"\n",
        "\n",
        "def cutResults(listResults, listHintsPerson):\n",
        "  \n",
        "  numResults = len(listResults)-1\n",
        "  resultsLeft = numResults-3\n",
        "\n",
        "  if (len(listResults) > 1):\n",
        "    if(resultsLeft > 0):\n",
        "      listHintsPerson.append(getConcatenationMultipleResults(listResults[0:4])+ \" and \" + str(resultsLeft) + \" others\")\n",
        "    else:\n",
        "      listHintsPerson.append(getConcatenationMultipleResults(listResults))"
      ],
      "metadata": {
        "id": "KDP4yoRpB_Sq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIOq3XcwIr2a"
      },
      "source": [
        "#concatenate multiple results with comma\n",
        "\n",
        "def getConcatenationMultipleResults(resultList):\n",
        "  concatenation = \"\"\n",
        "  commaCounter = len(resultList)-1\n",
        "  for item in resultList:\n",
        "    if(commaCounter == len(resultList)-1):\n",
        "      concatenation += item\n",
        "      commaCounter-=1\n",
        "    elif(commaCounter > 0):\n",
        "      concatenation += item + \", \"\n",
        "      commaCounter-=1\n",
        "    elif(commaCounter == 0):\n",
        "      concatenation += item\n",
        "  \n",
        "  return concatenation\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B74PSzscAzl8"
      },
      "source": [
        "def writePersonHintsInFile(listHintsPerson):\n",
        "  with open(\"hintsPerson.txt\", 'a') as writefile:\n",
        "    #writefile.write(\"\\n\\n---Hints for Person \" + person + \"---\\n\\n\")\n",
        "    randomHintsList = random.sample(listHintsPerson, len(listHintsPerson))\n",
        "    for item in randomHintsList:\n",
        "      writefile.write(item + \"\\n\")\n",
        "    writefile.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LORwHLog-t_"
      },
      "source": [
        "## Hints if answer is a Year"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dil5f183hhHJ"
      },
      "source": [
        "**Get all outlinks of wikipedia year page and pass to getBacklinks. Than look for return values on page.content and print line**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQFplu-7AF1L"
      },
      "source": [
        "import spacy\n",
        "def hintsYear(year):\n",
        "\n",
        "  page = wikipedia.page(year, auto_suggest=False)\n",
        "  pageContent = page.content\n",
        "\n",
        "  #get all links from year page\n",
        "  dictPageLinks = getOutlinks2(year)\n",
        "  #print(dictPageLinks)\n",
        "\n",
        "  #get rid of unnesercery links\n",
        "  filteredPageLinks = filterDict(dictPageLinks)\n",
        "\n",
        "  \n",
        "  #get backlinks of all links\n",
        "  dictBacklink = getBacklinksDict(filteredPageLinks)\n",
        "  #first200 = {k: filteredPageLinks[k] for k in list(filteredPageLinks)[:200]}\n",
        "  #dictBacklink = getBacklinksDict(first200)\n",
        "\n",
        "  #remove parantheses of links\n",
        "  dictAdjusted = removeParanthesesDict(dictBacklink)\n",
        "\n",
        "  #get all possible hint (all lines with a entity #backlinks > 1000)\n",
        "  dictPossibleHints = findLineInTextEvent(dictAdjusted, year)\n",
        "\n",
        "  with open('./resultsEvents.txt', 'a') as writefile:\n",
        "    writefile.write(\"Question: \" + questionYear + \"(\"+ str(year) + \")---\\n\")\n",
        "\n",
        "  #check if entity because of which line was found is subject\n",
        "  #if it is not the subject -> drop\n",
        "  dictFinalHints = getFinalHints(dictPossibleHints)\n",
        "\n",
        "  #get rid of number of foundevents at end of line\n",
        "  #need to put number at the and because it is dict and would be overwritten\n",
        "  dictFinalHintsAdjusted = {}\n",
        "  for key, val in dictFinalHints.items():\n",
        "    dictFinalHintsAdjusted.update({key.split('///', 1)[0]: val})  \n",
        "\n",
        "  #returns dict sorted after PageViews\n",
        "  dictSortedPV = sortHintsNumberPVSubject(dictFinalHintsAdjusted)\n",
        "\n",
        "  #get highest PageView\n",
        "  maxPV = list(dictSortedPV.values())[0]\n",
        "\n",
        "  #sort hints PageViews and Simscore combined\n",
        "  sortHintsUtilityScore(questionYear, dictSortedPV, maxPV)\n",
        "\n",
        "  listHumansNames = []\n",
        "\n",
        "  for key, val in dictAdjusted.items():\n",
        "    result = checkIfHuman(key)\n",
        "    if(result != None):\n",
        "      listHumansNames.append(result)\n",
        "\n",
        "  #findLineInTextBirthDeath(listHumansNames, year)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-dJk5vrhq6M"
      },
      "source": [
        "**Function to find line of most important link in year content**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0GonOTeikJm"
      },
      "source": [
        "import re\n",
        "\n",
        "def findLineInTextEvent(entitiesEvent, year):\n",
        "\n",
        "  page = wikipedia.page(year, auto_suggest=False)\n",
        "  pageText = page.content\n",
        "  splitText = []\n",
        "\n",
        "  splitText = (re.split(\"== Events ==|== Births ==\", pageText ))\n",
        "\n",
        "  contentEvents = splitText[1]\n",
        "\n",
        "  foundEvent = 0\n",
        "\n",
        "  dictHintsEntity = {}\n",
        "\n",
        "  for key, val in entitiesEvent.items():\n",
        "    #if (foundEvent == 20):\n",
        "      #break;\n",
        "\n",
        "    for line in contentEvents.split(\"\\n\"):\n",
        "      #if (foundEvent == 20):\n",
        "        #break;\n",
        "\n",
        "      if val in line:\n",
        "        words = line.split()\n",
        "        #counterEntitiesInLine = 0\n",
        "        if(len(words) < 15):\n",
        "          dictHintsEntity.update({line + \"///\" + str(foundEvent): val})\n",
        "          #listHints.append(line)\n",
        "          foundEvent += 1\n",
        "          #break;\n",
        "  return dictHintsEntity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZAM4MxB-8By"
      },
      "source": [
        "def findLineInTextBirthDeath(entitiesHuman, year):\n",
        "\n",
        "  page = wikipedia.page(year, auto_suggest=False)\n",
        "  pageText = page.content\n",
        "  splitText = []\n",
        "\n",
        "  splitText = (re.split(\"== Births ==|== Deaths ==\", pageText ))\n",
        "\n",
        "  print(splitText)\n",
        "\n",
        "  contentBirths = splitText[1]\n",
        "  contentDeaths = splitText[2]\n",
        "\n",
        "  foundBirth = 0\n",
        "  foundDeath = 0\n",
        "\n",
        "  #open file and append results\n",
        "  with open('./resultsBirthDeath.txt', 'a') as writefile:\n",
        "    writefile.write(\"---Birth/Death in year \" + str(year) + \"---\")\n",
        "\n",
        "    for entity in entitiesHuman:\n",
        "      #if (foundBirth == 3 and foundDeath == 3):\n",
        "        #writefile.write(\"\\n\")\n",
        "        #break;\n",
        "\n",
        "      for line in contentBirths.split(\"\\n\"):\n",
        "        #if (foundBirth == 3):\n",
        "          #break;\n",
        "\n",
        "        if entity in line:\n",
        "          writefile.write(\"Found with entity \" + entity + \"\\n\")\n",
        "          writefile.write(\"Birth: \" + line + \"\\n\")\n",
        "          writefile.write(\"\\n\")\n",
        "          foundBirth +=1\n",
        "          break;\n",
        "\n",
        "      for line in contentDeaths.split(\"\\n\"):\n",
        "        #if (foundDeath == 3):\n",
        "          #break;\n",
        "        if entity in line:\n",
        "          writefile.write(\"Found with entity \" + entity + \"\\n\")\n",
        "          writefile.write(\"Death: \" + line + \"\\n\")\n",
        "          writefile.write(\"\\n\")\n",
        "          foundDeath += 1\n",
        "          break;\n",
        "    writefile.write(\"\\n\")\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUy44eeb0EKg"
      },
      "source": [
        "def sortHintsNumberPVSubject(dictHints):\n",
        "\n",
        "  with open('./resultsEvents.txt', 'a') as writefile: \n",
        "\n",
        "    dictFinalHintsPV = {}\n",
        "\n",
        "    for key, val in dictHints.items():\n",
        "      dictFinalHintsPV.update({key: getPageViews(val)})\n",
        "\n",
        "    dictFinalHintsSortedPV = dict(sorted(dictFinalHintsPV.items(), key=lambda item: item[1], reverse=True))\n",
        "    \"\"\"writefile.write(\"\\n--Final Hints sorted #PV---\\n\")\n",
        "    for key, val in dictFinalHintsSortedPV.items():\n",
        "      writefile.write(key + \"(\" + str(val) + \" PageViews)\\n\")\"\"\"\n",
        "\n",
        "  return dictFinalHintsSortedPV\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4YpAjm5B6CM"
      },
      "source": [
        "\n",
        "\n",
        "def sortHintsUtilityScore(question, dictFinalHintsSortedPV, maxPV):\n",
        "  model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "  question_embedding = model.encode(question)\n",
        "\n",
        "  alpha = 0.5\n",
        "\n",
        "  #sentence_embeddings = model.encode(hints)\n",
        "\n",
        "  dictHintsBERT = {}\n",
        "\n",
        "  for key, val in dictFinalHintsSortedPV.items():\n",
        "    sentence_embedding = model.encode(key)\n",
        "    simScore = cosine_similarity([question_embedding], [sentence_embedding]).item()\n",
        "    impScore = alpha * (val/maxPV) + (1-alpha) * simScore\n",
        "    dictHintsBERT.update({key: impScore})\n",
        "\n",
        "  with open('./resultsEvents.txt', 'a') as writefile:\n",
        "\n",
        "    dictHintsBERT = dict(sorted(dictHintsBERT.items(), key=lambda item: item[1], reverse=True))\n",
        "    writefile.write(\"\\n--Final Hints sorted Utility Score---\\n\")\n",
        "    for key, val in dictHintsBERT.items():\n",
        "      writefile.write(key + \"(\" + str(val) + \" Utility Score)\\n\")\n",
        "\n",
        "  return dictHintsBERT\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxuRQ-FsZdQ_"
      },
      "source": [
        "def get_subject_phrase(doc):\n",
        "    for token in doc:\n",
        "        if (\"subj\" in token.dep_):\n",
        "            subtree = list(token.subtree)\n",
        "            start = subtree[0].i\n",
        "            end = subtree[-1].i + 1\n",
        "            return doc[start:end]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBif9Lc8_sg3"
      },
      "source": [
        "def getFinalHints(dictPossibleHints):\n",
        "\n",
        "  #nlp = spacy.load(\"en\")\n",
        "  nlp = spacy.load(\"en_core_web_sm\")\n",
        "  dictFinalHintsSubject = {}\n",
        "  \n",
        "  for key, val in dictPossibleHints.items():\n",
        "    doc = nlp(key)\n",
        "    subject = get_subject_phrase(doc)\n",
        "    if (val in str(subject)):\n",
        "      dictFinalHintsSubject.update({key: val})\n",
        "      #listFinalHints.append(key)\n",
        "\n",
        "  return dictFinalHintsSubject"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JUt_0x6jD-x"
      },
      "source": [
        "\n",
        "def getOutlinks2(year):\n",
        "\n",
        "  URL = 'https://en.wikipedia.org/wiki/' + str(year)\n",
        "\n",
        "  # Fetch all the HTML source from the url\n",
        "  response = requests.get(URL)\n",
        "\n",
        "\n",
        "  soup = bs4.BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "  listTextOfLinks = []\n",
        "  listLinks = []\n",
        "\n",
        "  dictLinkName = {}\n",
        "\n",
        "\n",
        "  for link in soup.find_all(\"a\"):\n",
        "    listTextOfLinks.append(link.get_text())\n",
        "    dictLinkName.update({str(link.get(\"href\")).split('/')[-1].replace('_', ' '): link.get_text()})\n",
        "\n",
        "  #newDict = filterDict(dictLinkName)\n",
        "\n",
        "  #print(len(dictLinkName))\n",
        "  #print(len(newDict))\n",
        "  #print(newDict)\n",
        "\n",
        "  return dictLinkName"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAuFgVXomCdf"
      },
      "source": [
        "def filterDict(dictNameLink):\n",
        "\n",
        "  patternYear = r\"[1-3][0-9]{3}\"\n",
        "  patternCentury = r\"[1-2][0-9]\\w+.century\"\n",
        "  patternAd = r\"AD.\\d\\d\\d\\d\"\n",
        "  patternMonth = r\"[a-zA-Z]+\\s\\d+\"\n",
        "  patternMillennium = r\".+millennium\"\n",
        "  patternHashtag = r\"#\"\n",
        "  patternCalendar = r\"calendar\"\n",
        "  patternCategory = r\"category\"\n",
        "  patternList = r\"list\"\n",
        "  patternTemplate = r\"template\"\n",
        "  patternWikipedia = r\"wikipedia\"\n",
        "  patternIndex = r\"index\"\n",
        "  patternHtml = r\"html\"\n",
        "  patternPercent = r\"%\"\n",
        "\n",
        "  newDict = {}\n",
        "\n",
        "  for key, val in dictNameLink.items():\n",
        "    if(not re.match(patternYear, key) and\n",
        "       not re.match(patternCentury, key) and \n",
        "       not re.match(patternAd, key) and \n",
        "       not re.match(patternMonth, key) and\n",
        "       not re.search(patternHashtag, key) and\n",
        "       not re.search(patternCalendar, key, re.IGNORECASE) and\n",
        "       not re.search(patternCategory, key, re.IGNORECASE) and\n",
        "       not re.search(patternList, key, re.IGNORECASE) and\n",
        "       not re.search(patternTemplate, key, re.IGNORECASE) and\n",
        "       not re.search(patternWikipedia, key, re.IGNORECASE) and\n",
        "       not re.search(patternIndex, key, re.IGNORECASE) and\n",
        "       not re.search(patternHtml, key, re.IGNORECASE) and\n",
        "       not re.search(patternPercent, key) and\n",
        "       not re.match(patternMillennium, key) and\n",
        "       not val == \"\" and not val == \" \"):\n",
        "      \n",
        "\n",
        "      newDict.update({key: val})\n",
        "\n",
        "\n",
        "  print(newDict)\n",
        "\n",
        "  return newDict\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Call Functions\n"
      ],
      "metadata": {
        "id": "cxaEE5DFfr_v"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bA1ZDHtbAkJJ"
      },
      "source": [
        "**Call hint function for all questions where answer is Location**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kHh8TAEt6OS"
      },
      "source": [
        "for index, row in location_df.iterrows():\n",
        "  location = row[\"Answer\"]\n",
        "  questionLocation = row[\"Question\"]\n",
        "  with open(\"hintsLocation.txt\", 'a') as writefile:\n",
        "    writefile.write(\"\\n\\n\" + questionLocation + \" (\" + location + \")\\n\\n\")\n",
        "    writefile.close()\n",
        "\n",
        "\n",
        "  #get Wikidata ID for SPARQL-Query\n",
        "  locationQitem = getQitemOfName(location)\n",
        "\n",
        "  #execute SPARQL-Query\n",
        "  queryResults_df = allHintsLocation(locationQitem)\n",
        "\n",
        "  #getFinalHints\n",
        "  listHintsLocation = getFormatedHintsLocation(queryResults_df)    \n",
        "\n",
        "  #write hints in file in random order\n",
        "  writeLocationHintsInFile(listHintsLocation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYzskrP0AtKs"
      },
      "source": [
        "**Call hint function for all questions where answer is Person**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B__QLqKMAX51"
      },
      "source": [
        "\n",
        "for index, row in person_df.iterrows():\n",
        "  person = row[\"Answer\"]\n",
        "  questionPerson = row[\"Question\"]\n",
        "\n",
        "  with open(\"hintsPerson.txt\", 'a') as writefile:\n",
        "    writefile.write(\"\\n\\n\" + questionPerson + \" (\"+ person +\")\\n\\n\")\n",
        "    writefile.close()\n",
        "  \n",
        "  #get Wikipedia ID for SPARQL-Query\n",
        "  personQitem = getQitemOfName(person)\n",
        "\n",
        "  #execute SPARQL-Query\n",
        "  queryResults_df = allHintsPerson(personQitem)\n",
        "  \n",
        "  #get Final Hints\n",
        "  listHintsPerson = getFormatedHintsPerson(queryResults_df)\n",
        "\n",
        "  #write hints in file in random order\n",
        "  writePersonHintsInFile(listHintsPerson)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjsplvG5DUJn"
      },
      "source": [
        "**Call hint function for all question where answer is Year**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z5x7mjMDY-b"
      },
      "source": [
        "for index, row in year_df.iterrows():\n",
        "  year = row[\"Answer\"]\n",
        "  questionYear = row[\"Question\"]\n",
        "  hintsYear(row[\"Answer\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pageview Test Functions\n"
      ],
      "metadata": {
        "id": "GXcHCM1dcbai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import requests\n",
        "\n",
        "from selenium import webdriver\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def get_table_info(url):\n",
        "    \"\"\"\n",
        "    Given a URL, this function opens the url and retrieves the information stored in a table.\n",
        "    \"\"\"\n",
        "    options = webdriver.FirefoxOptions()\n",
        "    #options.headless = True\n",
        "    options.add_argument('--headless')\n",
        "\n",
        "    driver = webdriver.Firefox(options=options)\n",
        "    driver.get(url)\n",
        "    time.sleep(1) # Wait for the page to load completely\n",
        "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "    driver.quit()\n",
        "    table = soup.find('table')\n",
        "    rows = table.find_all('tr')\n",
        "    headers = [header.text.strip() for header in rows[0].find_all('th')]\n",
        "    data = []\n",
        "    for row in rows[1:]:\n",
        "        data.append([cell.text.strip() for cell in row.find_all('td')])\n",
        "    return (headers, data)\n",
        "\n",
        "def get_table_info_requests(url):\n",
        "    \"\"\"\n",
        "    Given a URL, this function opens the url and retrieves the information stored in a table.\n",
        "    \"\"\"\n",
        "    headers = []\n",
        "    data = []\n",
        "\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    table = soup.find('table')\n",
        "    rows = table.find_all('tr')\n",
        "    headers = [header.text.strip() for header in rows[0].find_all('th')]\n",
        "    data = []\n",
        "    for row in rows[1:]:\n",
        "        data.append([cell.text.strip() for cell in row.find_all('td')])\n",
        "\n",
        "    return (headers, data)\n",
        "\n",
        "def get_links_in_section(wikipedia_url, section_heading):\n",
        "    \"\"\"\n",
        "    Given a Wikipedia URL, and a section of this article, returns an array of dictionaries containing the href, title, and description of each link on that section of the page.\n",
        "    \"\"\"\n",
        "    response = requests.get(wikipedia_url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    section = soup.find('span', {'id': section_heading})\n",
        "    if section is None:\n",
        "        return None\n",
        "    section_content = section.parent.find_next_sibling('ul')\n",
        "    if section_content is None:\n",
        "        return None\n",
        "    links = []\n",
        "    for item in section_content.find_all('li'):\n",
        "        link = item.find('a')\n",
        "        if link is not None and link.has_attr('href') and link['href'].startswith('/wiki/'):\n",
        "            title = link.get('title', '')\n",
        "            description = item.text.strip()\n",
        "            url = f\"https://en.wikipedia.org{link['href']}\"\n",
        "            links.append({'title': title, 'description': description, 'url': url})\n",
        "    print(links)\n",
        "    return links\n",
        "\n",
        "def get_links_in_section_with_sublinks(wikipedia_url, section_title):\n",
        "    \"\"\"\n",
        "    Given a Wikipedia URL, and a section of this article, returns an array of dictionaries containing the href, title, and description of each link  with its sublinks as well.\n",
        "    \"\"\"\n",
        "    response = requests.get(wikipedia_url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        section_heading = soup.find('span', {'id': section_title})\n",
        "        if section_heading is None:\n",
        "            return None\n",
        "        section = section_heading.parent\n",
        "        section_content = section.find_next_sibling('ul')\n",
        "        if section_content is None:\n",
        "            return None\n",
        "        links = []\n",
        "        for item in section_content.find_all('li'):\n",
        "            link = item.find('a')\n",
        "            if link is not None:\n",
        "                link_title = link.get('title')\n",
        "                link_url = 'https://en.wikipedia.org' + link.get('href')\n",
        "                link_description = item.text.replace(link_title, '').strip()\n",
        "                links.append({'title': link_title, 'url': link_url, 'description': link_description})\n",
        "                for sublink in item.find_all('a', href=True, recursive=False):\n",
        "                    sublink_title = sublink.get('title')\n",
        "                    sublink_url = 'https://en.wikipedia.org' + sublink.get('href')\n",
        "                    sublink_description = sublink.parent.text.replace(sublink_title, '').replace(link_title, '').strip()\n",
        "                    links.append({'title': sublink_title, 'url': sublink_url, 'description': sublink_description})\n",
        "        return links\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def get_events_links(title):\n",
        "    \"\"\"\n",
        "    Gets all the links in the \"Events\" section of a Wikipedia page.\n",
        "    Returns a list of strings containing the URLs of each event.\n",
        "    \"\"\"\n",
        "    url = f\"https://en.wikipedia.org/w/api.php?action=parse&format=json&page={title}&prop=text\"\n",
        "    response = requests.get(url)\n",
        "    data = response.json()\n",
        "\n",
        "    soup = BeautifulSoup(data['parse']['text']['*'], 'html.parser')\n",
        "    events_section = soup.find('span', {'class': 'mw-headline', 'id': 'Events'})\n",
        "    if events_section:\n",
        "        events_list = events_section.find_next('ul')\n",
        "        if events_list:\n",
        "            events = events_list.find_all('a')\n",
        "            event_links = [f\"https://en.wikipedia.org{event['href']}\" for event in events]\n",
        "            return event_links\n",
        "    return None\n",
        "\n",
        "def create_url_from_title(title):\n",
        "    \"\"\"\n",
        "    Given a list of dictionaries containing href, title, and description for each entry, extracts the title from each link and constructs a new URL using the title.\n",
        "    \"\"\"\n",
        "    urls = []\n",
        "    if title != None: \n",
        "      # Replace spaces with underscores in the title\n",
        "      title = title.replace(' ', '_')\n",
        "\n",
        "      # Construct a new URL using the title\n",
        "      url = 'https://pageviews.wmcloud.org/redirectviews/?project=en.wikipedia.org&platform=all-access&agent=user&range=all-time&sort=views&direction=1&view=list&page=' + title\n",
        "    return url\n",
        "\n",
        "def extract_visitorNumber_from_link(link):\n",
        "    \"\"\"\n",
        "    extract the first entry of the table\n",
        "    \"\"\"\n",
        "    headers, data = get_table_info(link)\n",
        "\n",
        "    if data[1]: \n",
        "      return data[1][2]\n",
        "    else:\n",
        "      return 0\n",
        "\n",
        "def get_visitor_numbers(pageviews_url):\n",
        "    \"\"\"\n",
        "    returns visitor numbers of the page\n",
        "    \"\"\"\n",
        "    response = requests.get(pageviews_url)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()['items']\n",
        "        visitors = {}\n",
        "        for item in data:\n",
        "            visitors[item['timestamp']] = item['views']\n",
        "        return visitors\n",
        "    else:\n",
        "        return None"
      ],
      "metadata": {
        "id": "5VwT46tA0CZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TEST\n",
        "\n",
        "#Seitenaufrufe\n",
        "#url = 'https://pageviews.toolforge.org/?project=en.wikipedia.org&pages=Formula1|Michael_Schumacher'\n",
        "#headers, data = get_table_info(url)\n",
        "#print(headers)\n",
        "#print(data)\n",
        "\n",
        "#get all wiki links on a page -> discard the one we dont need -> analyse the ones that could be usefull -> sort them after the backlinks\n",
        "#print(get_events_links(2004))\n",
        "\n",
        "# Search the most popular births and deaths in that year and then order them \n",
        "url1 = 'https://en.wikipedia.org/wiki/Category:1994_births'\n",
        "url2 = 'https://en.wikipedia.org/wiki/Category:1994_deaths'\n",
        "url3 = 'https://en.wikipedia.org/wiki/1994#Events'\n",
        "\n",
        "#links_events_section = get_links_in_section_with_sublinks(url3, 'Events')\n",
        "#print(len(links_events_section))\n",
        "#print(links_events_section)\n",
        "#print(get_births(1994))\n",
        "\n",
        "#visitor number test\n",
        "link = 'https://pageviews.wmcloud.org/redirectviews/?project=en.wikipedia.org&platform=all-access&agent=user&range=all-time&sort=views&direction=1&view=list&page=North_American_Free_Trade_Agreement'\n",
        "#print(extract_visitorNumber_from_link(link))"
      ],
      "metadata": {
        "id": "dYFPOhzAVotw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#When did Michael Schumacher win his first F1 World Drivers Title? -> 1994\n",
        "#First 1994 is searched on wikipedia, then all the links from the event sections of that page are filtered and written into a dict together with their pageviews.\n",
        "#Now order these lists such that the entry with most pageviews is on top\n",
        "# THIS SHOULD WORK\n",
        "\n",
        "year=2004\n",
        "url = 'https://en.wikipedia.org/wiki/2004'\n",
        "\n",
        "links_events_section = get_links_in_section_with_sublinks(url, 'Events')\n",
        "print(len(links_events_section))\n",
        "\n",
        "for link in links_events_section:\n",
        "    t_title =  link.get('title') \n",
        "    t_url = create_url_from_title(t_title)\n",
        "    link[\"pageview_url\"] = t_url\n",
        "    link[\"pageviews\"] = extract_visitorNumber_from_link(t_url)\n",
        "\n",
        "#sort the list links_event_section after the pageviews such that the entity with the highest pageviews is first\n",
        "sorted_list = sorted(links_events_section, key=lambda x: int(x['pageviews'].replace(',', '')), reverse=True)\n",
        "\n",
        "print(sorted_list) \n",
        "#print(links_events_section)"
      ],
      "metadata": {
        "id": "8hZTwIILMaNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R87Y1LPF55PL"
      },
      "source": [
        "# Clean GitHub directory\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "%rm -r optimizationOfHintGeneration\n",
        "%cd /content/\n",
        "%rm hintsLocation.txt\n",
        "%rm hintsPerson.txt\n",
        "%rm resultsEvents.txt"
      ],
      "metadata": {
        "id": "2cREJk4T5v7S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eccf6d1f-3292-430d-f382-d197831d1a54"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content\n",
            "rm: cannot remove 'hintsLocation.txt': No such file or directory\n",
            "rm: cannot remove 'hintsPerson.txt': No such file or directory\n",
            "rm: cannot remove 'resultsEvents.txt': No such file or directory\n"
          ]
        }
      ]
    }
  ]
}