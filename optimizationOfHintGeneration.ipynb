{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "cxaEE5DFfr_v",
        "WckjgVs-fZto",
        "s5rwSDisuN9s",
        "du5Un8VufnlG",
        "_lhmuuUNQ__8",
        "0Yf6Fi_34ye5",
        "0LORwHLog-t_"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexWalcher/optimizationOfHintGeneration/blob/Test/optimizationOfHintGeneration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##How to use:\n",
        "1. Run **All imports**\n",
        "2. Run **Load File**\n",
        "3. Run **New prediction method for years** or any of the **Old predictions methods**"
      ],
      "metadata": {
        "id": "sdufKBvPeQbe"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5rwSDisuN9s"
      },
      "source": [
        "# **All imports**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# This cell will not display any output\n",
        "\n",
        "%cd /content/\n",
        "%rm -r optimizationOfHintGeneration\n",
        "!git clone https://github.com/AlexWalcher/optimizationOfHintGeneration.git\n",
        "%cd /content/"
      ],
      "metadata": {
        "id": "b_jGbTrbz8x2"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# This cell will not display any output\n",
        "\n",
        "!pip install selenium\n",
        "!apt-get update \n",
        "!apt-get install firefox\n",
        "!apt install firefox-geckodriver\n",
        "\n",
        "!pip install sparqlwrapper\n",
        "!pip install pageviewapi\n",
        "!pip install sentence-transformers\n",
        "!pip install wikipedia"
      ],
      "metadata": {
        "id": "98Zh7IC7mSWI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# This cell will not display any output\n",
        "\n",
        "#new imports\n",
        "import random\n",
        "import spacy\n",
        "import time\n",
        "import requests\n",
        "import re\n",
        "import difflib\n",
        "import pprint\n",
        "import itertools\n",
        "import wikipedia\n",
        "# import wikipediaapi\n",
        "\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth',1000)\n",
        "\n",
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
        "\n",
        "import bs4\n",
        "from bs4 import BeautifulSoup, NavigableString, Tag\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "#!pip install -U pip setuptools wheel\n",
        "#!pip install -U spacy\n",
        "#!python -m spacy download en_core_web_sm\n",
        "#!python -m spacy link en_core_web_sm en\n",
        "\n",
        "from selenium import webdriver\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "from urllib.parse import urlparse, parse_qs\n",
        "from collections import OrderedDict\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "#from collections import OrderedDict\n",
        "import collections.abc as collections\n",
        "from collections.abc import Mapping\n",
        "#from collections import Mapping\n",
        "\n",
        "\n",
        "!pip install git+https://github.com/Commonists/pageview-api.git\n",
        "\n",
        "!pip install pageviewapi\n",
        "\n",
        "#import pageviewapi\n"
      ],
      "metadata": {
        "id": "chGS_8doxwwE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# This cell will not display any output\n",
        "\n",
        "!pip install Wikipedia-API\n",
        "import wikipediaapi\n"
      ],
      "metadata": {
        "id": "pTRRIW0tnxd4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load File**"
      ],
      "metadata": {
        "id": "WckjgVs-fZto"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_K8U-ZpsRss_"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.ExcelFile(\"./optimizationOfHintGeneration/testSet.xlsx\").parse(\"Sheet1\")\n",
        "x = []\n",
        "x.append(df[\"Answer\"])\n",
        "\n",
        "dataPerson = []\n",
        "dataYear = []\n",
        "dataLocation = []\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "  if(row[\"Category\"] == \"Person\"):\n",
        "    dataPerson.append([row[\"Question\"], row[\"Answer\"]])\n",
        "  elif(row[\"Category\"] == \"Year\"):\n",
        "    dataYear.append([row[\"Question\"], row[\"Answer\"]])\n",
        "  elif(row[\"Category\"] == \"Location\"):\n",
        "    dataLocation.append([row[\"Question\"], row[\"Answer\"]])\n",
        "\n",
        "person_df = pd.DataFrame(dataPerson, columns=[\"Question\", \"Answer\"])\n",
        "year_df = pd.DataFrame(dataYear, columns=[\"Question\", \"Answer\"])\n",
        "location_df = pd.DataFrame(dataLocation, columns=[\"Question\", \"Answer\"])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **New prediction method for years**"
      ],
      "metadata": {
        "id": "mKh1YgDQt3BC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dictionary for thumbcaption part of a year\n",
        "\n"
      ],
      "metadata": {
        "id": "oq8Zj9_w-aS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_table_info(url):\n",
        "    \"\"\"\n",
        "    Given a URL, this function opens the url and retrieves the information stored in a table.\n",
        "    \"\"\"\n",
        "    options = webdriver.FirefoxOptions()\n",
        "    #options.headless = True\n",
        "    options.add_argument('--headless')\n",
        "\n",
        "    driver = webdriver.Firefox(options=options)\n",
        "    driver.get(url)\n",
        "    time.sleep(5) # Wait for the page to load completely\n",
        "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "    driver.quit()\n",
        "    table = soup.find('table')\n",
        "    rows = table.find_all('tr')\n",
        "    headers = [header.text.strip() for header in rows[0].find_all('th')]\n",
        "    data = []\n",
        "    for row in rows[1:]:\n",
        "        data.append([cell.text.strip() for cell in row.find_all('td')])\n",
        "    return (headers, data)\n",
        "\n",
        "def get_table_info_requests(url):\n",
        "    \"\"\"\n",
        "    Given a URL, this function opens the url and retrieves the information stored in a table.\n",
        "    \"\"\"\n",
        "    headers = []\n",
        "    data = []\n",
        "\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    table = soup.find('table')\n",
        "    rows = table.find_all('tr')\n",
        "    headers = [header.text.strip() for header in rows[0].find_all('th')]\n",
        "    data = []\n",
        "    for row in rows[1:]:\n",
        "        data.append([cell.text.strip() for cell in row.find_all('td')])\n",
        "    return (headers, data)\n",
        "\n",
        "\n",
        "def get_links_in_section(wikipedia_url, section_heading):\n",
        "    \"\"\"\n",
        "    Given a Wikipedia URL, and a section of this article, returns an array of dictionaries containing the href, title, and description of each link on that section of the page.\n",
        "    \"\"\"\n",
        "    response = requests.get(wikipedia_url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    section = soup.find('span', {'id': section_heading})\n",
        "    if section is None:\n",
        "        return None\n",
        "    section_content = section.parent.find_next_sibling('ul')\n",
        "    if section_content is None:\n",
        "        return None\n",
        "    links = []\n",
        "    for item in section_content.find_all('li'):\n",
        "        link = item.find('a')\n",
        "        if link is not None and link.has_attr('href') and link['href'].startswith('/wiki/'):\n",
        "            title = link.get('title', '')\n",
        "            description = item.text.strip()\n",
        "            url = f\"https://en.wikipedia.org{link['href']}\"\n",
        "            links.append({'title': title, 'description': description, 'url': url})\n",
        "    print(links)\n",
        "    return links\n",
        "\n",
        "\n",
        "def get_links_in_section_with_sublinks(wikipedia_url, section_title):\n",
        "    \"\"\"\n",
        "    Given a Wikipedia URL, and a section of this article, returns an array of dictionaries containing the href, title, and description of each link  with its sublinks as well.\n",
        "    \"\"\"\n",
        "    response = requests.get(wikipedia_url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        section_heading = soup.find('span', {'id': section_title})\n",
        "        if section_heading is None:\n",
        "            return None\n",
        "        section = section_heading.parent\n",
        "        section_content = section.find_next_sibling('ul')\n",
        "        if section_content is None:\n",
        "            return None\n",
        "        links = []\n",
        "        for item in section_content.find_all('li'):\n",
        "            link = item.find('a')\n",
        "            if link is not None:\n",
        "                link_title = link.get('title')\n",
        "                link_url = 'https://en.wikipedia.org' + link.get('href')\n",
        "                link_description = item.text.replace(link_title, '').strip()\n",
        "                links.append({'title': link_title, 'url': link_url, 'description': link_description})\n",
        "                for sublink in item.find_all('a', href=True, recursive=False):\n",
        "                    sublink_title = sublink.get('title')\n",
        "                    sublink_url = 'https://en.wikipedia.org' + sublink.get('href')\n",
        "                    sublink_description = sublink.parent.text.replace(sublink_title, '').replace(link_title, '').strip()\n",
        "                    links.append({'title': sublink_title, 'url': sublink_url, 'description': sublink_description})\n",
        "        return links\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\"\"\"\n",
        "Returns all backlinks located in the thumbcaption section\n",
        "\"\"\"\n",
        "def get_wikipedia_backlinks_thumbcaption(url):\n",
        "    # Load the Wikipedia page HTML\n",
        "    page_html = requests.get(url).text\n",
        "    soup = BeautifulSoup(page_html, 'html.parser')\n",
        "\n",
        "    # Find the image caption on the page\n",
        "    caption = soup.find('div', class_='thumbcaption')\n",
        "\n",
        "    if caption is not None:\n",
        "      # Extract the caption text and any backlinks\n",
        "      backlink_sentences = {}\n",
        "      sentences = caption.text.strip().split('.')\n",
        "      for sentence in sentences:\n",
        "        links = caption.find_all('a', href=True, string=re.compile(sentence))\n",
        "        if len(links) > 0:\n",
        "            backlinks = []\n",
        "            for link in links:\n",
        "                backlink_url = 'https://en.wikipedia.org' + link['href']\n",
        "                backlink_title = link.get('title', '')\n",
        "                backlinks.append((backlink_url, backlink_title))\n",
        "            backlink_sentences[sentence] = backlinks\n",
        "    return backlink_sentences\n",
        "\n",
        "\"\"\"\n",
        "This function correctly prunes the links to only include the string after the last / character\n",
        "\"\"\"\n",
        "def prune_links(links):\n",
        "    pruned_links = []\n",
        "    for url, title in links:\n",
        "        pruned_url = url.split('/')[-1]\n",
        "        pruned_links.append((pruned_url, title))\n",
        "    return pruned_links\n",
        "\n",
        "\"\"\"\n",
        "This function first initializes an empty list combined_list that will hold the combined strings. Then, it uses a for loop to iterate through the input list in increments of 10 tuples at a time.\n",
        "For each sub-list of up to 10 tuples, it uses the list comprehension and join() method as before to combine the first elements of each tuple into a single string separated by '|'. \n",
        "Finally, the function appends each combined string to the combined_list and returns it at the end.\n",
        "\"\"\"\n",
        "def combine_first_elements(my_list):\n",
        "    combined_list = []\n",
        "    num_tuples = len(my_list)\n",
        "    for i in range(0, num_tuples, 10):\n",
        "        sub_list = my_list[i:i+10]\n",
        "        combined_str = '|'.join([tup[0] for tup in sub_list])\n",
        "        combined_list.append(combined_str)\n",
        "    return combined_list\n",
        "\n",
        "\"\"\"\n",
        "This function first initializes an empty list url_list that will hold the modified URLs. Then, it uses a for loop to iterate through each combined string in the input list. \n",
        "For each combined string, it concatenates the base URL with a forward slash and the combined string, and appends the resulting URL to the url_list. Finally, the function returns the url_list at the end.\n",
        "\"\"\"\n",
        "def add_combined_strings_to_url(base_url, combined_strings):\n",
        "    url_list = []\n",
        "    for string in combined_strings:\n",
        "        url_list.append(base_url + '/' + string)\n",
        "    return url_list\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "This will output a list of all the sentences in the thumbcaption, split by ';'.\n",
        "\"\"\"\n",
        "def get_thumbcaption_sentences(url):\n",
        "    # Get the HTML content of the page\n",
        "    page = requests.get(url)\n",
        "    soup = BeautifulSoup(page.content, 'html.parser')\n",
        "\n",
        "    # Find the thumbcaption element\n",
        "    thumbcaption = soup.find('div', class_='thumbcaption')\n",
        "\n",
        "    # Get all the sentences in the thumbcaption\n",
        "    sentences = thumbcaption.text.split('; ')\n",
        "\n",
        "    return sentences\n",
        "\n",
        "\"\"\"\n",
        "This function converts a list into a dict in the way that is needed.\n",
        "\"\"\"\n",
        "def list_to_dict(lst):\n",
        "    result = {}\n",
        "    for sublist in lst:\n",
        "        if len(sublist) >= 4:\n",
        "            key = sublist[1]\n",
        "            value = int(sublist[3].replace(',', ''))\n",
        "            result[key] = value\n",
        "    return result\n",
        "\n",
        "\"\"\"\n",
        "This function takes a list of links, opens each link to get the data, discards the header and converts the data into a dictionary using the list_to_dict() function, \n",
        "and then updates a combined dictionary with the resulting dictionary from each link. Finally, it returns the combined dictionary.\n",
        "\"\"\"\n",
        "def combine_dicts_from_links(link_list):\n",
        "    combined_dict = {}\n",
        "    for link in link_list:\n",
        "        header, data = get_table_info(link)\n",
        "        #print(data)\n",
        "        link_dict = list_to_dict(data)\n",
        "        combined_dict.update(link_dict)\n",
        "    return combined_dict\n",
        "\n",
        "\"\"\"\n",
        "This sorts the items in the dictionary based on the integer value of the second element in each key-value tuple (i.e. item[1]), in descending order (reverse=True).\n",
        "\"\"\"\n",
        "def sort_dict_desc(d):\n",
        "    return {k: v for k, v in sorted(d.items(), key=lambda item: int(item[1]), reverse=True)}\n",
        "\n",
        "\"\"\"\n",
        "This function takes in the ord dictionary and the sentences list as arguments.\n",
        "It initializes an empty list called result that we will append the found sentences to. It then iterates over each key in the ord dictionary and for each key, it iterates over each sentence in the sentences list. \n",
        "If the key is found in the sentence, the sentence is appended to the result list\n",
        "\"\"\"\n",
        "def find_sentences(ord, sentences):\n",
        "    result = []\n",
        "    for key in ord:\n",
        "        for sentence in sentences:\n",
        "            if key in sentence:\n",
        "                result.append(sentence)\n",
        "    return result\n",
        "\n",
        "\"\"\"\n",
        "This function takes a list of sentences and a keyword, removes the keyword from each sentence in the list, and returns the updated list.\n",
        "\"\"\"\n",
        "def remove_keyword(sentences, keyword):\n",
        "    updated_sentences = []\n",
        "    for sentence in sentences:\n",
        "        updated_sentence = sentence.replace(keyword, \"\")\n",
        "        updated_sentences.append(updated_sentence)\n",
        "    return updated_sentences\n",
        "\n",
        "\"\"\"\n",
        "This function takes a list of sentences and a list of keyword, removes the keyword from each sentence in the list, and returns the updated list. MAYBE NOT WORKING\n",
        "\"\"\"\n",
        "def remove_keywords(sentences, keywords):\n",
        "    \"\"\"\n",
        "    Removes one or more keywords from each sentence in the list of sentences.\n",
        "    \"\"\"\n",
        "    result = sentences\n",
        "    for entry in keywords:\n",
        "        result = remove_keyword(result, entry)\n",
        "    \n",
        "    return result\n",
        "\"\"\"\n",
        "This function prepends a given string to each sentence in a list.\n",
        "\"\"\"\n",
        "def prepend_string(sentences, prepend_str):\n",
        "    new_sentences = []\n",
        "    for sentence in sentences:\n",
        "        new_sentence = f\"{prepend_str}{sentence}\"\n",
        "        new_sentences.append(new_sentence)\n",
        "    return new_sentences\n",
        "\n",
        "\"\"\"\n",
        "This function creates a new list new_sentences and loops over each sentence in the input sentences list. \n",
        "It then uses a list comprehension and the difflib.SequenceMatcher class to compare the sentence to each sentence already in new_sentences. \n",
        "If the ratio of similarity between the two sentences is greater than 0.8 (adjust this threshold as needed), it considers the sentence to be similar and skips it. Otherwise, it adds the sentence to new_sentences. \n",
        "Finally, it returns the new list of unique sentences.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def remove_similar(sentences):\n",
        "    new_sentences = []\n",
        "    for sentence in sentences:\n",
        "        if not any(difflib.SequenceMatcher(None, sentence, s).ratio() > 0.8 for s in new_sentences):\n",
        "            new_sentences.append(sentence)\n",
        "    return new_sentences\n"
      ],
      "metadata": {
        "id": "wzL3elbnOlYl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#IMPORTANT\n",
        "def thumbcaption_hints_per_year(years_list):\n",
        "  thumbcaption_hints = {}\n",
        "  wiki_base_link= 'https://en.wikipedia.org/wiki/'\n",
        "  pageviews_range_url = 'https://pageviews.wmcloud.org/?project=en.wikipedia.org&platform=all-access&agent=user&redirects=0&range=all-time&pages='\n",
        "  \n",
        "  for y in years_list:\n",
        "    years_key = str(y)\n",
        "    test_link = wiki_base_link + years_key\n",
        "    sentences_of_thumbcaption = get_thumbcaption_sentences(test_link) #just gets the sentences from the thumbcaption section of a wikipedi years page \n",
        "    thumbcaption = get_wikipedia_backlinks_thumbcaption(test_link) #get all backlinks of the thumbcapture of the year (those are the most known events)\n",
        "    thumbcaption_key = next(iter(thumbcaption))\n",
        "    thumbcaption_val = thumbcaption[thumbcaption_key]\n",
        "    pruned = prune_links(thumbcaption_val) #prune those backlinks such that only the important part remains\n",
        "    com = combine_first_elements(pruned) #combine up to 10 of these links to create a request to pageview\n",
        "    url_list = add_combined_strings_to_url(pageviews_range_url, com) #now we have a list of pageview links with all the backlinks of the thumbcaption part of the wiki page\n",
        "    print(\"URL\")\n",
        "    print(url_list)\n",
        "    data=combine_dicts_from_links(url_list) #now we called the links and retreieved the pageviews; saved them as a dictionary\n",
        "    ord = sort_dict_desc(data) #now the list is ordered in ascending order\n",
        "    tmp = find_sentences(ord,sentences_of_thumbcaption) #search the corresponding sentence to the keyword (USA and school shooting for example)\n",
        "    #remove the years number from the hints OBVIOUSNESS\n",
        "    keywords_list = [years_key, 'clockwise ', 'Clockwise ', 'From top left', 'from top-left', 'from top left', 'From top-left', 'from top-left: ', ':', 'from left, clockwise'] #list of keywords that should be removed from the sentences\n",
        "    t4=remove_keywords(tmp, keywords_list)\n",
        "    prepend_str = 'In the same year, '\n",
        "    hints = prepend_string(t4, prepend_str)\n",
        "\n",
        "    final_hints = remove_similar(hints) #before adjusting the sentences\n",
        "    thumbcaption_hints[y] = final_hints\n",
        "\n",
        "  return thumbcaption_hints\n",
        "\n",
        "def get_year_thumbcaption_hints():\n",
        "  file_years_list = []\n",
        "  for index, row in year_df.iterrows():\n",
        "    file_years_list.append(row[\"Answer\"])\n",
        "\n",
        "  pop_thumb_hints = thumbcaption_hints_per_year(file_years_list)\n",
        "  return pop_thumb_hints\n",
        "\n",
        "#get_year_thumbcaption_hints()"
      ],
      "metadata": {
        "id": "cvcHIpCheNpL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dictionary for popular sports events of a year"
      ],
      "metadata": {
        "id": "O37okmzfPCcM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Some global varaibles for faster execution\n",
        "pop_year_hints = {}\n",
        "pop_thumb_hints = {}\n",
        "\n",
        "def get_all_tables(url):\n",
        "    \"\"\"\n",
        "    Given a URL, this function opens the url and retrieves all tables on the page.\n",
        "    \"\"\"\n",
        "    options = webdriver.FirefoxOptions()\n",
        "    options.add_argument('--headless')\n",
        "\n",
        "    driver = webdriver.Firefox(options=options)\n",
        "    driver.get(url)\n",
        "    time.sleep(1) # Wait for the page to load completely\n",
        "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "    driver.quit()\n",
        "\n",
        "    tables = soup.find_all('table')\n",
        "    all_tables = []\n",
        "    for table in tables:\n",
        "        rows = table.find_all('tr')\n",
        "        headers = [header.text.strip() for header in rows[0].find_all('th')]\n",
        "        data = []\n",
        "        for row in rows[1:]:\n",
        "            data.append([cell.text.strip() for cell in row.find_all('td')])\n",
        "        all_tables.append({'headers': headers, 'data': data})\n",
        "\n",
        "    return all_tables\n",
        "\n",
        "def get_first_elements(data_dict):\n",
        "    \"\"\"\n",
        "    Extracts the first element from each sublist in the 'data' list of lists in a dictionary and returns them in a separate list.\n",
        "    \n",
        "    Args:\n",
        "    - data_dict (dict): A dictionary containing a 'data' key with a list of lists as its value.\n",
        "    \n",
        "    Returns:\n",
        "    - A list containing the first element of each sublist in the 'data' list of lists.\n",
        "    \"\"\"\n",
        "    data_list = data_dict['data']\n",
        "    result = []\n",
        "    for sublist in data_list:\n",
        "        if len(sublist) > 0:\n",
        "            if len(sublist[0]) > 0:\n",
        "              result.append(sublist[0])\n",
        "    return result\n",
        "\n",
        "#prune the list of the hole wiki page down to the important table\n",
        "def prune_dict_list(dict_list, keyw):\n",
        "    for d in dict_list:\n",
        "        if 'headers' in d and d['headers'] == [keyw]:\n",
        "            return d\n",
        "    return None  # if no dict with the desired key-value pair is found\n",
        "\n",
        "#replace the \\n and create a list of lists \n",
        "def create_list_from_list_of_lists_key(lst, keyw):\n",
        "    result = []\n",
        "    for sublst in lst:\n",
        "        if sublst:\n",
        "          new_test = [item.replace(keyw, '') for item in sublst[0].split(keyw)]\n",
        "          result.append(new_test)\n",
        "    return result\n",
        "\n",
        "#replace the : and create a dict; {'year': 'CL-winner'}\n",
        "def create_dict_from_list_of_lists(lst):\n",
        "    result_dict = {}\n",
        "    for sublist in lst:\n",
        "        for item in sublist:\n",
        "            if \":\" in item:\n",
        "                key, value = item.split(\":\")\n",
        "                key = key.strip().replace(\"–\", \"-\")\n",
        "                value = value.strip()\n",
        "                result_dict[key] = value\n",
        "            else:\n",
        "                continue\n",
        "    return result_dict\n",
        "\n",
        "#split on the : and creates a dict\n",
        "def create_dict_from_list(lst):\n",
        "    result = {}\n",
        "    for item in lst:\n",
        "        parts = item.split(\":\")\n",
        "       # parts = parts.strip().replace(\":\", \"\")\n",
        "        result[parts[0]] = parts[1]\n",
        "    return result\n",
        "\n",
        "#splits on [ and creates a dict 1950: 'Farina'\n",
        "def get_year_with_driver(race_results):\n",
        "    results_dict = {}\n",
        "    for result in race_results:\n",
        "        if result:\n",
        "            results_dict[result[0].split(\"[\")[0]] = result[1]\n",
        "    return results_dict\n",
        "\n",
        "#get rid of the links ('Alberto Ascari[20]' => 'Alberto Ascari')\n",
        "def clean_driver_names(results_dict):\n",
        "    for year, driver in results_dict.items():\n",
        "        results_dict[year] = driver.split('[')[0].strip()\n",
        "    return results_dict\n",
        "\n",
        "#deletes the : from the key\n",
        "def clean_dict_keys(dict_to_clean):\n",
        "    cleaned_dict = {}\n",
        "    for key, value in dict_to_clean.items():\n",
        "        cleaned_dict[key.rstrip(':')] = value\n",
        "    return cleaned_dict\n",
        "\n",
        "#function to split on \\n but that lets the city names stay together\n",
        "def create_city_dict(city_list):\n",
        "    city_dict = {}\n",
        "    cities = city_list[0].split('\\n')\n",
        "    for city in cities:\n",
        "        if city:\n",
        "            year, *city_name = city.strip().split()\n",
        "            city_dict[year] = ' '.join(city_name)\n",
        "    return city_dict\n",
        "\n",
        "\n",
        "#get the dict of all the champions league winners \n",
        "def champions_league_winners_list():\n",
        "    champions_league_url = 'https://en.wikipedia.org/wiki/List_of_European_Cup_and_UEFA_Champions_League_finals#List_of_finals'\n",
        "    all = get_all_tables(champions_league_url) #gets all tables of wiki page\n",
        "    #first prune of that huge dict\n",
        "    keyw= 'showvteEuropean Cup and UEFA Champions League winners'\n",
        "    pruned_dict = prune_dict_list(all,keyw)\n",
        "    #second prune\n",
        "    inter = pruned_dict.get('data') \n",
        "    cl_list = inter[2:5] + inter[7:11]\n",
        "    tmp1 = create_list_from_list_of_lists_key(cl_list, '\\n')\n",
        "    tmp2 = create_dict_from_list_of_lists(tmp1)\n",
        "\n",
        "    return tmp2\n",
        "\n",
        "#get the dict of all the euros winners \n",
        "def uefa_euros_winners_list():\n",
        "    euros_url = 'https://en.wikipedia.org/wiki/List_of_UEFA_European_Championship_finals#List_of_finals'\n",
        "    all = get_all_tables(euros_url) #gets all tables of wiki page\n",
        "    #first prune of that huge dict\n",
        "    keyw= 'showvteUEFA European Championship winners'\n",
        "    pruned_dict = prune_dict_list(all,keyw)\n",
        "    #second prune\n",
        "    inter = pruned_dict.get('data') \n",
        "    tmp1 = create_list_from_list_of_lists_key(inter, '\\n')\n",
        "    tmp2 = create_dict_from_list_of_lists(tmp1)\n",
        "\n",
        "    return tmp2\n",
        "\n",
        "#get the dict of all the wold cup winners \n",
        "def uefa_worlds_winners_list():\n",
        "    worlds_url = 'https://en.wikipedia.org/wiki/List_of_FIFA_World_Cup_finals#List_of_final_matches'\n",
        "    all = get_all_tables(worlds_url) #gets all tables of wiki page\n",
        "    tmp3=get_first_elements(all[3])\n",
        "    #first prune of that huge dict\n",
        "    keyw= 'showvteFIFA World Cup'\n",
        "    pruned_dict = prune_dict_list(all,keyw)\n",
        "    #second prune\n",
        "    inter = pruned_dict.get('data') \n",
        "    inter = inter[2]\n",
        "    years = [s for s in inter[0].split('\\n')]\n",
        "    my_dict = dict(zip(years, tmp3))\n",
        "\n",
        "    return my_dict\n",
        "\n",
        "#get the dict of all the F1 drivers world champions\n",
        "def f1_winners_list():\n",
        "    euros_url = 'https://en.wikipedia.org/wiki/List_of_Formula_One_World_Drivers%27_Champions#By_season'\n",
        "    all = get_all_tables(euros_url) #gets all tables of wiki page\n",
        "    lst = all[2].get('data')\n",
        "    tmp2 = get_year_with_driver(lst)\n",
        "    tmp2 = clean_driver_names(tmp2)\n",
        "\n",
        "    return tmp2\n",
        "\n",
        "#get the dict of all summer olympics host cities\n",
        "def summer_olympics_hosts_list():\n",
        "    summerO_url = 'https://en.wikipedia.org/wiki/Summer_Olympic_Games#List_of_Summer_Olympic_Games'\n",
        "    all = get_all_tables(summerO_url) #gets all tables of wiki page\n",
        "    lst = all[10].get('data')\n",
        "    tmp1 = create_list_from_list_of_lists_key(lst, '\\n')\n",
        "    tmp1 = tmp1[0]\n",
        "    tmp1 = create_dict_from_list(tmp1)\n",
        "    tmp1 = clean_dict_keys(tmp1)\n",
        "    tmp1 = clean_driver_names(tmp1)\n",
        "\n",
        "    return tmp1\n",
        "\n",
        "#get the dict of all winter olympics host cities\n",
        "def winter_olympics_hosts_list():\n",
        "    winterO_url = 'https://en.wikipedia.org/wiki/Winter_Olympic_Games#List_of_Winter_Olympic_Games'\n",
        "    all = get_all_tables(winterO_url) #gets all tables of wiki page\n",
        "    lst = all[8].get('data')\n",
        "    tmp1 = create_list_from_list_of_lists_key(lst, '\\n')\n",
        "    tmp1 = tmp1[0]\n",
        "    tmp1 = create_dict_from_list(tmp1)\n",
        "    tmp1 = clean_dict_keys(tmp1)\n",
        "    tmp1 = clean_driver_names(tmp1)\n",
        "\n",
        "    return tmp1"
      ],
      "metadata": {
        "id": "ARMx75Z0QkVh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The basic sentences out of which we create the hint-sentences\n",
        "basic_sentences =  ['In the same year, ', 'In the previous year, ', 'In the following year, ']\n",
        "sport_sentences = [' has won the UEFA Champions League.', ' has won the UEFA Euro Football Championship.', ' has won the FIFA World Cup.', ' has won the F1 Drivers World Championship.', ]\n",
        "olympic_sentences = ['In the same year, the Summer Olympics were held in ', 'In the previous year, the Summer Olympics were held in ', 'In the following year, the Summer Olympics were held in ', 'In the same year, the Winter Olympics were held in ', 'In the previous year, the Winter Olympics were held in ', 'In the following year, the Winter Olympics were held in ']\n",
        "\n",
        "#write all of the winners of the different sports categories into lists\n",
        "cl_all = champions_league_winners_list() #For the Champions league, \n",
        "euro_all = uefa_euros_winners_list() #For Football-Euros\n",
        "worlds_all = uefa_worlds_winners_list() #For Football-Worlds\n",
        "f1_all = f1_winners_list() #For Fromula1\n",
        "summer_olympics_all = summer_olympics_hosts_list() #For OlympicSummerGames \n",
        "winter_olympics_all = winter_olympics_hosts_list() #For OlympicWinterGames \n",
        "\n",
        "'''\n",
        "takes a list of years and then creates a dict of dicts, where (if available) the most popular sports events of that year are saved as hints.\n",
        "returns a dict with the corresponding sports events from the years in years_list\n",
        "'''\n",
        "def popular_sports_per_year(years_list):\n",
        " \n",
        "  pop_sport_hints_year = {}\n",
        "  for index in years_list:\n",
        "    year = index\n",
        "    year_s = str(year)\n",
        "\n",
        "    year_dict = {\n",
        "        'cl': '', 'p_cl': '', 'f_cl': '',\n",
        "        'euros': '', 'p_euros': '', 'f_euros': '',\n",
        "        'worlds': '', 'p_worlds': '', 'f_worlds': '',\n",
        "        'f1': '', 'p_f1': '', 'f_f1': '',\n",
        "        'summer': '', 'p_summer': '', 'f_summer': '',\n",
        "        'winter': '', 'p_winter': '', 'f_winter': '', \n",
        "        }\n",
        "\n",
        "  # UEFA Champions League: Create the sentences like (In the same-, the following-, the previous-year)\n",
        "    for key in cl_all:\n",
        "      if int(key.split('-')[1]) == year % 100:\n",
        "        result = cl_all[key]\n",
        "        break\n",
        "    if result:\n",
        "      year_dict['cl'] = basic_sentences[0] + result + sport_sentences[0] \n",
        "    for key in cl_all:\n",
        "      if int(key.split('-')[1]) == (year - 1) % 100:\n",
        "        result = cl_all[key]\n",
        "        break\n",
        "    if result:\n",
        "      year_dict['p_cl'] = basic_sentences[1] + result + sport_sentences[0] \n",
        "    for key in cl_all:\n",
        "      if int(key.split('-')[1]) == (year + 1) % 100:\n",
        "        result = cl_all[key]\n",
        "        break\n",
        "    if result:\n",
        "      year_dict['f_cl'] = basic_sentences[2] + result + sport_sentences[0]       \n",
        "\n",
        "  # UEFA EURO Football Championship: Create the sentences like (In the same-, the following-, the previous-year)\n",
        "    for d in euro_all:\n",
        "      if year_s in d:\n",
        "        year_dict['euros'] = basic_sentences[0] + euro_all[year_s] + sport_sentences[1]       \n",
        "    for d in euro_all:\n",
        "      t = year - 1\n",
        "      year_int = str(t)\n",
        "      if year_int in d:\n",
        "        year_dict['p_euros'] = basic_sentences[1] + euro_all[year_int] + sport_sentences[1]\n",
        "    for d in euro_all:\n",
        "      t = year + 1\n",
        "      year_int = str(t)\n",
        "      if year_int in d:\n",
        "        year_dict['f_euros'] = basic_sentences[2] + euro_all[year_int] + sport_sentences[1]\n",
        "\n",
        "  # FIFA WORLD Football Championship: Create the sentences like (In the same-, the following-, the previous-year)\n",
        "    for d in worlds_all:\n",
        "      if year_s in d:\n",
        "        year_dict['worlds'] = basic_sentences[0] + worlds_all[year_s] + sport_sentences[2]\n",
        "    for d in worlds_all:\n",
        "      t = year - 1\n",
        "      year_int = str(t)\n",
        "      if year_int in d:\n",
        "        year_dict['p_worlds'] = basic_sentences[1] + worlds_all[year_int] + sport_sentences[2]\n",
        "    for d in worlds_all:\n",
        "      t = year + 1\n",
        "      year_int = str(t)\n",
        "      if year_int in d:\n",
        "        year_dict['f_worlds'] = basic_sentences[2] + worlds_all[year_int] + sport_sentences[2]\n",
        "\n",
        "  # F1 WORLD Drivers Championship: Create the sentences like (In the same-, the following-, the previous-year)\n",
        "    for d in f1_all:\n",
        "      if year_s in d:\n",
        "        year_dict['f1'] = basic_sentences[0] + f1_all[year_s] + sport_sentences[3]\n",
        "    for d in f1_all:\n",
        "      t = year - 1\n",
        "      year_int = str(t)\n",
        "      if year_int in d:\n",
        "        year_dict['p_f1'] = basic_sentences[1] + f1_all[year_int] + sport_sentences[3]  \n",
        "    for d in f1_all:\n",
        "      t = year + 1\n",
        "      year_int = str(t)\n",
        "      if year_int in d:\n",
        "        year_dict['f_f1'] = basic_sentences[2] + f1_all[year_int] + sport_sentences[3]    \n",
        "\n",
        "  # Summer Olympic Games: Create the sentences like (In the same-, the following-, the previous-year)\n",
        "    for d in summer_olympics_all:\n",
        "      if year_s in d:\n",
        "        year_dict['summer'] = olympic_sentences[0] + summer_olympics_all[year_s] \n",
        "    for d in summer_olympics_all:\n",
        "      t = year - 1\n",
        "      year_int = str(t)\n",
        "      if year_int in d:\n",
        "        year_dict['p_summer'] = olympic_sentences[1] + summer_olympics_all[year_int] \n",
        "    for d in summer_olympics_all:\n",
        "      t = year + 1\n",
        "      year_int = str(t)\n",
        "      if year_int in d:\n",
        "        year_dict['f_summer'] = olympic_sentences[2] + summer_olympics_all[year_int]  \n",
        "\n",
        "  # Winter Olympic Games: Create the sentences like (In the same-, the following-, the previous-year)\n",
        "    for d in winter_olympics_all:\n",
        "      if year_s in d:\n",
        "        year_dict['winter'] = olympic_sentences[3] + winter_olympics_all[year_s] \n",
        "    for d in winter_olympics_all:\n",
        "      t = year - 1\n",
        "      year_int = str(t)\n",
        "      if year_int in d:\n",
        "        year_dict['p_winter'] = olympic_sentences[4] + winter_olympics_all[year_int] \n",
        "    for d in winter_olympics_all:\n",
        "      t = year + 1\n",
        "      year_int = str(t)\n",
        "      if year_int in d:\n",
        "        year_dict['f_winter'] = olympic_sentences[5] + winter_olympics_all[year_int]  \n",
        "\n",
        "    #write the entry in the dict\n",
        "    pop_sport_hints_year[year] = year_dict\n",
        "  \n",
        "  return pop_sport_hints_year\n",
        "\n",
        "#CALL FUNCTION\n",
        "#returns a dictionary of the years that occured in the xls file, with its corresponding hints from the most popular sports events of that year.\n",
        "def get_year_sports_hints():\n",
        "  file_years_list = []\n",
        "  for index, row in year_df.iterrows():\n",
        "    file_years_list.append(row[\"Answer\"])\n",
        "  pop_sport_hints = popular_sports_per_year(file_years_list)\n",
        "  \n",
        "  return pop_sport_hints\n"
      ],
      "metadata": {
        "id": "kuWB4hrxPCcV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Load pre-trained model and tokenizer\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Tokenize the text and add special tokens\n",
        "    tokens = tokenizer.encode(text, add_special_tokens=True)\n",
        "    # Convert the tokens to a tensor\n",
        "    token_tensor = torch.tensor(tokens).unsqueeze(0)\n",
        "    return token_tensor\n",
        "\n",
        "def get_similarity_score(text1, text2):\n",
        "    # Preprocess both texts\n",
        "    tensor1 = preprocess_text(text1)\n",
        "    tensor2 = preprocess_text(text2)\n",
        "\n",
        "    # Pass both tensors to the model to get the embeddings\n",
        "    with torch.no_grad():\n",
        "        output1 = model(tensor1)\n",
        "        output2 = model(tensor2)\n",
        "    \n",
        "    # Compute the cosine similarity between the two embeddings\n",
        "    cosine_sim = torch.nn.functional.cosine_similarity(output1.last_hidden_state.mean(dim=1), output2.last_hidden_state.mean(dim=1), dim=1)\n",
        "    return cosine_sim.item()"
      ],
      "metadata": {
        "id": "OVdCk8gQQifE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test it!"
      ],
      "metadata": {
        "id": "6vIkujY8y8Dj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create the hints for the questions where the answer is a year and print them\n",
        "#if len(pop_year_hints) == 0 and len(pop_thumb_hints) == 0:\n",
        "pop_year_hints = get_year_sports_hints()\n",
        "pop_thumb_hints = get_year_thumbcaption_hints()\n",
        "\n",
        "years_hints = {}\n",
        "\n",
        "for y in pop_year_hints:\n",
        "  year_dict = {\n",
        "      'sports': pop_year_hints[y],\n",
        "      'thumbcaption': pop_thumb_hints[y]\n",
        "  }\n",
        "\n",
        "  years_hints[y] = year_dict\n",
        "\n",
        "generated_hints_for_years = years_hints\n",
        "pprint.pprint(generated_hints_for_years, indent=1)\n",
        "\n",
        "'''\n",
        "#Test for utility score of new questions; calculate score via BERT for each question,hint pair and write the score together with the question into the sim_scores dictionary.\n",
        "'''\n",
        "\n",
        "qa_dict = dict(zip(year_df['Answer'], year_df['Question']))\n",
        "sim_scores = years_hints\n",
        "\n",
        "for y, q in qa_dict.items():\n",
        "  for year, data in years_hints.items():\n",
        "    if y == year:\n",
        "      for category, subdata in data.items():\n",
        "        #sim_scores[year]['question'] = q\n",
        "        if category != 'thumbcaption':\n",
        "          for key, value in subdata.items():\n",
        "            sim_scores[year][category][key] = {}\n",
        "            similarity_score = get_similarity_score(q,value)\n",
        "            sim_scores[year][category][key][value] = similarity_score\n",
        "        else:\n",
        "          for i in subdata:\n",
        "            sim_scores[year][category] = {}\n",
        "            similarity_score = get_similarity_score(q,i)\n",
        "            sim_scores[year][category][i] = similarity_score\n",
        "    else:\n",
        "      continue\n",
        "\n",
        "for y, q in qa_dict.items():\n",
        "  for year, data in years_hints.items():\n",
        "    if y == year:\n",
        "      sim_scores[year]['question'] = q\n",
        "\n",
        "pprint.pprint(sim_scores, indent=1)"
      ],
      "metadata": {
        "id": "-Pcx5uKity9M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9c4b782-4490-4aa0-bbab-64a8c0f4b082"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{}\n",
            "{}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dictionary for thumbcaption part of a year\n",
        "\n"
      ],
      "metadata": {
        "id": "-sY3op9nbu0X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **New prediction Methods for people:**"
      ],
      "metadata": {
        "id": "98m65vq_tyWL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to create some sort of metric to explain how popular or well-known a entity is, to give better hints. Therefore we look at how many options there are (how many entries in a certain category) and at how diverse these options are. If a category for example has 100s of entries, then it probably isn't a good base for a hint because there are many to choose from.\n",
        "\n"
      ],
      "metadata": {
        "id": "uRT1_8HpefRE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reusable functions to retrieve, compare and modify the needed data "
      ],
      "metadata": {
        "id": "EtAXhTJq6NK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Given a list of Wikipedia category names, creates the corresponding Wikipedia category links.\n",
        "Args: categories (list): A list of Wikipedia category names to create links for.\n",
        "Returns: category_links (list): A list of Wikipedia category links corresponding to the input categories.\n",
        "\"\"\"\n",
        "def get_category_links(categories):\n",
        "  category_links = []\n",
        "  for category in categories:\n",
        "    category_link = \"https://en.wikipedia.org/wiki/Category:\" + category.replace(\" \", \"_\")\n",
        "    #category_link = \"https://wikipedia.org/wiki/Category:\" + category.replace(\" \", \"_\")\n",
        "    category_links.append(category_link)\n",
        "  return category_links\n",
        "\n",
        "\"\"\"\n",
        "Given a list of Wikipedia category names, creates the corresponding Wikipedia category links.\n",
        "Args: categories (list): A list of Wikipedia category names to create links for.\n",
        "Returns: category_links (list): A list of Wikipedia category links corresponding to the input categories.\n",
        "\"\"\"\n",
        "def get_category_with_underscores(categories):\n",
        "  \n",
        "  category_links = []\n",
        "  for category in categories:\n",
        "    category_link = category.replace(\" \", \"_\")\n",
        "    category_links.append(category_link)\n",
        "  return category_links\n",
        "\n",
        "\"\"\"\n",
        "Given a list of Wikipedia category names, retrieves the number of entries in each category and returns a dictionary\n",
        "with the category names as keys and the entry counts as values.\n",
        "Args: categories (list): A list of Wikipedia category names to retrieve entry counts for.\n",
        "Returns: category_entry_counts (dict): A dictionary of Wikipedia category names and their respective entry counts.\n",
        "\"\"\"\n",
        "def get_category_entry_counts(categories):\n",
        "  category_links = get_category_links(categories)\n",
        "  category_entry_counts = {}\n",
        "\n",
        "  for i, category_link in enumerate(category_links):\n",
        "    response = requests.get(category_link)\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "    try:\n",
        "      entry_count_text = soup.find(\"div\", {\"id\": \"catlinks\"}).find_all(\"a\")[1].text.strip()\n",
        "      entry_count = int(entry_count_text.split()[-2])\n",
        "      category_name = categories[i]\n",
        "      if category_name:\n",
        "        category_entry_counts[category_name] = entry_count\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "  return category_entry_counts\n",
        "\n",
        "# SPLIT OF 1. CELL\n",
        "# rewrite with wikiapi\n",
        "\"\"\"\n",
        "Retrieves the categories of a Wikipedia page using the wikipediaapi package.\n",
        "Args: title (str): The title of the Wikipedia page.\n",
        "Returns: categories (list): A list of categories associated with the page.\n",
        "\"\"\"\n",
        "def get_wikipedia_categories(title):\n",
        "  wikipedia = wikipediaapi.Wikipedia(\"en\")\n",
        "  page = wikipedia.page(title)\n",
        "  #print(page)\n",
        "  if not page.exists():\n",
        "    return []\n",
        "  categories = [c for c in page.categories]\n",
        "  return [cat.split(\":\")[1] for cat in categories]\n",
        "\n",
        "# SPLIT OF 2. CELL\n",
        "\n",
        "def get_category_subcategories(link):\n",
        "  #print(link)\n",
        "  # Create a Wikipedia API object\n",
        "  wiki_api = wikipediaapi.Wikipedia('en')\n",
        "  # Extract the category name from the link\n",
        "  category_name = link.split('/')[-1]\n",
        "  # Retrieve the category page\n",
        "  category_page = wiki_api.page(f\"Category:{category_name}\")\n",
        "  # Find the subcategories section of the page\n",
        "  subcategories_section = category_page.categorymembers\n",
        "  # Extract the subcategories from the section\n",
        "  subcategories = []\n",
        "  i=0\n",
        "  for subcategory in subcategories_section.values():\n",
        "    if i < 100: #threshold for how many entries per category\n",
        "      i +=1\n",
        "      if subcategory.ns == wikipediaapi.Namespace.CATEGORY:\n",
        "        subcategories.append(subcategory.fullurl)\n",
        "  return subcategories\n",
        "\n",
        "\n",
        "def get_category_pages(category_title, limit=100):\n",
        "  params = {\n",
        "    \"action\": \"query\",\n",
        "    \"format\": \"json\",\n",
        "    \"list\": \"categorymembers\",\n",
        "    \"cmtitle\": category_title,\n",
        "    \"cmlimit\": str(limit) # limit to 100 entries\n",
        "  }\n",
        "  #print(\"OK\")\n",
        "  pages = []\n",
        "  while True:\n",
        "    response = requests.get(\"https://en.wikipedia.org/w/api.php\", params=params).json()\n",
        "    pages += [p[\"title\"] for p in response[\"query\"][\"categorymembers\"]]\n",
        "    if \"continue\" in response:\n",
        "      params.update(response[\"continue\"])\n",
        "    else:\n",
        "      break\n",
        "    if len(pages) >= limit: # break the loop if the number of entries exceeds 100\n",
        "      break\n",
        "  return pages[:limit] # return only the first 100 entries\n",
        "\n",
        "# SPLIT OF 3. CELL\n",
        "\n",
        "\"\"\"\n",
        "Given a list of Wikipedia category names, retrieves the number of entries in each category and returns a dictionary\n",
        "with the category names as keys and the entry counts as values.\n",
        "Args: searched_location: The desired location for wich a hint should be created. (The answer to the question)\n",
        "Returns: dicto (dict): A dictionary of all categories with the subcategories and how many entries there are in each.\n",
        "\"\"\"\n",
        "def get_cat_with_all_subcats(searched_location):\n",
        "  #retrieves the list of categories from the location-wikipedia page\n",
        "  categories = get_wikipedia_categories(searched_location)\n",
        "  categories_with_underscore = get_category_with_underscores(categories)\n",
        "  categories_links = get_category_links(categories)\n",
        "\n",
        "  categories_with_links_dict = {}\n",
        "  for i in range(len(categories_with_underscore)):\n",
        "    key = categories_with_underscore[i]\n",
        "    link = categories_links[i]\n",
        "    categories_with_links_dict[key] = link\n",
        "\n",
        "  cat_with_subcats_dict = {}\n",
        "  for category in categories_links:\n",
        "    sub_cats = get_category_subcategories(category)\n",
        "    #print(category)\n",
        "    ct = get_category_title(category)\n",
        "    #print(ct)\n",
        "    pages_list = get_category_pages(ct)\n",
        "    #print(pages_list)\n",
        "    filtered_list = [str(entry) for entry in pages_list if not entry.startswith(\"Category:\")]\n",
        "    new_list = [len(filtered_list), filtered_list]\n",
        "    if sub_cats is None:\n",
        "      continue\n",
        "    else:\n",
        "      cat_with_subcats_dict[category] = [[len(sub_cats), sub_cats], new_list]\n",
        "\n",
        "  return cat_with_subcats_dict\n",
        "\n",
        "#input a url of a category, this returns the tilte\n",
        "def get_category_title(category_url):\n",
        "  parts = category_url.split('/')\n",
        "  title = [part for part in parts if part.startswith('Category:')]\n",
        "  if title:\n",
        "    return title[0]\n",
        "  else:\n",
        "    return None\n",
        "\n",
        "#input searched location and returns a dict with number of pages and number of subcategories\n",
        "def get_categories_ranking(searched_location):\n",
        "  categories = get_wikipedia_categories(searched_location)\n",
        "  categories_links = []\n",
        "  cat_without_articles = []\n",
        "\n",
        "  bad_list = ['Articles with', 'CS1', 'Wikipedia', 'Webarchive', 'Short', 'Biography', 'Commons', 'Pages', 'Use', 'All', 'Articles', 'Coordinates', 'Engvar', 'Lang', 'Official']\n",
        "\n",
        "  for c in categories:\n",
        "    if not any(c.startswith(word) for word in bad_list):\n",
        "      cat_without_articles.append(c)  \n",
        "\n",
        "  categories_links = get_category_links(cat_without_articles)\n",
        "  cat_with_amount = {}\n",
        "\n",
        "  for category in categories_links:\n",
        "    sub_cats = get_category_subcategories(category)\n",
        "    ct = get_category_title(category)\n",
        "    pages_list = get_category_pages(ct)\n",
        "\n",
        "    if sub_cats is None:\n",
        "      continue\n",
        "    else:\n",
        "      cat_with_amount[category] =  len(pages_list), len(sub_cats)\n",
        "  #x[0] sort after subcats and x[1] sorts after pages\n",
        "  sorted_dict = dict(sorted(cat_with_amount.items(), key=lambda x: x[1], reverse=True))\n",
        "  return sorted_dict\n",
        "\n",
        "#NEW\n",
        "#extract the category part from the wiki links\n",
        "def extract_last_parts(links):\n",
        "  last_parts = []\n",
        "  for link in links:\n",
        "    last_part = link.split('/')[-1]\n",
        "    last_parts.append(last_part)\n",
        "  return last_parts\n",
        "\n",
        "#function to concatenate the category links to insert into a pageviews url\n",
        "def concatenate_elements(elements, n=10):\n",
        "  result = []\n",
        "  for i in range(0, len(elements), n):\n",
        "    group = elements[i:i+n]\n",
        "    result.append('|'.join(group))\n",
        "  return result\n",
        "\n",
        "\"\"\"\n",
        "This function first initializes an empty list url_list that will hold the modified URLs. Then, it uses a for loop to iterate through each combined string in the input list. \n",
        "For each combined string, it concatenates the base URL with a forward slash and the combined string, and appends the resulting URL to the url_list. Finally, the function returns the url_list at the end.\n",
        "\"\"\"\n",
        "def combine_pv_urls(base_url, combined_strings):\n",
        "    url_list = []\n",
        "    for strin in combined_strings:\n",
        "      url_list.append(base_url  + strin)\n",
        "    return url_list\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "This function takes a list of links, opens each link to get the data, discards the header and converts the data into a dictionary using the list_to_dict() function, \n",
        "and then updates a combined dictionary with the resulting dictionary from each link. Finally, it returns the combined dictionary.\n",
        "\"\"\"\n",
        "def combine_dicts_from_links(link_list):\n",
        "  combined_dict = {}\n",
        "  for link in link_list:\n",
        "    header, data = get_table_info(link)\n",
        "    #print(data)\n",
        "    link_dict = list_to_dict(data)\n",
        "    combined_dict.update(link_dict)\n",
        "  return combined_dict\n",
        "\n",
        "# SPLIT OF 4. CELL\n",
        "\n",
        "def add_values_to_links(links_dict, values_dict):\n",
        "  new_dict = {}\n",
        "  for link, value in links_dict.items():\n",
        "    key = link.split(':')[-1]\n",
        "    if key in values_dict:\n",
        "      value += (values_dict[key],)\n",
        "    new_dict[link] = value\n",
        "  return new_dict\n",
        "\n",
        "def combine_catnumbers_pvs(ord_dict, norm_dict):\n",
        "  for key in ord_dict:\n",
        "    if key in norm_dict:\n",
        "      ord_dict[key] = ord_dict[key] + (norm_dict[key],)\n",
        "      #print(ord_dict)\n",
        "  return ord_dict\n",
        "\n",
        "def add_values_to_linkss(links_dict, values_dict):\n",
        "  new_dict = {}\n",
        "  for loc, ord_list in links_dict.items():\n",
        "    for loc2, ord_list_pv in values_dict.items():\n",
        "      if loc == loc2:\n",
        "        new_dict[loc] = combine_catnumbers_pvs(ord_list, ord_list_pv)\n",
        "        #new_dict[loc] = combine_catnumbers_pvs(ord_list_pv, ord_list)\n",
        "  return new_dict\n",
        "\n",
        "#combines the pageviews of the categories of the location together with the sub-categories and pages of those subcategories\n",
        "def combine_pv_cats(cat_dict, pv_dict):\n",
        "  tmp = cat_dict\n",
        "\n",
        "  for key, value in cat_dict.items():\n",
        "    for key2, value2 in pv_dict.items():\n",
        "      category_name = key.split('/')[-1]\n",
        "      new_string = key2.replace(' ', '_')\n",
        "\n",
        "      if category_name == new_string:\n",
        "        #print(category_name)\n",
        "        org_tup = cat_dict[key]\n",
        "        #new_tup = org_tup + (value2, 0)\n",
        "        new_tup = org_tup + (value2,)\n",
        "        tmp[key] = new_tup\n",
        "\n",
        "        if len(new_tup) != 3:\n",
        "          print(\"PROBLEM\\n\")\n",
        "\n",
        "  return tmp\n",
        "\n",
        "#combines the pageviews of the categories of the location together with the sub-categories and pages of those subcategories\n",
        "def get_dict_for_every_location(cat_ranking, cat_with_pv):\n",
        "  ret=cat_ranking\n",
        "\n",
        "  for location, value in cat_ranking.items():\n",
        "    for location1, value1 in cat_with_pv.items():\n",
        "      if location == location1:\n",
        "        tmp = combine_pv_cats(value, value1)\n",
        "        ret[location] = tmp\n",
        "  return ret"
      ],
      "metadata": {
        "id": "8Iqf7G5usErf"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipediaapi\n",
        "#import pageviewapi\n",
        "\n",
        "def get_categories(subject_dict):\n",
        "  categories_for_subject_dict= {}\n",
        "  rankings_for_categories_dict = {}\n",
        "  #creates a dict with all the dicts of each location with its categories and sub-categories\n",
        "  for subject, question in subject_dict.items():\n",
        "    dicto = get_cat_with_all_subcats(subject)\n",
        "    categories_for_subject_dict[subject] = dicto\n",
        "    ranking = get_categories_ranking(subject)\n",
        "    ordict = OrderedDict(ranking)\n",
        "    rankings_for_categories_dict[subject] = ordict\n",
        "\n",
        "  return rankings_for_categories_dict\n",
        "\n",
        "#given a dictionary with all the categories, the function returns the categories in a OrderedDict with the corresponding pageviews for each category\n",
        "def get_pageviews_for_categories(cat_dict):\n",
        "  pageviews_range_url = 'https://pageviews.wmcloud.org/?project=en.wikipedia.org&platform=all-access&agent=user&redirects=0&range=last-year&pages='\n",
        "  \n",
        "  all_cats_with_pvs = {}\n",
        "\n",
        "  for subject in cat_dict:\n",
        "    ord_dict = OrderedDict()\n",
        "    ordered_dict_sub =  cat_dict[subject]\n",
        "    links_list = [link for link in ordered_dict_sub.keys()]\n",
        "    #print(links_list)\n",
        "    pruned_link_parts_list = extract_last_parts(links_list)\n",
        "    concat_str_for_links = concatenate_elements(pruned_link_parts_list) #combine up to 10 of these links to create a request to pageview\n",
        "    pageviews_url_list = combine_pv_urls(pageviews_range_url, concat_str_for_links) #now we have a list of pageview links with all the backlinks of the thumbcaption part of the wiki page (REUSED FROM YEARS PART)\n",
        "    #print(pageviews_url_list)\n",
        "    categories_with_pageviews =combine_dicts_from_links(pageviews_url_list) #now we called the links and retreieved the pageviews; saved them as a dictionary\n",
        "    #print(categories_with_pageviews)\n",
        "    ordered_categories_with_pageviews = sort_dict_desc(categories_with_pageviews) #now the list is ordered in ascending order\n",
        "    #all_cats_with_pvs[loc] = OrderedDict(ordered_categories_with_pageviews)\n",
        "    all_cats_with_pvs[subject] = OrderedDict(categories_with_pageviews)\n",
        "  return all_cats_with_pvs\n",
        "\n",
        "def sorting_dict(sor_dict):\n",
        "  ret = {}\n",
        "  l2 = sor_dict\n",
        "  for loc, value in l2.items():\n",
        "    try:\n",
        "      sorted_dict = OrderedDict(sorted(value.items(), key=lambda x: x[1][2], reverse=True))\n",
        "    except Exception as e:\n",
        "      print(f\"Sorting failed for location {loc}: {e}\")\n",
        "      sorted_dict = value\n",
        "    ret[loc] = sorted_dict\n",
        "    #sorted_dict = dict(sorted(value.items(), key=lambda x: x[2], reverse=True))\n",
        "  return ret\n",
        "\n",
        "\n",
        "from collections import OrderedDict\n",
        "# function that takes a dictionary with an ordered dictionary as the value and prunes the ordered dictionary to keep only the first n entries:\n",
        "def prune_ordered_dict(dictionary, n):\n",
        "  pruned_dict = OrderedDict()\n",
        "  for key, value in dictionary.items():\n",
        "    pruned_dict[key] = OrderedDict(list(value.items())[:n])\n",
        "  return pruned_dict\n",
        "\n",
        "\n",
        "# function that takes a dictionary with an ordered dictionary as the value and prunes the ordered dictionary to keep only the first n entries and deltes certain categories:\n",
        "def prune_and_ordered_dict(dictionary, n):\n",
        "  pruned_dict = OrderedDict()\n",
        "  inter1_dict= OrderedDict()\n",
        "  for key, value in dictionary.items():\n",
        "    inter3_dict= OrderedDict()\n",
        "    for link, tuplee in value.items():\n",
        "      link_str = str(link)\n",
        "      if 'Living_people' not in link_str and '_births' not in link_str and '_deaths' not in link_str and 'Good_articles' not in link_str and 'Members' not in link_str:\n",
        "        inter3_dict[link] = tuplee\n",
        "    pruned_dict[key] = inter3_dict\n",
        "\n",
        "  for key, value in pruned_dict.items():\n",
        "    inter1_dict[key] = OrderedDict(list(value.items())[:n])\n",
        "  return inter1_dict\n",
        "\n",
        "\n",
        "#find the 20 most appearing categories \n",
        "def find_most_common_links(data_dict):\n",
        "  link_count = {}\n",
        "  for key, value in data_dict.items():\n",
        "    for link in value:\n",
        "      if link not in link_count:\n",
        "        link_count[link] = {\"count\": 1, \"keys\": [key]}\n",
        "      else:\n",
        "        link_count[link][\"count\"] += 1\n",
        "        link_count[link][\"keys\"].append(key)\n",
        "\n",
        "  # Sort the links by their count in descending order\n",
        "  sorted_links = sorted(link_count.items(), key=lambda x: x[1][\"count\"], reverse=True)\n",
        "\n",
        "  # Return a list of tuples with the link, count, and keys\n",
        "  return [(link, data[\"count\"], data[\"keys\"]) for link, data in sorted_links[:20]]"
      ],
      "metadata": {
        "id": "XfUS-hxe3mPX"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template_sentence_location = 'The location you are looking for, is a member of the category x'\n",
        "template_sentence_location2 = 'The location you are looking for, belongs to the category '\n",
        "\n",
        "template_sentence_person = 'The person you are looking for, is a member of the category x'\n",
        "template_sentence_person2 = 'The person you are looking for, belongs to the category '\n",
        "\n",
        "import itertools\n",
        "import requests\n",
        "\n",
        "def get_categories_with_pageviews_person(person_questions_dict):\n",
        "  cat_ranking_person = get_categories(person_questions_dict)\n",
        "  cat_with_pv_person = get_pageviews_for_categories(cat_ranking_person)\n",
        "  #for person: sorting the dict after pages per category\n",
        "  categories_with_subs_and_pageviews_person = get_dict_for_every_location(cat_ranking_person, cat_with_pv_person)\n",
        "  #pprint.pprint(categories_with_subs_and_pageviews_person,indent=1)\n",
        "  new_ordered_dict_person = sorting_dict(categories_with_subs_and_pageviews_person)\n",
        "  return new_ordered_dict_person\n",
        "\n",
        "#retrives the category name from the link\n",
        "def get_category_name(link):\n",
        "  category_prefix = \"Category:\"\n",
        "  if link.startswith(\"https://wikipedia.org/wiki/\"):\n",
        "    return link[len(\"https://wikipedia.org/wiki/\" + category_prefix):].replace(\"_\", \" \")\n",
        "  else:\n",
        "    if link.startswith(\"https://en.wikipedia.org/wiki/\"):\n",
        "      return link[len(\"https://en.wikipedia.org/wiki/\" + category_prefix):].replace(\"_\", \" \")\n",
        "    else:\n",
        "      return None\n",
        "      \n",
        "#just get the first 3 categories without any special formula\n",
        "def get_first_three_categs_location(copy_categories_with_subs_and_pageviews_location):\n",
        "  first_three_categories_per_location = {}\n",
        "  for key, value in copy_categories_with_subs_and_pageviews_location.items():\n",
        "    first_three = dict(itertools.islice(value.items(), 3))\n",
        "    first_three_categories_per_location[key] = first_three\n",
        "\n",
        "  hint_sentence_location = {}\n",
        "  for key, value in first_three_categories_per_location.items():\n",
        "    inter_hints = []\n",
        "    for link, tup in value.items():\n",
        "      cat_name=get_category_name(link)\n",
        "      if cat_name is not None:\n",
        "        ok = template_sentence_location2 + cat_name\n",
        "      else: \n",
        "        ok = template_sentence_location2\n",
        "      inter_hints.append(ok)\n",
        "    hint_sentence_location[key] = inter_hints\n",
        "  return hint_sentence_location\n",
        "\n",
        "#just get the first 3 categories without any special formula\n",
        "def get_first_three_categs_person(copy_categories_with_subs_and_pageviews_location):\n",
        "  first_three_categories_per_location = {}\n",
        "  for key, value in copy_categories_with_subs_and_pageviews_location.items():\n",
        "    first_three = dict(itertools.islice(value.items(), 3))\n",
        "    first_three_categories_per_location[key] = first_three\n",
        "\n",
        "  hint_sentence_location = {}\n",
        "  for key, value in first_three_categories_per_location.items():\n",
        "    inter_hints = []\n",
        "    for link, tup in value.items():\n",
        "      cat_name=get_category_name(link)\n",
        "      if cat_name is not None:\n",
        "        ok = template_sentence_person2 + cat_name\n",
        "      else: \n",
        "        ok = template_sentence_person2\n",
        "      inter_hints.append(ok)\n",
        "    hint_sentence_location[key] = inter_hints\n",
        "  return hint_sentence_location\n",
        "\n",
        "#get predicates of a wiki page via wikidataAPI\n",
        "def get_wikidata_predicates(page_title):\n",
        "    # First, get the Wikidata item ID of the page\n",
        "    url = f\"https://en.wikipedia.org/w/api.php?action=query&prop=pageprops&ppprop=wikibase_item&redirects=1&titles={page_title}&format=json\"\n",
        "    response = requests.get(url).json()\n",
        "    pages = response[\"query\"][\"pages\"]\n",
        "    if \"-1\" in pages:\n",
        "        return None\n",
        "    page_id = next(iter(pages))\n",
        "    wikidata_id = pages[page_id][\"pageprops\"][\"wikibase_item\"]\n",
        "\n",
        "    # Then, get the predicates and their values of the Wikidata item\n",
        "    url = f\"https://www.wikidata.org/w/api.php?action=wbgetentities&ids={wikidata_id}&format=json&props=claims\"\n",
        "    response = requests.get(url).json()\n",
        "    entity = response[\"entities\"][wikidata_id]\n",
        "    predicates = {}\n",
        "    for claim_id, claim in entity[\"claims\"].items():\n",
        "        predicate_id = claim[\"mainsnak\"][\"property\"]\n",
        "        predicate_value = claim[\"mainsnak\"][\"datavalue\"][\"value\"]\n",
        "        predicates[predicate_id] = predicate_value\n",
        "    return predicates\n",
        "\n",
        "#TEST\n",
        "people_list=[]\n",
        "for l in dataPerson:\n",
        "  people_list.append(l[1])\n",
        "\n",
        "# takes a list of all the people and the list of category occurences where all the categories that were assigned to these wiki-pages \n",
        "# are listed and ranked after how often they occur; \n",
        "# output: a list where the person is the key and the entries show wich categories the entity shares with wich other entitities and how many they are\n",
        "def get_people_dict(people_list, occurences_list):\n",
        "  people_dict = {}\n",
        "  for person in people_list:\n",
        "    person_categories = []\n",
        "    for occurence in occurences_list:\n",
        "      if person in occurence[2]:\n",
        "        person_categories.append((occurence[0], occurence[1], occurence[2]))\n",
        "    people_dict[person] = person_categories\n",
        "  return people_dict\n",
        "\n",
        "#returns a dict where for each person in people_list we calculate how often they occur in the same category as the keys in person_dict \n",
        "# (where already for each person we looked at each of the categories he appears and counted how many other people from people_list appear in these )\n",
        "def count_people_occurrences(person_dict, people_list):\n",
        "  # Initialize an empty dictionary to hold the counts  \n",
        "  count_dict = {}\n",
        "  # Loop through all pairs of drivers and count the number of occurrences\n",
        "  for key, value in person_dict.items():\n",
        "    counter = {}\n",
        "    for person in people_list:\n",
        "      counter[person] = 0\n",
        "      for cat in value:\n",
        "        if person in cat[2]:\n",
        "          counter[person] += 1  \n",
        "    count_dict[key] = counter\n",
        "  return count_dict\n",
        "\n",
        "#sorting\n",
        "def sort_dict_by_value_desc(d):\n",
        "  # Sort the inner dictionary by value in descending order\n",
        "  sorted_dict = OrderedDict(sorted(d.items(), key=lambda x: x[1], reverse=True))\n",
        "  return sorted_dict\n",
        "\n",
        "def get_categories_union(overlap_dict, people_list):\n",
        "  # Initialize an empty dictionary to hold the counts  \n",
        "  count_dict = {}\n",
        "  for key, value in overlap_dict.items():\n",
        "    counter = {}\n",
        "    for item, number in value.items():\n",
        "      #print(number)\n",
        "      counter[item] = number_categories_per_person[key] + number_categories_per_person[item] - number\n",
        "    count_dict[key] = counter\n",
        "  return count_dict\n",
        "\n",
        "def get_avg_pairwise_sim(cat_div_union, cat_div_overlap, number_categories_per_person, people_list):\n",
        "  count_dict = {}\n",
        "  for key, value in cat_div_union.items():\n",
        "    inter = {}\n",
        "    for key1, value1 in cat_div_overlap.items():\n",
        "      if key == key1:\n",
        "        for name, number in value.items():\n",
        "          for name1, number1 in value1.items():\n",
        "            if name == name1:\n",
        "              if not number  or not number1:\n",
        "                inter[name] = 0\n",
        "              else:\n",
        "                inter[name] = (number1 / number)\n",
        "    count_dict[key] = inter\n",
        "  return count_dict\n",
        "\n",
        "def get_cat_diversity(shared_categories, copy_new_ordered_dict_person):\n",
        "  count_dict = {}\n",
        "  for key, value in copy_new_ordered_dict_person.items():\n",
        "    for item in value.items():\n",
        "      pv = 0\n",
        "      #print(item)\n",
        "      link = item[0]\n",
        "      trip = item[1]\n",
        "      if len(trip) == 3:\n",
        "        pv = trip[2]\n",
        "      if link not in count_dict:\n",
        "        count_dict[link] = pv\n",
        "  return count_dict\n",
        "\n",
        "\n",
        "#---- SPLIT ----\n",
        "\n",
        "#shared_categories; number_categories_per_person; unsorted_category_diversity; cat_div_overlap; unsorted_cat_div_union; cat_div_union; unordered_avg_pairwise_sim; avg_pairwise_sim; unordered_cat_popularity;cat_popularity\n",
        "\n",
        "def get_categories_with_ranking():\n",
        "  person_questions_dict = dict(zip(person_df['Answer'], person_df['Question']))\n",
        "  cat_with_pv_person = {}\n",
        "\n",
        "  cat_ranking_person = get_categories(person_questions_dict)\n",
        "  cat_with_pv_person = get_pageviews_for_categories(cat_ranking_person)\n",
        "  categories_with_subs_and_pageviews_person = get_dict_for_every_location(cat_ranking_person, cat_with_pv_person)\n",
        "  new_ordered_dict_person = sorting_dict(categories_with_subs_and_pageviews_person)\n",
        "\n",
        "  return new_ordered_dict_person\n",
        "\n",
        "#all_people_cat_ranked = get_categories_with_ranking() #copy_new_ordered_dict_person\n",
        "\n",
        "import wikipediaapi\n",
        "import requests\n",
        "\n",
        "#given a name, retrieve the infomration in the short-description part of the wiki page\n",
        "def get_page_short_description(page_title):\n",
        "  # Prepare the API request URL\n",
        "  url = f\"https://en.wikipedia.org/api/rest_v1/page/summary/{page_title.replace(' ', '_')}\"\n",
        "  # Send the API request\n",
        "  response = requests.get(url)\n",
        "  data = response.json()\n",
        "  # Extract the short description from the API response\n",
        "  short_description = data.get('description', '')\n",
        "  return short_description\n",
        "\n",
        "def find_most_similar_category(persons, specific_category, num_similar_categories=3):\n",
        "  most_similar_categories = {}\n",
        "\n",
        "  for person, categories in persons.items():\n",
        "    category_texts = [category_link.split('/')[-1].replace('_', ' ') for category_link in categories.keys()]\n",
        "    category_texts.append(specific_category[person])  # Add the specific category for comparison\n",
        "\n",
        "    # Vectorize the category texts\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(category_texts)\n",
        "    #print(category_texts)\n",
        "    # Calculate cosine similarity between the specific category and other categories\n",
        "    similarities = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1])[0]\n",
        "      # Calculate cosine similarity between the specific category and other categories\n",
        "    similarities = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1])[0]\n",
        "    # Find the most similar categories textually\n",
        "    similar_indices = similarities.argsort()[-num_similar_categories:][::-1]\n",
        "    similar_categories = [category_texts[index] for index in similar_indices]\n",
        "    most_similar_categories[person] = similar_categories\n",
        "  return most_similar_categories\n",
        "\n",
        "\n",
        "def get_work_category(similar_categories):\n",
        "  person_with_work_categories = {}\n",
        "  person_with_work_category = {}\n",
        "  strings_to_check = [\"births\", \"deaths\"]\n",
        "  for person, categories in similar_categories.items():\n",
        "    for strs in strings_to_check:\n",
        "      for cats in categories:\n",
        "        if strs in cats:\n",
        "          categories.remove(cats)\n",
        "        else:\n",
        "          continue\n",
        "      person_with_work_categories[person] = categories\n",
        "  for person, categories in person_with_work_categories.items():\n",
        "    person_with_work_category[person] = categories[0]\n",
        "  return person_with_work_category\n",
        "\n",
        "\n",
        "def get_container_categories(work_cats):\n",
        "  for person, categories in work_cats.items():\n",
        "    wiki_category = categories\n",
        "    base_url = 'https://en.wikipedia.org/w/api.php'\n",
        "    params = {\n",
        "        'action': 'query',\n",
        "        'titles': wiki_category,\n",
        "        'prop': 'categories',\n",
        "        'format': 'json',\n",
        "        'cllimit': 'max'\n",
        "    }\n",
        "    response = requests.get(base_url, params=params)\n",
        "    data = response.json()\n",
        "    page_id = next(iter(data['query']['pages'].keys()))\n",
        "    categories = data['query']['pages'][page_id].get('categories', [])\n",
        "    container_categories = []\n",
        "    for category in categories:\n",
        "        if category['title'].startswith('Category:'):\n",
        "            container_categories.append(category['title'][9:])\n",
        "\n",
        "    strings_to_check = [\"CatAutoTOC \", \"Commons\", \"Template\"]\n",
        "    for strs in strings_to_check:\n",
        "      for cats in container_categories:\n",
        "        if strs in cats:\n",
        "          container_categories.remove(cats)\n",
        "        else:\n",
        "          continue\n",
        "    work_cats[person] = container_categories\n",
        "  return work_cats\n",
        "\n",
        "def replace_spaces_with_underscore(dictionary):\n",
        "  updated_dict = {}\n",
        "  for key, value in dictionary.items():\n",
        "    updated_dict[key] = value.replace(\" \", \"_\")\n",
        "  return updated_dict\n",
        "\n",
        "def format_category_dict(category_dict):\n",
        "  formatted_dict = {}\n",
        "  for key, value in category_dict.items():\n",
        "    if isinstance(value, list):\n",
        "      formatted_value = [f\"Category:{v.replace(' ', '_')}\" for v in value]\n",
        "    else:\n",
        "      formatted_value = f\"Category:{value.replace(' ', '_')}\"\n",
        "    formatted_dict[key] = formatted_value\n",
        "  return formatted_dict\n",
        "\n",
        "\n",
        "#---- SPLIT ----\n",
        "\n",
        "#person_with_job; similar_categories; work_category; container_category_work; test2\n",
        "\n",
        "\"\"\"\n",
        "Functions to retrieve the occupation of a person\n",
        "\n",
        "The wikipedia-api library does not provide a direct method to retrieve the infobox of a Wikipedia page. \n",
        "However, we can use beautifulsoup4 to parse the HTML content of the Wikipedia page and extract the infobox.\n",
        "\"\"\"\n",
        "\n",
        "#function that retrieves the infobox using beautifulsoup4\n",
        "def get_infobox_from_wikipedia(page_title):\n",
        "  # Format the page title for the Wikipedia URL\n",
        "  formatted_title = page_title.replace(' ', '_')\n",
        "  # Construct the Wikipedia page URL\n",
        "  url = f\"https://en.wikipedia.org/wiki/{formatted_title}\"\n",
        "  # Send a GET request to the Wikipedia page\n",
        "  response = requests.get(url)\n",
        "  # Check if the page exists\n",
        "  if response.status_code != 200:\n",
        "    print(f\"Page '{page_title}' does not exist.\")\n",
        "    return None\n",
        "  # Create a BeautifulSoup object to parse the page content\n",
        "  soup = BeautifulSoup(response.content, 'html.parser')\n",
        "  # Find the infobox section of the page\n",
        "  infobox = soup.find(class_='infobox')\n",
        "  return infobox\n",
        "\n",
        "#function that retrieves the infobox from a Wikipedia page and specifically extracts the entries from the \"occupation\" or \"occupations\" field:\n",
        "def get_occupations_from_infobox(page_title):\n",
        "  # Retrieve the infobox from the Wikipedia page\n",
        "  infobox = get_infobox_from_wikipedia(page_title)\n",
        "  # Check if the infobox exists\n",
        "  if not infobox:\n",
        "    return []\n",
        "  # Search for the \"occupation\" or \"occupations\" field in the infobox\n",
        "  occupation_keywords = ['occupation', 'occupations', 'Occupation', 'Occupations']\n",
        "  occupations = []\n",
        "  for keyword in occupation_keywords:\n",
        "    field = infobox.find(string=keyword)\n",
        "    if field:\n",
        "      # Retrieve the occupation entries from the field\n",
        "      tmp_str = field.find_next(\"td\").find_next(\"div\").find_next(\"ul\")\n",
        "      from_table = extract_list_elements(tmp_str)\n",
        "      from_field = field.find_next(\"td\").text.strip()\n",
        "      from_field = from_field.split()\n",
        "      if len(from_table) > 0:\n",
        "        for i in from_table:\n",
        "          occupations.append(i)\n",
        "      elif len(from_field) > 0:\n",
        "        for i in from_field:\n",
        "          occupations.append(i)\n",
        "      else:\n",
        "        print(\"problem\")\n",
        "  return occupations\n",
        "\n",
        "#extract the elements between the <li> tags\n",
        "def extract_list_elements(html_string):\n",
        "  occupations_list = []\n",
        "  li_entries_list = []\n",
        "  html_str = str(html_string)\n",
        "  html_str = html_str.replace(\"</ul>\", \"\")\n",
        "  html_str = html_str.replace(\"<ul>\", \"\")\n",
        "  html_str = html_str.replace(\"</li>\", \"\")\n",
        "  li_entries_list = html_str.split(\"<li>\")\n",
        "  for i in li_entries_list:\n",
        "    if len(i) > 0: #check if empty\n",
        "      if len(i) < 16: #check if entry is a link\n",
        "        occupations_list.append(i)\n",
        "  return occupations_list\n",
        "\n",
        "#searches the occupation for every entry in people list\n",
        "def get_occupations(people_list):\n",
        "  occupation_person_dict = {}\n",
        "  for person in people_list:\n",
        "    occupation_person_dict[person] = get_occupations_from_infobox(person) \n",
        "  return occupation_person_dict\n",
        "\n",
        "#people_list; occupation_person_dict\n",
        "\n",
        "#---- SPLIT ----\n",
        "\n",
        "#retrieves all of the relate links of a wiki page\n",
        "def get_related_links(wiki_link):\n",
        "  # Extract the page title from the Wikipedia link\n",
        "  title = wiki_link.split('/')[-1]\n",
        "  # Format the API URL to get the page content\n",
        "  api_url = f'https://en.wikipedia.org/w/api.php?action=query&titles={title}&prop=links&pllimit=max&format=json'\n",
        "  # Send a GET request to the API\n",
        "  response = requests.get(api_url)\n",
        "  if response.status_code == 200:\n",
        "    data = response.json()\n",
        "    # Extract the page ID from the API response\n",
        "    page_id = next(iter(data['query']['pages']))\n",
        "    # Check if the page exists and has links\n",
        "    if page_id != '-1' and 'links' in data['query']['pages'][page_id]:\n",
        "      # Retrieve the links from the API response\n",
        "      links = data['query']['pages'][page_id]['links']\n",
        "      # Extract the link titles and URLs\n",
        "      related_links = [{'url': f'https://en.wikipedia.org/wiki/{link[\"title\"]}', 'title': link['title']} for link in links]\n",
        "      return related_links\n",
        "  return []\n",
        "\n",
        "# function to check if the title consists of two words -> discrad a lot of entries for performance reasons\n",
        "def filter_two_word_titles(links):\n",
        "  filtered_links = []\n",
        "  url_concat = \"\"\n",
        "  for link in links:\n",
        "    url = link['url']\n",
        "    link['url'] = url.replace(' ', '_')\n",
        "    title = link['title']\n",
        "    words = title.split()\n",
        "    if len(words) == 2:\n",
        "      filtered_links.append(link)\n",
        "\n",
        "  return filtered_links\n",
        "\n",
        "#check if a given Wikipedia link corresponds to an entity with the \"instance of\" (P31) property set to \"human\" (Q5), indicating it represents a person\n",
        "def is_person_page(link):\n",
        "  # Extract the page title from the link\n",
        "  title = link['title']\n",
        "  # Query Wikidata API to check if the page corresponds to a person\n",
        "  wikidata_url = f\"https://www.wikidata.org/w/api.php?action=wbgetentities&format=json&sites=enwiki&titles={title}\"\n",
        "  response = requests.get(wikidata_url).json()\n",
        "  # Check if the response contains any entities\n",
        "  if 'entities' in response:\n",
        "    entities = response['entities']\n",
        "    # Check if there is an entity with P31 (instance of) set to Q5 (human)\n",
        "    for entity_id, entity in entities.items():\n",
        "      if 'claims' in entity and 'P31' in entity['claims']:\n",
        "        for claim in entity['claims']['P31']:\n",
        "          if claim['mainsnak']['datavalue']['value']['id'] == 'Q5':\n",
        "            return True\n",
        "  return False\n",
        "\n",
        "#function that calls the is_perso_page() function for every link in the pre discraded list\n",
        "def check_if_person(links):\n",
        "  person_links= []\n",
        "  for link in links:\n",
        "    if is_person_page(link):\n",
        "      person_links.append(link)\n",
        "  return person_links\n",
        "\n",
        "#function that selects 5 entries at random from a list of related people:\n",
        "def select_random_entries(related_people_list, num_entries=5):\n",
        "  random_entries = random.sample(related_people_list, num_entries)\n",
        "  return random_entries\n",
        "\n",
        "# takes a list of all the people and the list of category occurences where all the categories that were assigned to these wiki-pages \n",
        "# are listed and ranked after how often they occur; \n",
        "# output: a list where the person is the key and the entries show wich categories the entity shares with wich other entitities and how many they are\n",
        "def get_people_dict(people_list, occurences_list):\n",
        "  people_dict = {}\n",
        "  for person in people_list:\n",
        "    person_categories = []\n",
        "    for occurence in occurences_list:\n",
        "      if person in occurence[2]:\n",
        "        person_categories.append((occurence[0], occurence[1], occurence[2]))\n",
        "    people_dict[person] = person_categories\n",
        "  return people_dict\n",
        "\n",
        "#wiki_link; related_articles; filtered_links; related_people_list; select_random_people; \n",
        "\n",
        "\n",
        "def get_most_known_related_people(related_people_list):\n",
        "  pageviews_range_url = 'https://pageviews.wmcloud.org/?project=en.wikipedia.org&platform=all-access&agent=user&redirects=0&range=last-year&pages='\n",
        "  ord_dict = OrderedDict()\n",
        "  link_list = []\n",
        "  for item in related_people_list:\n",
        "    link_list.append(item['url'])\n",
        "  pruned_link_parts_list = extract_last_parts(link_list)\n",
        "  concat_str_for_links = concatenate_elements(pruned_link_parts_list) #combine up to 10 of these links to create a request to pageview\n",
        "  pageviews_url_list = combine_pv_urls(pageviews_range_url, concat_str_for_links) #now we have a list of pageview links with all the backlinks of the thumbcaption part of the wiki page (REUSED FROM YEARS PART)\n",
        "  categories_with_pageviews =combine_dicts_from_links(pageviews_url_list) #now we called the links and retreieved the pageviews; saved them as a dictionary\n",
        "  ordered_categories_with_pageviews = sort_dict_desc(categories_with_pageviews) #now the list is ordered in ascending order\n",
        "  ord_dict = ordered_categories_with_pageviews\n",
        "\n",
        "  return ord_dict\n",
        "\n",
        "#tester; \n",
        "#related_people_ranked; top_most_popular_people; cat_ranking_related_person; cat_with_pv_person; categories_with_subs_and_pageviews_person\n",
        "#new_ordered_dict_related_person; copy_new_ordered_dict_person_test; ordered_dict_related_person\n",
        "\n",
        "#---- SPLIT ----\n",
        "\n",
        "\n",
        "def get_ordered_categories_of_most_related_people(most_related_peoples_list):\n",
        "  #tester = get_most_known_related_people()\n",
        "  ret = {}\n",
        "  for k,v in most_related_peoples_list.items():\n",
        "    related_people_ranked = dict(sorted(v.items(), key=lambda x: x[1], reverse=True))\n",
        "    top_most_popular_people = dict(itertools.islice(related_people_ranked.items(), 5))#take the top 5 most known people from the list\n",
        "    \n",
        "    cat_ranking_related_person = get_categories(top_most_popular_people)\n",
        "    print(cat_ranking_related_person)\n",
        "    cat_with_pv_person = get_pageviews_for_categories(cat_ranking_related_person)\n",
        "    print(cat_with_pv_person)\n",
        "    #categories_with_subs_and_pageviews_person = get_dict_for_every_location(cat_ranking_person, cat_with_pv_person)\n",
        "    categories_with_subs_and_pageviews_person = get_dict_for_every_location(cat_ranking_related_person, cat_with_pv_person)\n",
        "    print(categories_with_subs_and_pageviews_person)\n",
        "\n",
        "    new_ordered_dict_related_person = sorting_dict(categories_with_subs_and_pageviews_person)\n",
        "    \n",
        "    copy_new_ordered_dict_person_test = prune_and_ordered_dict(new_ordered_dict_related_person, 20)\n",
        "    ordered_dict_related_person = sorting_dict(copy_new_ordered_dict_person_test)\n",
        "    ret[key] = ordered_dict_related_person\n",
        "  return ret\n",
        "\n",
        "#get the links of all answer-entities via link creation of the key\n",
        "def get_rel_pep():\n",
        "  person_questions_dict = dict(zip(person_df['Answer'], person_df['Question']))\n",
        "  wiki_link = 'https://en.wikipedia.org/wiki/'\n",
        "  ret = {}\n",
        "\n",
        "  for key,value in person_questions_dict.items():\n",
        "    modified_text = key.replace(' ', '_')\n",
        "    link = f\"{wiki_link}/{modified_text}\" \n",
        "    related_articles = get_related_links(link)\n",
        "    filtered_links = filter_two_word_titles(related_articles)\n",
        "    related_people_list = check_if_person(filtered_links)\n",
        "    ret[key] = related_people_list\n",
        "  return ret\n",
        "\n",
        "#a; inter; intermediate\n",
        "\n",
        "#get the related people with the categories\n",
        "def get_related_with_categories():\n",
        "  ret = {}\n",
        "  person_questions_dict = dict(zip(person_df['Answer'], person_df['Question']))\n",
        "\n",
        "  cat_ranking_person = get_categories(person_questions_dict)\n",
        "  cat_with_pv_person = get_pageviews_for_categories(cat_ranking_person)\n",
        "  categories_with_subs_and_pageviews_person = get_dict_for_every_location(cat_ranking_person, cat_with_pv_person)\n",
        "  new_ordered_dict_person = sorting_dict(categories_with_subs_and_pageviews_person)\n",
        "\n",
        "  for key,value in new_ordered_dict_person.items():\n",
        "    ret[key] = value\n",
        "    \n",
        "  return ret"
      ],
      "metadata": {
        "id": "srvRakz78Jsn"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Print and Test the functions!"
      ],
      "metadata": {
        "id": "-q_xTwdBHXTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#for locations\n",
        "\n",
        "location_questions_dict = dict(zip(location_df['Answer'], location_df['Question']))\n",
        "cat_with_pv_location = {}\n",
        "\n",
        "#for locations\n",
        "cat_ranking_location = get_categories(location_questions_dict)\n",
        "#pprint.pprint(cat_ranking_location, indent=1)\n",
        "cat_with_pv_location = get_pageviews_for_categories(cat_ranking_location)\n",
        "#pprint.pprint(cat_with_pv_location, indent=1)\n",
        "#for locations: sorting the dict after pages per category\n",
        "categories_with_subs_and_pageviews_location = get_dict_for_every_location(cat_ranking_location, cat_with_pv_location)\n",
        "#pprint.pprint(categories_with_subs_and_pageviews_location,indent=1)\n",
        "new_ordered_dict_location = sorting_dict(categories_with_subs_and_pageviews_location)\n",
        "#pprint.pprint(new_ordered_dict_location,indent=1)\n",
        "\n",
        "#for 3 locations 1m33s"
      ],
      "metadata": {
        "id": "sJ7C8z2j_Qqi"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for person\n",
        "person_questions_dict = dict(zip(person_df['Answer'], person_df['Question']))\n",
        "cat_with_pv_person = {}\n",
        "\n",
        "#print(person_questions_dict)\n",
        "#del str\n",
        "\n",
        "#for person\n",
        "cat_ranking_person = get_categories(person_questions_dict)\n",
        "#pprint.pprint(cat_ranking_person, indent=1)\n",
        "cat_with_pv_person = get_pageviews_for_categories(cat_ranking_person)\n",
        "#pprint.pprint(cat_with_pv_person, indent=1)\n",
        "#for person: sorting the dict after pages per category\n",
        "categories_with_subs_and_pageviews_person = get_dict_for_every_location(cat_ranking_person, cat_with_pv_person)\n",
        "#pprint.pprint(categories_with_subs_and_pageviews_person,indent=1)\n",
        "new_ordered_dict_person = sorting_dict(categories_with_subs_and_pageviews_person)\n",
        "#pprint.pprint(new_ordered_dict_person,indent=1)\n",
        "\n",
        "#for 3 people 17m22s\n",
        "#27s\n",
        "# for 20 F1 drivers -> 20mins"
      ],
      "metadata": {
        "id": "k1_R-UOV1ZYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#just for easy printing such that you dont have to0 calculate everything everytime\n",
        "copy_categories_with_subs_and_pageviews_location = categories_with_subs_and_pageviews_location\n",
        "copy_new_ordered_dict_location = new_ordered_dict_location\n",
        "#print(\"Ordered after the number of pages per category:\")\n",
        "#pprint.pprint(copy_categories_with_subs_and_pageviews_location,indent=1)\n",
        "print(\"Ordered after the number of pageviews per category:\")\n",
        "pprint.pprint(copy_new_ordered_dict_location,indent=1, compact=True)\n",
        "\n",
        "copy_categories_with_subs_and_pageviews_person = categories_with_subs_and_pageviews_person\n",
        "copy_new_ordered_dict_person = new_ordered_dict_person\n",
        "#print(\"Ordered after the number of pages per category:\")\n",
        "#pprint.pprint(copy_categories_with_subs_and_pageviews_person,indent=1)\n",
        "print(\"Ordered after the number of pageviews per category:\")\n",
        "#prune the dict down to 20 most visited categories per person\n",
        "#copy_new_ordered_dict_person = prune_ordered_dict(copy_new_ordered_dict_person, 20)\n",
        "#first delete categories like (livingpeople, deaths, births) and prune the dict down to 20 most visited categories per person\n",
        "copy_new_ordered_dict_person_test = prune_and_ordered_dict(copy_new_ordered_dict_person, 20)\n",
        "copy_new_ordered_dict_person = sorting_dict(copy_new_ordered_dict_person_test)\n",
        "pprint.pprint(copy_new_ordered_dict_person,indent=1, compact=True)\n",
        "\n",
        "number_categories_per_person = {}\n",
        "for key, value in copy_new_ordered_dict_person.items():\n",
        "    number_categories_per_person[key] = len(value)\n",
        "\n",
        "#8.30 for 10 drivers\n",
        "category_occurences = find_most_common_links(copy_new_ordered_dict_person)\n",
        "#print('These are the categories that occur most between the person-entities:')\n",
        "#pprint.pprint(category_occurences, indent=1, compact=True)"
      ],
      "metadata": {
        "id": "mJj4vnEC6TKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions for the unexpected categories approach:"
      ],
      "metadata": {
        "id": "Wl3CKiM1lVqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"\n",
        "1. retrieve all of the related links of a wiki-persons-page (first 500);\n",
        "2. pre prune the list, such that only entries with 2 words in the title are left; (for performance reasons we assume 2 words equaly is human)\n",
        "3. analyse the links that are left via wikipedia APi and the \"instance of\" (P31) property set to \"human\" (Q5)\n",
        "4. now we have a list of related people to compare to our answer-person-entity\n",
        "5.1 retrieve the categories of the answer_entities (top 10-20)\n",
        "5.2 retrieve the categories of those related people as done above with the answer_entities (top 10-20)\n",
        "\n",
        "6. compare the categories of the related-people of the corresponding answer-entity and list the ones that appear most often (between answer-entity=A AND related_people of A )\n",
        "7. look for the \"unexpected\", a popular category that is well known and where from a list of related_people our answer-entity is the only one that appears in\n",
        "\n",
        "8. create some metrics and threshholds -> testing\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "Function that retrieves the related/popular people of the answer-entities;\n",
        "Args:     the answer-entity name \n",
        "Returns:  a list where each entry of the list is a dictionary; the keys and values are the (title and url) of the related people\n",
        "\"\"\"\n",
        "def get_related_people_from_person_name(person_name):\n",
        "  wiki_link = 'https://en.wikipedia.org/wiki'\n",
        "\n",
        "  modified_text = person_name.replace(' ', '_') #replace spaces with underscores in the name to use it in link\n",
        "  link = f\"{wiki_link}/{modified_text}\" \n",
        "  print(link)\n",
        "  related_articles = get_related_links(link)\n",
        "  filtered_links = filter_two_word_titles(related_articles)\n",
        "  related_people_list = check_if_person(filtered_links)\n",
        "  \n",
        "  return related_people_list\n",
        "\n",
        "\"\"\"\n",
        "Function that gets the pageviews of up to 10 links at a time;\n",
        "Args:     dictionary where the keys are the answer-entities and the value is a link_list \n",
        "Returns:  dictionary where the keys are the answer-entities and the value is a list with the links and pageviews\n",
        "\"\"\"\n",
        "def get_pageviews_from_links(link_dict):\n",
        "  pageviews_range_url = 'https://pageviews.wmcloud.org/?project=en.wikipedia.org&platform=all-access&agent=user&redirects=0&range=last-year&pages='\n",
        "  return_dict = {}\n",
        "\n",
        "  for key,value in link_dict.items():\n",
        "    link_list = value\n",
        "    \n",
        "    pruned_link_parts_list = extract_last_parts(link_list)\n",
        "    concat_str_for_links = concatenate_elements(pruned_link_parts_list)             #combine up to 10 of these links to create a request to pageview\n",
        "    pageviews_url_list = combine_pv_urls(pageviews_range_url, concat_str_for_links) #now we have a list of pageview links with all the backlinks of the thumbcaption part of the wiki page (REUSED FROM YEARS PART)\n",
        "    categories_with_pageviews =combine_dicts_from_links(pageviews_url_list)         #now we called the links and retreieved the pageviews; saved them as a dictionary\n",
        "    ordered_categories_with_pageviews = sort_dict_desc(categories_with_pageviews)   #now the list is ordered in ascending order\n",
        "    \n",
        "    return_dict[key] = ordered_categories_with_pageviews\n",
        "  \n",
        "  return return_dict\n",
        "\n",
        "\"\"\"\n",
        "Function that takes the 5 most popular related-people from the related_people_pageviews_dict and  \n",
        "retrieves the corresponding wiki-categories with the their pageviews;\n",
        "Args:     dictionary where the keys are the answer-entities and the value is a dict of related people with their pageviews \n",
        "Returns:  dictionary where the keys are the answer-entities and the value is \n",
        "  a dict where the keys are the related-people and the valus are tupples with the 10 most popular categories they appear and with the pageviews of that category\n",
        "\"\"\"\n",
        "def get_categories_of_people_list(people_list, limit=5):\n",
        "  return_dict = {}\n",
        "  #intermediate_relatedpeople_with_categories_dict = {}\n",
        "\n",
        "  for key,value in people_list.items():\n",
        "    related_people_orderd = dict(sorted(value.items(), key=lambda x: x[1], reverse=True))   #order the dict after the pageviews in descending order\n",
        "    top_most_popular_people = dict(itertools.islice(related_people_orderd.items(), 5))      #take the top 5 most known people from the list\n",
        "\n",
        "    categories_of_related_people = get_categories(top_most_popular_people)\n",
        "    #print(\"Test_1\")\n",
        "    categories_with_pageviews_person = get_pageviews_for_categories(categories_of_related_people)\n",
        "    #print(\"Test_2\")\n",
        "    categories_with_subs_and_pageviews_person = get_dict_for_every_location(categories_of_related_people, categories_with_pageviews_person)\n",
        "\n",
        "    #print(\"Test_3\")\n",
        "    new_ordered_dict_related_person = sorting_dict(categories_with_subs_and_pageviews_person)\n",
        "    copy_new_ordered_dict_person_test = prune_and_ordered_dict(new_ordered_dict_related_person, 10)   \n",
        "    ordered_dict_related_person = sorting_dict(copy_new_ordered_dict_person_test)\n",
        "    return_dict[key] = ordered_dict_related_person\n",
        "  return return_dict\n",
        "\n",
        "\"\"\"\n",
        "Function that is created with help of reusable function from above\n",
        "Args:     person_question_dict with all people with their questions \n",
        "Returns:  dictionary where the keys are the answer-entities and the value is a OrderedDict of all categories with pageviews\n",
        "\"\"\"\n",
        "def get_categories_with_pv_answerEntities():\n",
        "  person_questions_dict = dict(zip(person_df['Answer'], person_df['Question']))\n",
        "  cat_ranking_person = get_categories(person_questions_dict)\n",
        "  #pprint.pprint(cat_ranking_person, indent=1)\n",
        "  cat_with_pv_person = get_pageviews_for_categories(cat_ranking_person)\n",
        "  #pprint.pprint(cat_with_pv_person, indent=1)\n",
        "  #for person: sorting the dict after pages per category\n",
        "  categories_with_subs_and_pageviews_person = get_dict_for_every_location(cat_ranking_person, cat_with_pv_person)\n",
        "  #pprint.pprint(categories_with_subs_and_pageviews_person,indent=1)\n",
        "  new_ordered_dict_person = sorting_dict(categories_with_subs_and_pageviews_person)\n",
        "\n",
        "  return new_ordered_dict_person\n",
        "\n",
        "\"\"\"\n",
        "Counts the occurrences of categories (links) in the input dictionary.\n",
        "Args: data (dict): A dictionary containing category links as keys and tuples as values.\n",
        "Returns: dict: A new dictionary where the keys are the category links and the values are\n",
        "  the corresponding tuples of the category with a middle value indicating how\n",
        "  often the link/category appeared in the other keys.\n",
        "\"\"\"\n",
        "def count_categories(related_people_with_categories, answer_entities_with_categories):\n",
        "  category_appereances = {}\n",
        "\n",
        "  for answerEntityKey, aeCategory in answer_entities_with_categories.items():\n",
        "    inner_dict = {}\n",
        "    for answerKey, relatedDict in related_people_with_categories.items():\n",
        "      if answerEntityKey == answerKey:\n",
        "        for catLink, tupl in aeCategory.items():\n",
        "          people_list = []\n",
        "          for relatedPersonKey, catWithPvs in relatedDict.items():\n",
        "            for relatedLink, relatedTupl in  catWithPvs.items():\n",
        "              if catLink == relatedLink:\n",
        "                rel_pers_str = str(relatedPersonKey)\n",
        "                if rel_pers_str not in people_list and catLink == relatedLink:\n",
        "                  people_list.append(rel_pers_str)\n",
        "                inner_dict[relatedLink] = (len(people_list),relatedTupl, people_list)\n",
        "    category_appereances[answerEntityKey] = inner_dict\n",
        "\n",
        "  return category_appereances\n",
        "\n",
        "\"\"\"\n",
        "Calculates the Intersection over Union between the answer-entity and each of the related-person entries seperately\n",
        "Args: data (dict): A dictionary containing the answer-entity with the corresponding related-people and their most popular categories that they share with ther answer-entity\n",
        "Returns: dict: A dict where the value is a list of tuple like this: ('Max Verstappen', 'Charles Leclerc', 5, 20, 0.25) (person_a, person_b, num_shared_categories, num_total_categories, num_shared_categories/num_total_categories)\n",
        "\"\"\"\n",
        "def calculate_IoU_from_countedCategoryDict(counted_category_apperances):\n",
        "  #first we recover the list of people that are related to answer-entity\n",
        "  p_dict = {}\n",
        "  for key,value in counted_category_apperances.items():\n",
        "    person_list = []\n",
        "    for link, f_tup in value.items():\n",
        "      #print(f_tup)\n",
        "      num= f_tup[0]\n",
        "      tup= f_tup[1] \n",
        "      p_lst = f_tup[2]\n",
        "      for person in p_lst:\n",
        "        if person not in person_list:\n",
        "          person_list.append(person)\n",
        "    p_dict[key] = person_list\n",
        "  \n",
        "  iou_between_person_list = {}\n",
        "  \n",
        "  #now calculate the IoU\n",
        "  for key_cat,value_cat in counted_category_apperances.items():\n",
        "    for key,value in p_dict.items():\n",
        "      if key == key_cat:\n",
        "        inter_list = {}\n",
        "        for person in value:\n",
        "          p_count = 0\n",
        "          for link, f_tup in value_cat.items():\n",
        "            p_lst = f_tup[2]\n",
        "            if person in p_lst:\n",
        "              p_count += 1\n",
        "          inter_list[person] = p_count\n",
        "      iou_between_person_list[key_cat] = inter_list\n",
        "\n",
        "  IoU_dict = {}\n",
        "  for key, value in iou_between_person_list.items():\n",
        "    IoU_list = []\n",
        "    for person, number in value.items():\n",
        "      num_total_categs = 20\n",
        "      IoU_list.append((person,number,num_total_categs, number/num_total_categs))\n",
        "    IoU_dict[key] = IoU_list\n",
        "\n",
        "  #return iou_between_person_list\n",
        "  return IoU_dict\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Calculates the avg_diversity_from_IoU\n",
        "Args: \n",
        "Returns: \n",
        "\"\"\"\n",
        "def calculate_avg_diversity_from_IoU(intersection_between_people_with_ae):\n",
        "  avg_diversity_dict = {}\n",
        "\n",
        "  for key,value in intersection_between_people_with_ae.items():\n",
        "    diversity_sum = 0\n",
        "    pairwise_comparisons = 0\n",
        "    for item in value:\n",
        "      all_categs = item[2]\n",
        "      IoU = item[1]\n",
        "      diversity_sum += (all_categs - IoU)\n",
        "      pairwise_comparisons += 1\n",
        "    avg_diversity_dict[key] = (diversity_sum / pairwise_comparisons)\n",
        "  return avg_diversity_dict\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Calculates the categories score from the category diversity (calculated by calculate_avg_diversity_from_IoU() ) and the cat_popularity (=pageviews)\n",
        "Args: \n",
        "Returns: \n",
        "\"\"\"\n",
        "def calculate_categories_score(counted_category_apperances, avg_diversity_from_IoU):\n",
        "  categories_scores_dict = {}\n",
        "  for key,value in counted_category_apperances.items():\n",
        "    inter_dict={}\n",
        "    for item in value.items():\n",
        "      link = item[0]\n",
        "      cat_popularity = item[1][1][2]\n",
        "      for name, cat_div in avg_diversity_from_IoU.items():\n",
        "        if name == key:\n",
        "          inter_dict[link] = cat_popularity * cat_div\n",
        "    categories_scores_dict[key] = inter_dict\n",
        "  \n",
        "  ordered_dicter = {}\n",
        "  ordered_scores ={}\n",
        "  for k,v in categories_scores_dict.items():\n",
        "    ordered_scores[k] = OrderedDict(v)\n",
        "  for k,v in ordered_scores.items():\n",
        "    ordered_dicter[k] = OrderedDict(sorted(v.items(), key=lambda x: x[1], reverse=True))\n",
        "    \n",
        "  return ordered_dicter\n"
      ],
      "metadata": {
        "id": "cnbHXuXYsvjA"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Execute this part once (ca. 60m for 3people), then ruese the test variables from this section (faster)"
      ],
      "metadata": {
        "id": "aoazxgQByklM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "1. retrieve all of the related links of a wiki-persons-page (first 500);\n",
        "2. pre prune the list, such that only entries with 2 words in the title are left; (for performance reasons)\n",
        "3. analyse the links that are left via wikipedia APi and the \"instance of\" (P31) property set to \"human\" (Q5)\n",
        "4. now we have a list of related people to compare to our answer-person-entity\n",
        "\"\"\"\n",
        "#this is just for reducing time during testing - 7m+ (30s-4m)\n",
        "\n",
        "test_person_answers_dict = dict(zip(person_df['Answer'], person_df['Question']))\n",
        "test_related_people_dict = {}\n",
        "test_related_people_link_dict= {}\n",
        "test_related_people_pageviews_dict = {}\n",
        "test_most_popular_related_people_with_categories = {}\n",
        "\n",
        "#time saving for first part (related people recovery) 2m\n",
        "for key,value in test_person_answers_dict.items():\n",
        "  test_related_people_dict[key] = get_related_people_from_person_name(key)\n",
        "#pprint.pprint(test_related_people_dict)"
      ],
      "metadata": {
        "id": "zWj3izpw2Sc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98d02fae-1b71-4a7e-8d9b-cfdbcdf78079"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://en.wikipedia.org/wiki/Max_Verstappen\n",
            "https://en.wikipedia.org/wiki/Michael_Jackson\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#time saving for second part (related peoples with pageviews and ordering) - 16m (6-7m)\n",
        "for key,value in test_related_people_dict.items():\n",
        "  inter_link_list = []\n",
        "  for item in value:\n",
        "    inter_link_list.append(item['url'])\n",
        "  test_related_people_link_dict[key] = inter_link_list\n",
        "  test_related_people_pageviews_dict = get_pageviews_from_links(test_related_people_link_dict)\n",
        "#pprint.pprint(test_related_people_pageviews_dict)"
      ],
      "metadata": {
        "id": "r9eHU-2jk9PJ"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#time saving for third part (related peoples categories recovery and ordering) - 43m+ (24m)\n",
        "test_most_popular_related_people_with_categories = get_categories_of_people_list(test_related_people_pageviews_dict)\n",
        "pprint.pprint(test_most_popular_related_people_with_categories)"
      ],
      "metadata": {
        "id": "gvw9PvhrlCK7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "03a72590-3a45-45bf-c7cb-610a1e0d1e50"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorting failed for location Charles Leclerc: tuple index out of range\n",
            "Sorting failed for location Ayrton Senna: tuple index out of range\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "JSONDecodeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    909\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcomplexjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mJSONDecodeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-956b9e07a3d0>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#time saving for third part (related peoples categories recovery and ordering) - 43m+ (24m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_most_popular_related_people_with_categories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_categories_of_people_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_related_people_pageviews_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_most_popular_related_people_with_categories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-799093188b3a>\u001b[0m in \u001b[0;36mget_categories_of_people_list\u001b[0;34m(people_list, limit)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mtop_most_popular_people\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mislice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelated_people_orderd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m#take the top 5 most known people from the list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mcategories_of_related_people\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_categories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_most_popular_people\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0;31m#print(\"Test_1\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mcategories_with_pageviews_person\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_pageviews_for_categories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategories_of_related_people\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-ccdeb779bdbe>\u001b[0m in \u001b[0;36mget_categories\u001b[0;34m(subject_dict)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mdicto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_cat_with_all_subcats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mcategories_for_subject_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdicto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mranking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_categories_ranking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mordict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mranking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mrankings_for_categories_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mordict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-d0cac8bbdca8>\u001b[0m in \u001b[0;36mget_categories_ranking\u001b[0;34m(searched_location)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mcategory\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcategories_links\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     \u001b[0msub_cats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_category_subcategories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m     \u001b[0mct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_category_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0mpages_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_category_pages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-d0cac8bbdca8>\u001b[0m in \u001b[0;36mget_category_subcategories\u001b[0;34m(link)\u001b[0m\n\u001b[1;32m     76\u001b[0m   \u001b[0mcategory_page\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwiki_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Category:{category_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m   \u001b[0;31m# Find the subcategories section of the page\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m   \u001b[0msubcategories_section\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcategory_page\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorymembers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m   \u001b[0;31m# Extract the subcategories from the section\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m   \u001b[0msubcategories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wikipediaapi/__init__.py\u001b[0m in \u001b[0;36mcategorymembers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m         \"\"\"\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_called\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"categorymembers\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"categorymembers\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_categorymembers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wikipediaapi/__init__.py\u001b[0m in \u001b[0;36m_fetch\u001b[0;34m(self, call)\u001b[0m\n\u001b[1;32m   1027\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"WikipediaPage\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m         \u001b[0;34m\"\"\"Fetches some data?.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m         \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwiki\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1030\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_called\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wikipediaapi/__init__.py\u001b[0m in \u001b[0;36mcategorymembers\u001b[0;34m(self, page, **kwargs)\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0mused_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m         \u001b[0mraw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mused_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_common_attributes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"query\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wikipediaapi/__init__.py\u001b[0m in \u001b[0;36m_query\u001b[0;34m(self, page, params)\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"redirects\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_build_extracts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"WikipediaPage\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    915\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mRequestsJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRequestsJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mJSONDecodeError\u001b[0m: [Errno Expecting value] <!DOCTYPE html>\n<html lang=\"en\">\n<meta charset=\"utf-8\">\n<title>Wikimedia Error</title>\n<style>\n* { margin: 0; padding: 0; }\nbody { background: #fff; font: 15px/1.6 sans-serif; color: #333; }\n.content { margin: 7% auto 0; padding: 2em 1em 1em; max-width: 640px; }\n.footer { clear: both; margin-top: 14%; border-top: 1px solid #e5e5e5; background: #f9f9f9; padding: 2em 0; font-size: 0.8em; text-align: center; }\nimg { float: left; margin: 0 2em 2em 0; }\na img { border: 0; }\nh1 { margin-top: 1em; font-size: 1.2em; }\n.content-text { overflow: hidden; overflow-wrap: break-word; word-wrap: break-word; -webkit-hyphens: auto; -moz-hyphens: auto; -ms-hyphens: auto; hyphens: auto; }\np { margin: 0.7em 0 1em 0; }\na { color: #0645ad; text-decoration: none; }\na:hover { text-decoration: underline; }\ncode { font-family: sans-serif; }\n.text-muted { color: #777; }\n</style>\n<div class=\"content\" role=\"main\">\n<a href=\"https://www.wikimedia.org\"><img src=\"https://www.wikimedia.org/static/images/wmf-logo.png\" srcset=\"https://www.wikimedia.org/static/images/wmf-logo-2x.png 2x\" alt=\"Wikimedia\" width=\"135\" height=\"101\">\n</a>\n<h1>Error</h1>\n<div class=\"content-text\">\n<p>Our servers are currently under maintenance or experiencing a technical problem.\n\nPlease <a href=\"\" title=\"Reload this page\" onclick=\"window.location.reload(false); return false\">try again</a> in a few&nbsp;minutes.</p>\n\n<p>See the error message at the bottom of this page for more&nbsp;information.</p>\n</div>\n</div>\n<div class=\"footer\"><p>If you report this error to the Wikimedia System Administrators, please include the details below.</p><p class=\"text-muted\"><code>Request from 34.86.11.180 via cp1077 cp1077, Varnish XID 892410961<br>Upstream caches: cp1077 int<br>Error: 429, Too many requests - for unthrottling, contact noc@wikimedia.org to discuss a less disruptive approach at Thu, 18 May 2023 12:26:43 GMT</code></p>\n</div>\n</html>\n: 0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#time saving for third part retrieves the categories of the answer-entities - 9m+ (6m)\n",
        "test_answer_entities_with_categories = get_categories_with_pv_answerEntities()\n",
        "#pprint.pprint(test_answer_entities_with_categories, indent=1, compact=True)"
      ],
      "metadata": {
        "id": "OEduFlSGlDlZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b8e94df-7f22-44c4-ac0e-d861e0933605"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorting failed for location Max Verstappen: tuple index out of range\n",
            "Sorting failed for location Michael Jackson: tuple index out of range\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#time saving for fourth part counts the categories of the answer-entities - 3s\n",
        "test_counted_category_apperances = count_categories(test_most_popular_related_people_with_categories, test_answer_entities_with_categories)\n",
        "#just for the ordering of the inner list\n",
        "ordered_data = {}\n",
        "for key,value in test_counted_category_apperances.items():\n",
        "  tmp = OrderedDict(value)\n",
        "  ordered_data[key] = OrderedDict(sorted(tmp.items(), key=lambda x: x[1][0], reverse=True) )\n",
        "test_counted_category_apperances = ordered_data\n",
        "pprint.pprint(test_counted_category_apperances)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3t42JlHlE1D",
        "outputId": "c270651d-5d4d-4b3c-db87-b68f909979ae"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Max Verstappen': OrderedDict(), 'Michael Jackson': OrderedDict()}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1. calculate the IoU between max and every other person:\n",
        "# (M,C) = 5/20; (M,L) = 2/20; (M,D) = 2/20; (M,A) = 2/20; (M,F) = 2/20;\n",
        "intersection_between_people_with_ae = calculate_IoU_from_countedCategoryDict(test_counted_category_apperances)\n",
        "print(\"IoU between the people and the answer-entity:\")\n",
        "pprint.pprint(intersection_between_people_with_ae)\n",
        "\n",
        "#2. calculate the average diversity between the 6 drivers:\n",
        "# (20-5) + (20-2) + (20-2) + (20-2) + (20-2) = 87; (#avg_diversity/#pairwise_comparison) = 87/5 = 17,4\n",
        "avg_diversity_from_IoU = calculate_avg_diversity_from_IoU(intersection_between_people_with_ae)\n",
        "print(\"Avergage diversity from IoU:\")\n",
        "pprint.pprint(avg_diversity_from_IoU)\n",
        "\n",
        "#3. calculate a type of categoreis_score\n",
        "# categories_score = cat_diversity * cat_popularity(pvs)\n",
        "categories_scores_dict = calculate_categories_score(test_counted_category_apperances, avg_diversity_from_IoU)\n",
        "print(\"Category score:\")\n",
        "pprint.pprint(categories_scores_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "RQR9zKqVbIG8",
        "outputId": "8a07290b-5f45-4b66-c2fc-c3b2eb86fcd9"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IoU between the people and the answer-entity:\n",
            "{'Max Verstappen': [], 'Michael Jackson': []}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ZeroDivisionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-f0d50fa530bc>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#2. calculate the average diversity between the 6 drivers:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# (20-5) + (20-2) + (20-2) + (20-2) + (20-2) = 87; (#avg_diversity/#pairwise_comparison) = 87/5 = 17,4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mavg_diversity_from_IoU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_avg_diversity_from_IoU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintersection_between_people_with_ae\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Avergage diversity from IoU:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_diversity_from_IoU\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-799093188b3a>\u001b[0m in \u001b[0;36mcalculate_avg_diversity_from_IoU\u001b[0;34m(intersection_between_people_with_ae)\u001b[0m\n\u001b[1;32m    189\u001b[0m       \u001b[0mdiversity_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mall_categs\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mIoU\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m       \u001b[0mpairwise_comparisons\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m     \u001b[0mavg_diversity_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdiversity_sum\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mpairwise_comparisons\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mavg_diversity_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions for testing extensions and new approaches"
      ],
      "metadata": {
        "id": "wnBVditQf6Bo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "TEST OF FUNCTION IN CELL DIRECTLY ABOVE \n",
        "continue here\n",
        "Loops and function-calls to get the most popular related people of each answer-entity,\n",
        "with the categories of each related-person ranked from most-popular to least,\n",
        "to then beeing able of comparing those categories and finding an unusual one, for this specific answer-entity;\n",
        "\"\"\"\n",
        "\n",
        "#Testing out the new functions: \n",
        "person_answers_dict = dict(zip(person_df['Answer'], person_df['Question']))\n",
        "related_people_dict = {}\n",
        "related_people_link_dict= {}\n",
        "related_people_pageviews_dict = {}\n",
        "\n",
        "#PART 1:\n",
        "#for key,value in person_answers_dict.items():\n",
        "#  related_people_dict[key] = get_related_people_from_person_name(key)\n",
        "\n",
        "#this is just for reducing time during testing \n",
        "related_people_dict = test_related_people_dict\n",
        "pprint.pprint(related_people_dict)\n",
        "\n",
        "\n",
        "#PART 2:\n",
        "# for key,value in related_people_dict.items():\n",
        "#   inter_link_list = []\n",
        "#   for item in value:\n",
        "#     inter_link_list.append(item['url'])\n",
        "#   related_people_link_dict[key] = inter_link_list\n",
        "#   related_people_pageviews_dict = get_pageviews_from_links(related_people_link_dict)\n",
        "\n",
        "#this is just for reducing time during testing \n",
        "related_people_pageviews_dict = test_related_people_pageviews_dict\n",
        "pprint.pprint(related_people_pageviews_dict)\n",
        "\n",
        "\n",
        "#PART 3:\n",
        "#most_popular_related_people_with_categories = get_categories_of_people_list(related_people_pageviews_dict)\n",
        "most_popular_related_people_with_categories = test_most_popular_related_people_with_categories\n",
        "pprint.pprint(most_popular_related_people_with_categories)\n",
        "\n",
        "\n",
        "#PART 4:\n",
        "#creates a list with all the answer-entity names\n",
        "#answer_entities_with_categories = get_categories_with_pv_answerEntities()\n",
        "answer_entities_with_categories = test_answer_entities_with_categories\n",
        "pprint.pprint(answer_entities_with_categories, indent=1, compact=True)\n",
        "\n",
        "#PART 4:\n",
        "#compare anc count\n",
        "#counted_category_apperances = count_categories(most_popular_related_people_with_categories, answer_entities_with_categories)\n",
        "counted_category_apperances = test_counted_category_apperances\n",
        "pprint.pprint(counted_category_apperances, indent=1, compact=True)\n",
        "\n",
        "#shared_categories_between_people = count_categories(most_popular_related_people_with_categories)\n",
        "#pprint.pprint(shared_categories_between_people, indent=1, compact=True)\n",
        "\n",
        "#Now compare the categories: \n",
        "#Most appearing category between person-answer-entity and the corresponding related-people\n",
        "#Most popular category, that corresponds to just the person-answer-entity and as few of the related-people as possible (=unexpected category)"
      ],
      "metadata": {
        "id": "XU0XbS0QfnWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing new things and approaches\n",
        "wiki_link = 'https://en.wikipedia.org/wiki/'\n",
        "\n",
        "def unexpected_categories(person_answer_entity_list):\n",
        "  wiki_link_list={}\n",
        "  for val in person_answer_entity_list:\n",
        "    names_underscores = val.replace(' ', '_')\n",
        "    answer_entity_wiki_link = wiki_link + names_underscores\n",
        "    wiki_link_list[val] =  answer_entity_wiki_link\n",
        "  \n",
        "  categories = get_categories(wiki_link_list)\n",
        "  categories_with_pv = get_pageviews_for_categories(categories)\n",
        "  categories_with_subs_pvs = get_dict_for_every_location(categories, categories_with_pv)\n",
        "\n",
        "  test1 = check_inner_tuple_length(categories_with_subs_pvs )\n",
        "  test2 = prune_and_ordered_dict(test1, 20)\n",
        "  ordered_categories_dict_AnswerPeopleEntities = sorting_dict(test2)\n",
        "  #up to this point, the function retrieves,pre-prunes and orderes the most popular categories from the person-answer-entities\n",
        "  #now wee need to do the same for each of the selected_related_people_list, to then compare the categories and rank them from most to least occuring\n",
        "  return(ordered_categories_dict_AnswerPeopleEntities)\n",
        "\n",
        "def related_people_with_categories(categs_list):\n",
        "  selected_people = {}\n",
        "  selected_people_with_categories = {}\n",
        "  for key, value in categs_list.items():\n",
        "    names_underscores = key.replace(' ', '_')\n",
        "    related_articles = get_related_links(wiki_link + names_underscores)\n",
        "    filtered_links = filter_two_word_titles(related_articles)\n",
        "    related_people_list = check_if_person(filtered_links)\n",
        "    select_random_people = select_random_entries(related_people_list)\n",
        "\n",
        "    selected_people[key] = select_random_people\n",
        "\n",
        "  return selected_people\n",
        "\n",
        "# now calculate the most popukar categories of the selected_people and compare them\n",
        "\n",
        "def check_inner_tuple_length(pv_and_pages_dict):\n",
        "  # Iterate over the dictionary\n",
        "  for key, value in pv_and_pages_dict.items():\n",
        "    # Iterate over the ordered dict values\n",
        "    for inner_key, inner_value in value.items():\n",
        "      # Check if the inner tuple has two elements\n",
        "      if len(inner_value) == 2:\n",
        "        # Update the value by adding a zero at the end\n",
        "        value[inner_key] = inner_value + (0,)\n",
        "  return pv_and_pages_dict\n",
        "\n",
        "person_answer_entity_list = list(person_df['Answer'])\n",
        "#print(person_answer_entity_list)\n",
        "#unexpected_categories(person_answer_entity_list)\n",
        "#print(unexpected_categories(person_answer_entity_list))\n",
        "a = unexpected_categories(person_answer_entity_list)\n",
        "#pprint.pprint(unexpected_categories(person_answer_entity_list))\n",
        "#b = related_people_with_categories(a)\n",
        "#pprint.pprint(b)\n",
        "#print(person_questions_dict)\n",
        "#10 mins"
      ],
      "metadata": {
        "id": "Yd4AeTR_KPir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Old predictions methods**"
      ],
      "metadata": {
        "id": "bp5WmYAKw6Mn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "du5Un8VufnlG"
      },
      "source": [
        "## Functions for reuse"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8o8t6UzdAaKj"
      },
      "source": [
        "#get Wikidata ID\n",
        "def getQitemOfName(entity):\n",
        "  \n",
        "  searched = \"'\"+entity+\"'\"\n",
        "  sparql.setQuery('''\n",
        "  SELECT ?item ?itemLabel WHERE {\n",
        "    ?item rdfs:label '''+searched+'''@en.\n",
        "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
        "\n",
        "  }\n",
        "  ''')\n",
        "  \n",
        "  sparql.setReturnFormat(JSON)\n",
        "  entities = sparql.query().convert()\n",
        "  entities_df = pd.json_normalize(entities['results']['bindings'])\n",
        "  words = entities_df[[\"item.value\"]].iloc[0].str.split(\"/\")[0]\n",
        "  return words[-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4tNPxs6Cp8f"
      },
      "source": [
        "#get name of entity from Wikidata ID\n",
        "\n",
        "def getLabelOfQitems(entities):\n",
        "\n",
        "  finalLabels = []\n",
        "  for item in entities:\n",
        "    #searched = \"'\"+item+\"'\"\n",
        "    sparql.setQuery('''\n",
        "    SELECT * WHERE {\n",
        "        wd:'''+item+ ''' rdfs:label ?label .\n",
        "        FILTER (langMatches( lang(?label), \"EN\" ) )\n",
        "    } LIMIT 1\n",
        "    ''')\n",
        "    sparql.setReturnFormat(JSON)\n",
        "    entities = sparql.query().convert()\n",
        "    entities_df = pd.json_normalize(entities['results']['bindings'])\n",
        "    label = entities_df[\"label.value\"].iloc[0]\n",
        "    finalLabels.append(label)\n",
        "\n",
        "  return finalLabels  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gK0zkvbXxRpU"
      },
      "source": [
        "#function to get all backlinks to wikipage and saves backlinks >= 1000 in array\n",
        "#source: https://www.mediawiki.org/wiki/API:Backlinks#Response\n",
        "\n",
        "import requests\n",
        "\n",
        "def getBacklinks(entities):\n",
        "  S = requests.Session()\n",
        "\n",
        "  URL = \"https://en.wikipedia.org/w/api.php\"\n",
        "\n",
        "  listWithBacklink500 = []\n",
        "  dictWithAllEntries = {}\n",
        "\n",
        "  for val in entities:\n",
        "    #print(row[\"itemLabel.value\"])\n",
        "    PARAMS = {\n",
        "        \"action\": \"query\",\n",
        "        \"format\": \"json\",\n",
        "        \"list\": \"backlinks\",\n",
        "        \"bltitle\": val,\n",
        "        \"bllimit\": \"max\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "      R = S.get(url=URL, params=PARAMS)\n",
        "      DATA = R.json()\n",
        "   \n",
        "\n",
        "      BACKLINKS = DATA[\"query\"][\"backlinks\"]\n",
        "\n",
        "      count = 0\n",
        "      for links in BACKLINKS:\n",
        "        count +=1\n",
        "\n",
        "      \n",
        "      while \"continue\" in DATA:\n",
        "        if (count >= 1000):\n",
        "          break;\n",
        "        blcontinue = DATA[\"continue\"][\"blcontinue\"]\n",
        "        PARAMS[\"blcontinue\"] = blcontinue\n",
        "\n",
        "        R = S.get(url=URL, params=PARAMS)\n",
        "        DATA = R.json()\n",
        "        BACKLINKS = DATA[\"query\"][\"backlinks\"]\n",
        "\n",
        "        for links in BACKLINKS:\n",
        "          count += 1\n",
        "      \n",
        "      dictWithAllEntries[val] = count\n",
        "      #print(val + \": \" + str(count))\n",
        "\n",
        "    except ValueError:\n",
        "      print(\"Value Error\")\n",
        "\n",
        "    if (count >= 1000):\n",
        "      listWithBacklink500.append(val) \n",
        "\n",
        "  if (len(listWithBacklink500) > 0):\n",
        "    #return getPageViews(listWithBacklink500)\n",
        "    return listWithBacklink500\n",
        "\n",
        "  else:\n",
        "    listMostBacklinks = []\n",
        "    sortedDict = dict(sorted(dictWithAllEntries.items(), key=lambda item:item[1]))\n",
        "\n",
        "    for k, v in sortedDict.items():\n",
        "      listMostBacklinks.append(k)\n",
        "    \n",
        "    numberToPass = len(listMostBacklinks)//3\n",
        "        \n",
        "    return listMostBacklinks[-numberToPass:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-0fPfmxUZSW"
      },
      "source": [
        "#function to get all backlinks to wikipage and saves backlink == 500 in array\n",
        "#source: https://www.mediawiki.org/wiki/API:Backlinks#Response\n",
        "\n",
        "import requests\n",
        "\n",
        "def getBacklinksDict(entities):\n",
        "  S = requests.Session()\n",
        "\n",
        "  URL = \"https://en.wikipedia.org/w/api.php\"\n",
        "\n",
        "  dictWithBacklink500 = {}\n",
        "  dictWithAllEntries = {}\n",
        "\n",
        "  for key, val in entities.items():\n",
        "    print(key, val)\n",
        "\n",
        "    PARAMS = {\n",
        "        \"action\": \"query\",\n",
        "        \"format\": \"json\",\n",
        "        \"list\": \"backlinks\",\n",
        "        \"bltitle\": key,\n",
        "        \"bllimit\": \"max\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "      R = S.get(url=URL, params=PARAMS)\n",
        "      DATA = R.json()\n",
        "   \n",
        "\n",
        "      BACKLINKS = DATA[\"query\"][\"backlinks\"]\n",
        "\n",
        "      count = 0\n",
        "      for links in BACKLINKS:\n",
        "        count +=1\n",
        "\n",
        "      \n",
        "      while \"continue\" in DATA:\n",
        "        if (count >= 1000):\n",
        "          break;\n",
        "        blcontinue = DATA[\"continue\"][\"blcontinue\"]\n",
        "        PARAMS[\"blcontinue\"] = blcontinue\n",
        "\n",
        "        R = S.get(url=URL, params=PARAMS)\n",
        "        DATA = R.json()\n",
        "        BACKLINKS = DATA[\"query\"][\"backlinks\"]\n",
        "\n",
        "        for links in BACKLINKS:\n",
        "          count += 1\n",
        "      \n",
        "      dictWithAllEntries[key] = count\n",
        "      #print(val + \": \" + str(count))\n",
        "\n",
        "    except ValueError:\n",
        "      print(\"Value Error\")\n",
        "    except KeyError:\n",
        "      print(\"Key Error\")\n",
        "\n",
        "    if (count >= 1000):\n",
        "      dictWithBacklink500.update({key: val}) \n",
        "\n",
        "  if (len(dictWithBacklink500) > 0):\n",
        "    #return getPageViews(listWithBacklink500)\n",
        "    return dictWithBacklink500\n",
        "\n",
        "    #print(listMostBacklinks[-numberToPass:])\n",
        "\n",
        "    \n",
        "    return listMostBacklinks[-numberToPass:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hz4SZXwl1j7d"
      },
      "source": [
        "#get avg pageViews of entities\n",
        "\n",
        "def getPageViewsList(listEntities):\n",
        "  avgDict = {}\n",
        "  \n",
        "  for val in listEntities:\n",
        "    try:\n",
        "      resp = pageviewapi.per_article('en.wikipedia', val, '20191106', '20201120', access='all-access', agent='all-agents', granularity='monthly')\n",
        "    #print(resp)\n",
        "      avgViews = 0\n",
        "      count = 0\n",
        "      for i in resp.get(\"items\"):\n",
        "        #print(i.get(\"views\"))\n",
        "        avgViews += i.get(\"views\")\n",
        "        count += 1\n",
        "      avgDict.update({val: avgViews//count})\n",
        "    except:\n",
        "      print(\"Page not found to check pageViews for page: \" + val)\n",
        "      \n",
        "  #return (max(avgDict, key=avgDict.get))\n",
        "  return avgDict\n",
        "\n",
        "\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhL5jdCN-NdY"
      },
      "source": [
        "#get avg pageViews of entity\n",
        "import pageviewapi\n",
        "\n",
        "\n",
        "def getPageViews(entity):\n",
        "  avgDict = {}\n",
        "  \n",
        "  try:\n",
        "    resp = pageviewapi.per_article('en.wikipedia', entity, '20191106', '20201120', access='all-access', agent='all-agents', granularity='monthly')\n",
        "\n",
        "    avgViews = 0\n",
        "    count = 0\n",
        "    for i in resp.get(\"items\"):\n",
        "      #print(i.get(\"views\"))\n",
        "      avgViews += i.get(\"views\")\n",
        "      count += 1\n",
        "\n",
        "    pageViews = avgViews//count\n",
        "    #avgDict.update({val: avgViews//count})\n",
        "\n",
        "    #return (max(avgDict, key=avgDict.get))\n",
        "    return pageViews\n",
        "  #print(resp)\n",
        "  except:\n",
        "    print(\"Page not found to check pageViews for page: \" + entity)\n",
        "    return -1\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check if entity is a person\n",
        "#return list of persons\n",
        "\n",
        "def checkIfHuman(entity):\n",
        "\n",
        "  #listHumans = []\n",
        "\n",
        "  #for test in entity:\n",
        "    #print(item)\n",
        "  searched = \"'\"+entity+\"'\"\n",
        "\n",
        "  #print(searched)\n",
        "\n",
        "  try:\n",
        "\n",
        "    sparql.setQuery('''\n",
        "        SELECT distinct ?item ?itemLabel \n",
        "        WHERE{\n",
        "          ?item ?label '''+searched+'''.  \n",
        "          ?item wdt:P31 wd:Q5.\n",
        "          ?article schema:about ?item .\n",
        "          ?article schema:inLanguage \"en\" .\n",
        "          ?article schema:isPartOf <https://en.wikipedia.org/>.\t\n",
        "          SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }    \n",
        "        }\n",
        "    ''')\n",
        "    sparql.setReturnFormat(JSON)\n",
        "    entities = sparql.query().convert()\n",
        "    entities_df = pd.json_normalize(entities['results']['bindings'])\n",
        "\n",
        "    #print(entities)\n",
        "    #print(entities_df[\"itemLabel.value\"].item())\n",
        "\n",
        "    #for index, row in entities_df.iterrows():\n",
        "    return entities_df[\"itemLabel.value\"].item()\n",
        "    \n",
        "    \n",
        "  except:\n",
        "    return None\n"
      ],
      "metadata": {
        "id": "QHdp21OCwtgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOJJtksOgQYk"
      },
      "source": [
        "#if parantheses in entity -> remove\n",
        "\n",
        "def removeParanthesesDict(inputDict):\n",
        "\n",
        "  newDict = {}\n",
        "  for key, val in inputDict.items():\n",
        "    if ('(' in key):\n",
        "      newElem = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", key).rstrip()\n",
        "      newDict.update({newElem: val})\n",
        "    else:\n",
        "      newDict.update({key: val})\n",
        "\n",
        "  return newDict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lhmuuUNQ__8"
      },
      "source": [
        "##Hints if answer is a Location"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-T2fLcTRD54"
      },
      "source": [
        "#Query with all properties to get data\n",
        "\n",
        "def allHintsLocation(entity):\n",
        "  sparql.setQuery('''\n",
        "  SELECT ?itemLabel ?property\n",
        "  WHERE\n",
        "  {  \n",
        "    VALUES ?property {wdt:P35 wdt:P30 wdt:P36 wdt:P610 wdt:P1082 wdt:P421 wdt:P38 wdt:P17 wdt:P1376 wdt:P131 wdt:P6 wdt:P1830 wdt:P47 wdt:P206 wdt:P37 wdt:P463 wdt:793}\n",
        "    wd:'''+entity+''' ?property ?item.\n",
        "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
        "  }\n",
        "  ''')\n",
        "  sparql.setReturnFormat(JSON)\n",
        "  hintsLocation = sparql.query().convert()\n",
        "\n",
        "  hintsLocation_df = pd.json_normalize(hintsLocation['results']['bindings'])\n",
        "\n",
        "  return hintsLocation_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "co1A0iK-sery"
      },
      "source": [
        "#create template based hints\n",
        "\n",
        "def getFormatedHintsLocation(queryResults_df):\n",
        "  timeZone = []\n",
        "  isLocatedIn = []\n",
        "  ownerOf = []\n",
        "  shareBorder = []\n",
        "  bodyOfWater = []\n",
        "  languages = []\n",
        "  memberOf = []\n",
        "  significantEvents = []\n",
        "\n",
        "  listHintsLocation = []\n",
        "\n",
        "  searchString = \"http://www.wikidata.org/prop/direct/\"\n",
        "  \n",
        "  for index, row in queryResults_df.iterrows():\n",
        "    if(not location in row[\"itemLabel.value\"]):\n",
        "      if(row[\"property.value\"] == searchString + \"P35\"):\n",
        "        listHintsLocation.append(\"Head of state of searched country is \" + row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P30\"):\n",
        "        listHintsLocation.append(\"The searched location is on continent \" + row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P36\"):\n",
        "        listHintsLocation.append(\"The capital of searched country is \" + row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"\"):\n",
        "        listHintsLocation.append(\"The highest point in searched location is \" + row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P1082\"):\n",
        "        listHintsLocation.append(\"The searched location has a population of \" + row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P38\"):\n",
        "        listHintsLocation.append(\"Currency in searched location is \" + row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P17\"):\n",
        "        if(not location in row[\"itemLabel.value\"]):\n",
        "          listHintsLocation.append(\"The searched location is in country \"+ row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P1376\"):\n",
        "        listHintsLocation.append(\"The searched city is capital of \" + row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P131\"):\n",
        "        listHintsLocation.append(\"The searched location is located in \" + row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P6\"):\n",
        "        listHintsLocation.append(\"Head of government of searched location is \" + row[\"itemLabel.value\"])\n",
        "      \n",
        "      #if get multiple values save all in list and pick one afterwards\n",
        "      elif(row[\"property.value\"] == searchString + \"P1830\"):\n",
        "        if(not location in row[\"itemLabel.value\"]):\n",
        "          ownerOf.append(row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P47\"):\n",
        "        shareBorder.append(row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P206\"):\n",
        "        bodyOfWater.append(row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P37\"):\n",
        "        if(not location in row[\"itemLabel.value\"]):\n",
        "          languages.append(row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P421\"):\n",
        "        timeZone.append(row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P463\"):\n",
        "        memberOf.append(row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P793\"):\n",
        "        if(not location in row[\"itemLabel.value\"]):\n",
        "          significantEvents.append(row[\"itemLabel.value\"])\n",
        "\n",
        "  #getting more than one result with one query, pick one for output\n",
        "  if(len(ownerOf) > 0):\n",
        "    listHintsLocation.append(\"The searched location is owner of \" + ownerOf[0])\n",
        "  if(len(shareBorder) > 0):\n",
        "    listHintsLocation.append(\"The searched location shares border with \" + shareBorder[0])\n",
        "  if(len(bodyOfWater) > 0):\n",
        "    listHintsLocation.append(\"The next body of water of searched location is \" + bodyOfWater[0])\n",
        "  if(len(memberOf) > 0):\n",
        "    listBacklinks = getBacklinks(memberOf)\n",
        "    dictPageViews = getPageViewsList(listBacklinks)\n",
        "    listHintsLocation.append(\"Searched location is member of \" + (max(dictPageViews, key=dictPageViews.get)))\n",
        "  if(len(significantEvents) > 0):\n",
        "    listBacklinks = getBacklinks(significantEvents)\n",
        "    dictPageViews = getPageViewsList(listBacklinks)\n",
        "    listHintsLocation.append(\"A significant event happened in this location was \" + (max(dictPageViews, key=dictPageViews.get)))\n",
        "\n",
        "  #if getting more than one language concatenate all\n",
        "  commaCounter = len(languages)-1\n",
        "  stringLanguages = \"\"\n",
        "  for language in languages:\n",
        "    if(commaCounter > 0):\n",
        "      stringLanguages += language + \", \"\n",
        "      commaCounter -= 1\n",
        "    else:\n",
        "      stringLanguages += language\n",
        "  if(stringLanguages != \"\"):\n",
        "    listHintsLocation.append(\"Spoken language(s) in searched location is/are \" + stringLanguages)\n",
        "  if(len(timeZone) > 0):\n",
        "    listHintsLocation.append(\"The searched location is in time Zone \" + timeZone[0])\n",
        "\n",
        "  \n",
        "  return listHintsLocation\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g65hUdVTRbXU"
      },
      "source": [
        "def writeLocationHintsInFile(listHintsLocation):\n",
        "  randomHintsList = random.sample(listHintsLocation, len(listHintsLocation))\n",
        "  \n",
        "  for item in randomHintsList:\n",
        "    with open(\"hintsLocation.txt\", 'a') as writefile:\n",
        "      writefile.write(item + \"\\n\")\n",
        "      writefile.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Yf6Fi_34ye5"
      },
      "source": [
        "##Hints if answer is a Person"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Query with all properties to get data\n",
        "\n",
        "def allHintsPerson(entity):\n",
        "  sparql.setQuery('''\n",
        "  SELECT ?itemLabel ?property\n",
        "  WHERE\n",
        "  {  \n",
        "    VALUES ?property {wdt:P21 wdt:P27 wdt:P569 wdt:P19 wdt:P1971 wdt:P106 wdt:P1340 wdt:P1884 wdt:P2048 wdt:P39 wdt:P69 wdt:P512 wdt:P102 wdt:P3602 wdt:P800 wdt:P166 wdt:P3373 wdt:P1412 wdt:P413 wdt:P118 wdt:P54}\n",
        "    wd:'''+entity+''' ?property ?item.\n",
        "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
        "  }\n",
        "  ''')\n",
        "  sparql.setReturnFormat(JSON)\n",
        "  hintsPerson = sparql.query().convert()\n",
        "\n",
        "  hintsPerson_df = pd.json_normalize(hintsPerson['results']['bindings'])\n",
        "  \n",
        "  return hintsPerson_df"
      ],
      "metadata": {
        "id": "Lm8zYu5C3xSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qS1ZtKwL5hM-"
      },
      "source": [
        "#create temolate based hints\n",
        "\n",
        "def getFormatedHintsPerson(queryResults_df):\n",
        "\n",
        "  listOccupation = [\"Searched person was occuupied as \"]\n",
        "  listPositionHeld = [\"Searched person held the position(s) \"]\n",
        "  listEducatedAt = [\"Searched person was educated at \"]\n",
        "  listAcademicDegrees = [\"Searched person has academic degrees \"]\n",
        "  listPoliticParty = [\"Searched was/is member of politic party \"]\n",
        "  listCandidacyElection = [\"Searched person was candidacy at election(s) \"]\n",
        "  listNotableWork = [\"Searched person did notable work \"]\n",
        "  listAwardsReceived = [\"Searched person received awards \"]\n",
        "  listLanguages = [\"Searched person speaks language(s) \"]\n",
        "  listPlayedPositions = [\"Searched person played position \"]\n",
        "  listLeague = [\"Searched person played in leagues \"]\n",
        "  listTeams = [\"Searched person played in teams \"]\n",
        "  listCitizenship = [\"Searched person has following citizenships \"]\n",
        "  listSiblings = []\n",
        "  listHair = []\n",
        "\n",
        "  listHintsPerson = []\n",
        "\n",
        "  for index, row in queryResults_df.iterrows():\n",
        "    if(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P21\"):\n",
        "      listHintsPerson.append(\"Gender of searched person is \" + row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P569\"):\n",
        "      listHintsPerson.append(\"Searched person was born in \" + row[\"itemLabel.value\"][:4])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P19\"):\n",
        "      listHintsPerson.append(\"Searched person was born in \" + row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P1971\"):\n",
        "      listHintsPerson.append(\"Number of children of seared person is \" + row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P1340\"):\n",
        "      listHintsPerson.append(\"Searched person has \" + row[\"itemLabel.value\"] + \" eyes\")    \n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P2048\"):\n",
        "      listHintsPerson.append(\"Searched persons height is \" + row[\"itemLabel.value\"])    \n",
        "\n",
        "    \n",
        "    #if get multiple values save all in list and pick one afterwards\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P27\"):\n",
        "      listCitizenship.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P106\"):\n",
        "      listOccupation.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P39\"):\n",
        "      listPositionHeld.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P69\"):\n",
        "      listEducatedAt.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P512\"):\n",
        "      listAcademicDegrees.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P102\"):\n",
        "      listPoliticParty.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P3602\"):\n",
        "      listCandidacyElection.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P800\"):\n",
        "      listNotableWork.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P166\"):\n",
        "      listAwardsReceived.append(row[\"itemLabel.value\"])\n",
        "\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P1412\"):\n",
        "      listLanguages.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P413\"):\n",
        "      listPlayedPositions.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P118\"):\n",
        "      listLeague.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P54\"):\n",
        "      listTeams.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P3373\"):\n",
        "      listSiblings.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P1884\"):\n",
        "      listHair.append(row[\"itemLabel.value\"])\n",
        "\n",
        "  #if too many results could be return check\n",
        "  #if more thane 3 -> cut\n",
        "  listResultsCut = [listPositionHeld, listAcademicDegrees, listPoliticParty, \n",
        "                 listCandidacyElection, listNotableWork, listLanguages, \n",
        "                 listPlayedPositions, listLeague, listTeams, listCitizenship, listAwardsReceived, listEducatedAt, listOccupation]\n",
        "  for singleList in listResultsCut:\n",
        "    cutResults(singleList, listHintsPerson)\n",
        "\n",
        "  #just get one hair color\n",
        "  if(len(listHair) > 0):\n",
        "    listHintsPerson.append(\"The searched person has \" + listHair[0])\n",
        "\n",
        "  #get number of siblings\n",
        "  if(len(listSiblings) > 0):\n",
        "    listHintsPerson.append(\"The searched person has \" + str(len(listSiblings)) + \" siblings\")  \n",
        "  \n",
        "  return listHintsPerson"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#if more results than 3 -> print 3 results + \"and x others\"\n",
        "\n",
        "def cutResults(listResults, listHintsPerson):\n",
        "  \n",
        "  numResults = len(listResults)-1\n",
        "  resultsLeft = numResults-3\n",
        "\n",
        "  if (len(listResults) > 1):\n",
        "    if(resultsLeft > 0):\n",
        "      listHintsPerson.append(getConcatenationMultipleResults(listResults[0:4])+ \" and \" + str(resultsLeft) + \" others\")\n",
        "    else:\n",
        "      listHintsPerson.append(getConcatenationMultipleResults(listResults))"
      ],
      "metadata": {
        "id": "KDP4yoRpB_Sq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIOq3XcwIr2a"
      },
      "source": [
        "#concatenate multiple results with comma\n",
        "\n",
        "def getConcatenationMultipleResults(resultList):\n",
        "  concatenation = \"\"\n",
        "  commaCounter = len(resultList)-1\n",
        "  for item in resultList:\n",
        "    if(commaCounter == len(resultList)-1):\n",
        "      concatenation += item\n",
        "      commaCounter-=1\n",
        "    elif(commaCounter > 0):\n",
        "      concatenation += item + \", \"\n",
        "      commaCounter-=1\n",
        "    elif(commaCounter == 0):\n",
        "      concatenation += item\n",
        "  \n",
        "  return concatenation\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B74PSzscAzl8"
      },
      "source": [
        "def writePersonHintsInFile(listHintsPerson):\n",
        "  with open(\"hintsPerson.txt\", 'a') as writefile:\n",
        "    #writefile.write(\"\\n\\n---Hints for Person \" + person + \"---\\n\\n\")\n",
        "    randomHintsList = random.sample(listHintsPerson, len(listHintsPerson))\n",
        "    for item in randomHintsList:\n",
        "      writefile.write(item + \"\\n\")\n",
        "    writefile.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LORwHLog-t_"
      },
      "source": [
        "## Hints if answer is a Year"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dil5f183hhHJ"
      },
      "source": [
        "**Get all outlinks of wikipedia year page and pass to getBacklinks. Than look for return values on page.content and print line**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQFplu-7AF1L"
      },
      "source": [
        "import spacy\n",
        "def hintsYear(year):\n",
        "\n",
        "  page = wikipedia.page(year, auto_suggest=False)\n",
        "  pageContent = page.content\n",
        "\n",
        "  #get all links from year page\n",
        "  dictPageLinks = getOutlinks2(year)\n",
        "  #print(dictPageLinks)\n",
        "\n",
        "  #get rid of unnesercery links\n",
        "  filteredPageLinks = filterDict(dictPageLinks)\n",
        "\n",
        "  \n",
        "  #get backlinks of all links\n",
        "  dictBacklink = getBacklinksDict(filteredPageLinks)\n",
        "  #first200 = {k: filteredPageLinks[k] for k in list(filteredPageLinks)[:200]}\n",
        "  #dictBacklink = getBacklinksDict(first200)\n",
        "\n",
        "  #remove parantheses of links\n",
        "  dictAdjusted = removeParanthesesDict(dictBacklink)\n",
        "\n",
        "  #get all possible hint (all lines with a entity #backlinks > 1000)\n",
        "  dictPossibleHints = findLineInTextEvent(dictAdjusted, year)\n",
        "\n",
        "  with open('./resultsEvents.txt', 'a') as writefile:\n",
        "    writefile.write(\"Question: \" + questionYear + \"(\"+ str(year) + \")---\\n\")\n",
        "\n",
        "  #check if entity because of which line was found is subject\n",
        "  #if it is not the subject -> drop\n",
        "  dictFinalHints = getFinalHints(dictPossibleHints)\n",
        "\n",
        "  #get rid of number of foundevents at end of line\n",
        "  #need to put number at the and because it is dict and would be overwritten\n",
        "  dictFinalHintsAdjusted = {}\n",
        "  for key, val in dictFinalHints.items():\n",
        "    dictFinalHintsAdjusted.update({key.split('///', 1)[0]: val})  \n",
        "\n",
        "  #returns dict sorted after PageViews\n",
        "  dictSortedPV = sortHintsNumberPVSubject(dictFinalHintsAdjusted)\n",
        "\n",
        "  #get highest PageView\n",
        "  maxPV = list(dictSortedPV.values())[0]\n",
        "\n",
        "  #sort hints PageViews and Simscore combined\n",
        "  sortHintsUtilityScore(questionYear, dictSortedPV, maxPV)\n",
        "\n",
        "  listHumansNames = []\n",
        "\n",
        "  for key, val in dictAdjusted.items():\n",
        "    result = checkIfHuman(key)\n",
        "    if(result != None):\n",
        "      listHumansNames.append(result)\n",
        "\n",
        "  #findLineInTextBirthDeath(listHumansNames, year)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-dJk5vrhq6M"
      },
      "source": [
        "**Function to find line of most important link in year content**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0GonOTeikJm"
      },
      "source": [
        "import re\n",
        "\n",
        "def findLineInTextEvent(entitiesEvent, year):\n",
        "\n",
        "  page = wikipedia.page(year, auto_suggest=False)\n",
        "  pageText = page.content\n",
        "  splitText = []\n",
        "\n",
        "  splitText = (re.split(\"== Events ==|== Births ==\", pageText ))\n",
        "\n",
        "  contentEvents = splitText[1]\n",
        "\n",
        "  foundEvent = 0\n",
        "\n",
        "  dictHintsEntity = {}\n",
        "\n",
        "  for key, val in entitiesEvent.items():\n",
        "    #if (foundEvent == 20):\n",
        "      #break;\n",
        "\n",
        "    for line in contentEvents.split(\"\\n\"):\n",
        "      #if (foundEvent == 20):\n",
        "        #break;\n",
        "\n",
        "      if val in line:\n",
        "        words = line.split()\n",
        "        #counterEntitiesInLine = 0\n",
        "        if(len(words) < 15):\n",
        "          dictHintsEntity.update({line + \"///\" + str(foundEvent): val})\n",
        "          #listHints.append(line)\n",
        "          foundEvent += 1\n",
        "          #break;\n",
        "  return dictHintsEntity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZAM4MxB-8By"
      },
      "source": [
        "def findLineInTextBirthDeath(entitiesHuman, year):\n",
        "\n",
        "  page = wikipedia.page(year, auto_suggest=False)\n",
        "  pageText = page.content\n",
        "  splitText = []\n",
        "\n",
        "  splitText = (re.split(\"== Births ==|== Deaths ==\", pageText ))\n",
        "\n",
        "  print(splitText)\n",
        "\n",
        "  contentBirths = splitText[1]\n",
        "  contentDeaths = splitText[2]\n",
        "\n",
        "  foundBirth = 0\n",
        "  foundDeath = 0\n",
        "\n",
        "  #open file and append results\n",
        "  with open('./resultsBirthDeath.txt', 'a') as writefile:\n",
        "    writefile.write(\"---Birth/Death in year \" + str(year) + \"---\")\n",
        "\n",
        "    for entity in entitiesHuman:\n",
        "      #if (foundBirth == 3 and foundDeath == 3):\n",
        "        #writefile.write(\"\\n\")\n",
        "        #break;\n",
        "\n",
        "      for line in contentBirths.split(\"\\n\"):\n",
        "        #if (foundBirth == 3):\n",
        "          #break;\n",
        "\n",
        "        if entity in line:\n",
        "          writefile.write(\"Found with entity \" + entity + \"\\n\")\n",
        "          writefile.write(\"Birth: \" + line + \"\\n\")\n",
        "          writefile.write(\"\\n\")\n",
        "          foundBirth +=1\n",
        "          break;\n",
        "\n",
        "      for line in contentDeaths.split(\"\\n\"):\n",
        "        #if (foundDeath == 3):\n",
        "          #break;\n",
        "        if entity in line:\n",
        "          writefile.write(\"Found with entity \" + entity + \"\\n\")\n",
        "          writefile.write(\"Death: \" + line + \"\\n\")\n",
        "          writefile.write(\"\\n\")\n",
        "          foundDeath += 1\n",
        "          break;\n",
        "    writefile.write(\"\\n\")\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUy44eeb0EKg"
      },
      "source": [
        "def sortHintsNumberPVSubject(dictHints):\n",
        "\n",
        "  with open('./resultsEvents.txt', 'a') as writefile: \n",
        "\n",
        "    dictFinalHintsPV = {}\n",
        "\n",
        "    for key, val in dictHints.items():\n",
        "      dictFinalHintsPV.update({key: getPageViews(val)})\n",
        "\n",
        "    dictFinalHintsSortedPV = dict(sorted(dictFinalHintsPV.items(), key=lambda item: item[1], reverse=True))\n",
        "    \"\"\"writefile.write(\"\\n--Final Hints sorted #PV---\\n\")\n",
        "    for key, val in dictFinalHintsSortedPV.items():\n",
        "      writefile.write(key + \"(\" + str(val) + \" PageViews)\\n\")\"\"\"\n",
        "\n",
        "  return dictFinalHintsSortedPV\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4YpAjm5B6CM"
      },
      "source": [
        "\n",
        "\n",
        "def sortHintsUtilityScore(question, dictFinalHintsSortedPV, maxPV):\n",
        "  model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "  question_embedding = model.encode(question)\n",
        "\n",
        "  alpha = 0.5\n",
        "\n",
        "  #sentence_embeddings = model.encode(hints)\n",
        "\n",
        "  dictHintsBERT = {}\n",
        "\n",
        "  for key, val in dictFinalHintsSortedPV.items():\n",
        "    sentence_embedding = model.encode(key)\n",
        "    simScore = cosine_similarity([question_embedding], [sentence_embedding]).item()\n",
        "    impScore = alpha * (val/maxPV) + (1-alpha) * simScore\n",
        "    dictHintsBERT.update({key: impScore})\n",
        "\n",
        "  with open('./resultsEvents.txt', 'a') as writefile:\n",
        "\n",
        "    dictHintsBERT = dict(sorted(dictHintsBERT.items(), key=lambda item: item[1], reverse=True))\n",
        "    writefile.write(\"\\n--Final Hints sorted Utility Score---\\n\")\n",
        "    for key, val in dictHintsBERT.items():\n",
        "      writefile.write(key + \"(\" + str(val) + \" Utility Score)\\n\")\n",
        "\n",
        "  return dictHintsBERT\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxuRQ-FsZdQ_"
      },
      "source": [
        "def get_subject_phrase(doc):\n",
        "    for token in doc:\n",
        "        if (\"subj\" in token.dep_):\n",
        "            subtree = list(token.subtree)\n",
        "            start = subtree[0].i\n",
        "            end = subtree[-1].i + 1\n",
        "            return doc[start:end]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBif9Lc8_sg3"
      },
      "source": [
        "def getFinalHints(dictPossibleHints):\n",
        "\n",
        "  #nlp = spacy.load(\"en\")\n",
        "  nlp = spacy.load(\"en_core_web_sm\")\n",
        "  dictFinalHintsSubject = {}\n",
        "  \n",
        "  for key, val in dictPossibleHints.items():\n",
        "    doc = nlp(key)\n",
        "    subject = get_subject_phrase(doc)\n",
        "    if (val in str(subject)):\n",
        "      dictFinalHintsSubject.update({key: val})\n",
        "      #listFinalHints.append(key)\n",
        "\n",
        "  return dictFinalHintsSubject"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JUt_0x6jD-x"
      },
      "source": [
        "\n",
        "def getOutlinks2(year):\n",
        "\n",
        "  URL = 'https://en.wikipedia.org/wiki/' + str(year)\n",
        "\n",
        "  # Fetch all the HTML source from the url\n",
        "  response = requests.get(URL)\n",
        "\n",
        "\n",
        "  soup = bs4.BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "  listTextOfLinks = []\n",
        "  listLinks = []\n",
        "\n",
        "  dictLinkName = {}\n",
        "\n",
        "\n",
        "  for link in soup.find_all(\"a\"):\n",
        "    listTextOfLinks.append(link.get_text())\n",
        "    dictLinkName.update({str(link.get(\"href\")).split('/')[-1].replace('_', ' '): link.get_text()})\n",
        "\n",
        "  #newDict = filterDict(dictLinkName)\n",
        "\n",
        "  #print(len(dictLinkName))\n",
        "  #print(len(newDict))\n",
        "  #print(newDict)\n",
        "\n",
        "  return dictLinkName"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAuFgVXomCdf"
      },
      "source": [
        "def filterDict(dictNameLink):\n",
        "\n",
        "  patternYear = r\"[1-3][0-9]{3}\"\n",
        "  patternCentury = r\"[1-2][0-9]\\w+.century\"\n",
        "  patternAd = r\"AD.\\d\\d\\d\\d\"\n",
        "  patternMonth = r\"[a-zA-Z]+\\s\\d+\"\n",
        "  patternMillennium = r\".+millennium\"\n",
        "  patternHashtag = r\"#\"\n",
        "  patternCalendar = r\"calendar\"\n",
        "  patternCategory = r\"category\"\n",
        "  patternList = r\"list\"\n",
        "  patternTemplate = r\"template\"\n",
        "  patternWikipedia = r\"wikipedia\"\n",
        "  patternIndex = r\"index\"\n",
        "  patternHtml = r\"html\"\n",
        "  patternPercent = r\"%\"\n",
        "\n",
        "  newDict = {}\n",
        "\n",
        "  for key, val in dictNameLink.items():\n",
        "    if(not re.match(patternYear, key) and\n",
        "       not re.match(patternCentury, key) and \n",
        "       not re.match(patternAd, key) and \n",
        "       not re.match(patternMonth, key) and\n",
        "       not re.search(patternHashtag, key) and\n",
        "       not re.search(patternCalendar, key, re.IGNORECASE) and\n",
        "       not re.search(patternCategory, key, re.IGNORECASE) and\n",
        "       not re.search(patternList, key, re.IGNORECASE) and\n",
        "       not re.search(patternTemplate, key, re.IGNORECASE) and\n",
        "       not re.search(patternWikipedia, key, re.IGNORECASE) and\n",
        "       not re.search(patternIndex, key, re.IGNORECASE) and\n",
        "       not re.search(patternHtml, key, re.IGNORECASE) and\n",
        "       not re.search(patternPercent, key) and\n",
        "       not re.match(patternMillennium, key) and\n",
        "       not val == \"\" and not val == \" \"):\n",
        "      \n",
        "\n",
        "      newDict.update({key: val})\n",
        "\n",
        "\n",
        "  print(newDict)\n",
        "\n",
        "  return newDict\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Call Functions\n"
      ],
      "metadata": {
        "id": "cxaEE5DFfr_v"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bA1ZDHtbAkJJ"
      },
      "source": [
        "**Call hint function for all questions where answer is Location**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kHh8TAEt6OS"
      },
      "source": [
        "for index, row in location_df.iterrows():\n",
        "  location = row[\"Answer\"]\n",
        "  questionLocation = row[\"Question\"]\n",
        "  with open(\"hintsLocation.txt\", 'a') as writefile:\n",
        "    writefile.write(\"\\n\\n\" + questionLocation + \" (\" + location + \")\\n\\n\")\n",
        "    writefile.close()\n",
        "\n",
        "\n",
        "  #get Wikidata ID for SPARQL-Query\n",
        "  locationQitem = getQitemOfName(location)\n",
        "\n",
        "  #execute SPARQL-Query\n",
        "  queryResults_df = allHintsLocation(locationQitem)\n",
        "\n",
        "  #getFinalHints\n",
        "  listHintsLocation = getFormatedHintsLocation(queryResults_df)    \n",
        "\n",
        "  #write hints in file in random order\n",
        "  writeLocationHintsInFile(listHintsLocation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYzskrP0AtKs"
      },
      "source": [
        "**Call hint function for all questions where answer is Person**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B__QLqKMAX51"
      },
      "source": [
        "\n",
        "for index, row in person_df.iterrows():\n",
        "  person = row[\"Answer\"]\n",
        "  questionPerson = row[\"Question\"]\n",
        "\n",
        "  with open(\"hintsPerson.txt\", 'a') as writefile:\n",
        "    writefile.write(\"\\n\\n\" + questionPerson + \" (\"+ person +\")\\n\\n\")\n",
        "    writefile.close()\n",
        "  \n",
        "  #get Wikipedia ID for SPARQL-Query\n",
        "  personQitem = getQitemOfName(person)\n",
        "\n",
        "  #execute SPARQL-Query\n",
        "  queryResults_df = allHintsPerson(personQitem)\n",
        "  \n",
        "  #get Final Hints\n",
        "  listHintsPerson = getFormatedHintsPerson(queryResults_df)\n",
        "\n",
        "  #write hints in file in random order\n",
        "  writePersonHintsInFile(listHintsPerson)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjsplvG5DUJn"
      },
      "source": [
        "**Call hint function for all question where answer is Year**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z5x7mjMDY-b"
      },
      "source": [
        "for index, row in year_df.iterrows():\n",
        "  year = row[\"Answer\"]\n",
        "  questionYear = row[\"Question\"]\n",
        "  hintsYear(row[\"Answer\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pageview Test Functions\n"
      ],
      "metadata": {
        "id": "GXcHCM1dcbai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import requests\n",
        "\n",
        "from selenium import webdriver\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def get_table_info(url):\n",
        "    \"\"\"\n",
        "    Given a URL, this function opens the url and retrieves the information stored in a table.\n",
        "    \"\"\"\n",
        "    options = webdriver.FirefoxOptions()\n",
        "    #options.headless = True\n",
        "    options.add_argument('--headless')\n",
        "\n",
        "    driver = webdriver.Firefox(options=options)\n",
        "    driver.get(url)\n",
        "    time.sleep(3) # Wait for the page to load completely\n",
        "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "    driver.quit()\n",
        "    table = soup.find('table')\n",
        "    rows = table.find_all('tr')\n",
        "    headers = [header.text.strip() for header in rows[0].find_all('th')]\n",
        "    data = []\n",
        "    for row in rows[1:]:\n",
        "        data.append([cell.text.strip() for cell in row.find_all('td')])\n",
        "    return (headers, data)\n",
        "\n",
        "def get_table_info_requests(url):\n",
        "    \"\"\"\n",
        "    Given a URL, this function opens the url and retrieves the information stored in a table.\n",
        "    \"\"\"\n",
        "    headers = []\n",
        "    data = []\n",
        "\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    table = soup.find('table')\n",
        "    rows = table.find_all('tr')\n",
        "    headers = [header.text.strip() for header in rows[0].find_all('th')]\n",
        "    data = []\n",
        "    for row in rows[1:]:\n",
        "        data.append([cell.text.strip() for cell in row.find_all('td')])\n",
        "\n",
        "    return (headers, data)\n",
        "\n",
        "def get_links_in_section(wikipedia_url, section_heading):\n",
        "    \"\"\"\n",
        "    Given a Wikipedia URL, and a section of this article, returns an array of dictionaries containing the href, title, and description of each link on that section of the page.\n",
        "    \"\"\"\n",
        "    response = requests.get(wikipedia_url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    section = soup.find('span', {'id': section_heading})\n",
        "    if section is None:\n",
        "        return None\n",
        "    section_content = section.parent.find_next_sibling('ul')\n",
        "    if section_content is None:\n",
        "        return None\n",
        "    links = []\n",
        "    for item in section_content.find_all('li'):\n",
        "        link = item.find('a')\n",
        "        if link is not None and link.has_attr('href') and link['href'].startswith('/wiki/'):\n",
        "            #title = link.get('title', '')\n",
        "            title = link.get('title')\n",
        "            description = item.text.strip()\n",
        "            url = f\"https://en.wikipedia.org{link['href']}\"\n",
        "            links.append({'title': title, 'description': description, 'url': url})\n",
        "    #print(links)\n",
        "    return links\n",
        "\n",
        "def get_links_in_section_with_sublinks(wikipedia_url, section_title):\n",
        "    \"\"\"\n",
        "    Given a Wikipedia URL, and a section of this article, returns an array of dictionaries containing the href, title, and description of each link  with its sublinks as well.\n",
        "    \"\"\"\n",
        "    response = requests.get(wikipedia_url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        section_heading = soup.find('span', {'id': section_title})\n",
        "        if section_heading is None:\n",
        "            return None\n",
        "        section = section_heading.parent\n",
        "        section_content = section.find_next_sibling('ul')\n",
        "        if section_content is None:\n",
        "            return None\n",
        "        links = []\n",
        "        for item in section_content.find_all('li'):\n",
        "            link = item.find('a')\n",
        "            if link is not None:\n",
        "                link_title = link.get('title')\n",
        "                link_url = 'https://en.wikipedia.org' + link.get('href')\n",
        "                link_description = item.text.replace(link_title, '').strip()\n",
        "                links.append({'title': link_title, 'url': link_url, 'description': link_description})\n",
        "                for sublink in item.find_all('a', href=True, recursive=False):\n",
        "                    sublink_title = sublink.get('title')\n",
        "                    sublink_url = 'https://en.wikipedia.org' + sublink.get('href')\n",
        "                    sublink_description = sublink.parent.text.replace(sublink_title, '').replace(link_title, '').strip()\n",
        "                    links.append({'title': sublink_title, 'url': sublink_url, 'description': sublink_description})\n",
        "        return links\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def get_events_links(title):\n",
        "    \"\"\"\n",
        "    Gets all the links in the \"Events\" section of a Wikipedia page.\n",
        "    Returns a list of strings containing the URLs of each event.\n",
        "    \"\"\"\n",
        "    url = f\"https://en.wikipedia.org/w/api.php?action=parse&format=json&page={title}&prop=text\"\n",
        "    response = requests.get(url)\n",
        "    data = response.json()\n",
        "\n",
        "    soup = BeautifulSoup(data['parse']['text']['*'], 'html.parser')\n",
        "    events_section = soup.find('span', {'class': 'mw-headline', 'id': 'Events'})\n",
        "    if events_section:\n",
        "        events_list = events_section.find_next('ul')\n",
        "        if events_list:\n",
        "            events = events_list.find_all('a')\n",
        "            event_links = [f\"https://en.wikipedia.org{event['href']}\" for event in events]\n",
        "            return event_links\n",
        "    return None\n",
        "\n",
        "def create_url_from_title(title):\n",
        "    \"\"\"\n",
        "    Given a list of dictionaries containing href, title, and description for each entry, extracts the title from each link and constructs a new URL using the title.\n",
        "    \"\"\"\n",
        "    urls = []\n",
        "    if title != None: \n",
        "      # Replace spaces with underscores in the title\n",
        "      title = title.replace(' ', '_')\n",
        "\n",
        "      # Construct a new URL using the title\n",
        "      url = 'https://pageviews.wmcloud.org/redirectviews/?project=en.wikipedia.org&platform=all-access&agent=user&range=all-time&sort=views&direction=1&view=list&page=' + title\n",
        "    return url\n",
        "\n",
        "def extract_visitorNumber_from_link(link):\n",
        "    \"\"\"\n",
        "    extract the first entry of the table\n",
        "    \"\"\"\n",
        "    headers, data = get_table_info(link)\n",
        "\n",
        "    try:\n",
        "      if data[1]: \n",
        "        return data[1][2]\n",
        "      else:\n",
        "        return 0\n",
        "    except IndexError:\n",
        "      print(\"Index out of range\")\n",
        "\n",
        "def get_visitor_numbers(pageviews_url):\n",
        "    \"\"\"\n",
        "    returns visitor numbers of the page\n",
        "    \"\"\"\n",
        "    response = requests.get(pageviews_url)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()['items']\n",
        "        visitors = {}\n",
        "        for item in data:\n",
        "            visitors[item['timestamp']] = item['views']\n",
        "        return visitors\n",
        "    else:\n",
        "        return None"
      ],
      "metadata": {
        "id": "5VwT46tA0CZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TEST\n",
        "\n",
        "#Seitenaufrufe\n",
        "#url = 'https://pageviews.toolforge.org/?project=en.wikipedia.org&pages=Formula1|Michael_Schumacher'\n",
        "#headers, data = get_table_info(url)\n",
        "#print(headers)\n",
        "#print(data)\n",
        "\n",
        "#get all wiki links on a page -> discard the one we dont need -> analyse the ones that could be usefull -> sort them after the backlinks\n",
        "#print(get_events_links(2004))\n",
        "\n",
        "# Search the most popular births and deaths in that year and then order them \n",
        "url1 = 'https://en.wikipedia.org/wiki/Category:1994_births'\n",
        "url2 = 'https://en.wikipedia.org/wiki/Category:1994_deaths'\n",
        "url3 = 'https://en.wikipedia.org/wiki/1994#Events'\n",
        "\n",
        "#links_events_section = get_links_in_section_with_sublinks(url3, 'Events')\n",
        "#print(len(links_events_section))\n",
        "#print(links_events_section)\n",
        "#print(get_births(1994))\n",
        "\n",
        "#visitor number test\n",
        "link = 'https://pageviews.wmcloud.org/redirectviews/?project=en.wikipedia.org&platform=all-access&agent=user&range=all-time&sort=views&direction=1&view=list&page=North_American_Free_Trade_Agreement'\n",
        "#print(extract_visitorNumber_from_link(link))"
      ],
      "metadata": {
        "id": "dYFPOhzAVotw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#When did Michael Schumacher win his first F1 World Drivers Title? -> 1994\n",
        "#First 1994 is searched on wikipedia, then all the links from the event sections of that page are filtered and written into a dict together with their pageviews.\n",
        "#Now order these lists such that the entry with most pageviews is on top\n",
        "#THIS SHOULD WORK\n",
        "\n",
        "year=2004\n",
        "url = 'https://en.wikipedia.org/wiki/2004'\n",
        "\n",
        "links_events_section = get_links_in_section_with_sublinks(url, 'Events')\n",
        "print(len(links_events_section))\n",
        "\n",
        "for link in links_events_section:\n",
        "    t_title =  link.get('title') \n",
        "    t_url = create_url_from_title(t_title)\n",
        "    link[\"pageview_url\"] = t_url\n",
        "    link[\"pageviews\"] = extract_visitorNumber_from_link(t_url)\n",
        "\n",
        "#sort the list links_event_section after the pageviews such that the entity with the highest pageviews is first\n",
        "sorted_list = sorted(links_events_section, key=lambda x: int(x['pageviews'].replace(',', '')), reverse=True)"
      ],
      "metadata": {
        "id": "8hZTwIILMaNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint\n",
        "pageview_test_generated_hints_for_years = sorted_list\n",
        "\n",
        "pprint.pprint(pageview_test_generated_hints_for_years, indent=4)"
      ],
      "metadata": {
        "id": "IGiGzaMzbF-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R87Y1LPF55PL"
      },
      "source": [
        "# Clean GitHub directory\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "%rm -r optimizationOfHintGeneration\n",
        "%cd /content/\n",
        "%rm hintsLocation.txt\n",
        "%rm hintsPerson.txt\n",
        "%rm resultsEvents.txt"
      ],
      "metadata": {
        "id": "2cREJk4T5v7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dXyrxO6XmXKj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}