{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "cxaEE5DFfr_v",
        "WckjgVs-fZto",
        "s5rwSDisuN9s",
        "du5Un8VufnlG",
        "_lhmuuUNQ__8",
        "0Yf6Fi_34ye5",
        "0LORwHLog-t_"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexWalcher/optimizationOfHintGeneration/blob/Test/optimizationOfHintGeneration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Call Functions\n"
      ],
      "metadata": {
        "id": "cxaEE5DFfr_v"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bA1ZDHtbAkJJ"
      },
      "source": [
        "**Call hint function for all questions where answer is Location**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kHh8TAEt6OS"
      },
      "source": [
        "for index, row in location_df.iterrows():\n",
        "  location = row[\"Answer\"]\n",
        "  questionLocation = row[\"Question\"]\n",
        "  with open(\"hintsLocation.txt\", 'a') as writefile:\n",
        "    writefile.write(\"\\n\\n\" + questionLocation + \" (\" + location + \")\\n\\n\")\n",
        "    writefile.close()\n",
        "\n",
        "\n",
        "  #get Wikidata ID for SPARQL-Query\n",
        "  locationQitem = getQitemOfName(location)\n",
        "\n",
        "  #execute SPARQL-Query\n",
        "  queryResults_df = allHintsLocation(locationQitem)\n",
        "\n",
        "  #getFinalHints\n",
        "  listHintsLocation = getFormatedHintsLocation(queryResults_df)    \n",
        "\n",
        "  #write hints in file in random order\n",
        "  writeLocationHintsInFile(listHintsLocation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYzskrP0AtKs"
      },
      "source": [
        "**Call hint function for all questions where answer is Person**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B__QLqKMAX51"
      },
      "source": [
        "\n",
        "for index, row in person_df.iterrows():\n",
        "  person = row[\"Answer\"]\n",
        "  questionPerson = row[\"Question\"]\n",
        "\n",
        "  with open(\"hintsPerson.txt\", 'a') as writefile:\n",
        "    writefile.write(\"\\n\\n\" + questionPerson + \" (\"+ person +\")\\n\\n\")\n",
        "    writefile.close()\n",
        "  \n",
        "  #get Wikipedia ID for SPARQL-Query\n",
        "  personQitem = getQitemOfName(person)\n",
        "\n",
        "  #execute SPARQL-Query\n",
        "  queryResults_df = allHintsPerson(personQitem)\n",
        "  \n",
        "  #get Final Hints\n",
        "  listHintsPerson = getFormatedHintsPerson(queryResults_df)\n",
        "\n",
        "  #write hints in file in random order\n",
        "  writePersonHintsInFile(listHintsPerson)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjsplvG5DUJn"
      },
      "source": [
        "**Call hint function for all question where answer is Year**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z5x7mjMDY-b"
      },
      "source": [
        "for index, row in year_df.iterrows():\n",
        "  year = row[\"Answer\"]\n",
        "  questionYear = row[\"Question\"]\n",
        "  hintsYear(row[\"Answer\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load File"
      ],
      "metadata": {
        "id": "WckjgVs-fZto"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_K8U-ZpsRss_"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.ExcelFile(\"./optimizationOfHintGeneration/testSet.xlsx\").parse(\"Sheet1\")\n",
        "x = []\n",
        "x.append(df[\"Answer\"])\n",
        "\n",
        "dataPerson = []\n",
        "dataYear = []\n",
        "dataLocation = []\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "  if(row[\"Category\"] == \"Person\"):\n",
        "    dataPerson.append([row[\"Question\"], row[\"Answer\"]])\n",
        "  elif(row[\"Category\"] == \"Year\"):\n",
        "    dataYear.append([row[\"Question\"], row[\"Answer\"]])\n",
        "  elif(row[\"Category\"] == \"Location\"):\n",
        "    dataLocation.append([row[\"Question\"], row[\"Answer\"]])\n",
        "\n",
        "person_df = pd.DataFrame(dataPerson, columns=[\"Question\", \"Answer\"])\n",
        "year_df = pd.DataFrame(dataYear, columns=[\"Question\", \"Answer\"])\n",
        "location_df = pd.DataFrame(dataLocation, columns=[\"Question\", \"Answer\"])"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pageview Test Functions\n"
      ],
      "metadata": {
        "id": "GXcHCM1dcbai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import requests\n",
        "\n",
        "from selenium import webdriver\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def get_table_info(url):\n",
        "    \"\"\"\n",
        "    Given a URL, this function opens the url and retrieves the information stored in a table.\n",
        "    \"\"\"\n",
        "    options = webdriver.FirefoxOptions()\n",
        "    options.headless = True\n",
        "\n",
        "    driver = webdriver.Firefox(options=options)\n",
        "    driver.get(url)\n",
        "    time.sleep(5) # Wait for the page to load completely\n",
        "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "    driver.quit()\n",
        "    table = soup.find('table')\n",
        "    rows = table.find_all('tr')\n",
        "    headers = [header.text.strip() for header in rows[0].find_all('th')]\n",
        "    data = []\n",
        "    for row in rows[1:]:\n",
        "        data.append([cell.text.strip() for cell in row.find_all('td')])\n",
        "    return (headers, data)\n",
        "\n",
        "\n",
        "def get_table_info_requests(url):\n",
        "    \"\"\"\n",
        "    Given a URL, this function opens the url and retrieves the information stored in a table.\n",
        "    \"\"\"\n",
        "    headers = []\n",
        "    data = []\n",
        "\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    table = soup.find('table')\n",
        "    rows = table.find_all('tr')\n",
        "    headers = [header.text.strip() for header in rows[0].find_all('th')]\n",
        "    data = []\n",
        "    for row in rows[1:]:\n",
        "        data.append([cell.text.strip() for cell in row.find_all('td')])\n",
        "\n",
        "    return (headers, data)\n",
        "\n",
        "\n",
        "\n",
        "def get_links_in_section(wikipedia_url, section_heading):\n",
        "    \"\"\"\n",
        "    Given a Wikipedia URL, and a section of this article, returns an array of dictionaries containing the href, title, and description of each link on that section of the page.\n",
        "    \"\"\"\n",
        "    response = requests.get(wikipedia_url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    section = soup.find('span', {'id': section_heading})\n",
        "    if section is None:\n",
        "        return None\n",
        "    section_content = section.parent.find_next_sibling('ul')\n",
        "    if section_content is None:\n",
        "        return None\n",
        "    links = []\n",
        "    for item in section_content.find_all('li'):\n",
        "        link = item.find('a')\n",
        "        if link is not None and link.has_attr('href') and link['href'].startswith('/wiki/'):\n",
        "            title = link.get('title', '')\n",
        "            description = item.text.strip()\n",
        "            url = f\"https://en.wikipedia.org{link['href']}\"\n",
        "            links.append({'title': title, 'description': description, 'url': url})\n",
        "    print(links)\n",
        "    return links\n",
        "\n",
        "\n",
        "def get_links_in_section_with_sublinks(wikipedia_url, section_title):\n",
        "    \"\"\"\n",
        "    Given a Wikipedia URL, and a section of this article, returns an array of dictionaries containing the href, title, and description of each link  with its sublinks as well.\n",
        "    \"\"\"\n",
        "    response = requests.get(wikipedia_url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        section_heading = soup.find('span', {'id': section_title})\n",
        "        if section_heading is None:\n",
        "            return None\n",
        "        section = section_heading.parent\n",
        "        section_content = section.find_next_sibling('ul')\n",
        "        if section_content is None:\n",
        "            return None\n",
        "        links = []\n",
        "        for item in section_content.find_all('li'):\n",
        "            link = item.find('a')\n",
        "            if link is not None:\n",
        "                link_title = link.get('title')\n",
        "                link_url = 'https://en.wikipedia.org' + link.get('href')\n",
        "                link_description = item.text.replace(link_title, '').strip()\n",
        "                links.append({'title': link_title, 'url': link_url, 'description': link_description})\n",
        "                for sublink in item.find_all('a', href=True, recursive=False):\n",
        "                    sublink_title = sublink.get('title')\n",
        "                    sublink_url = 'https://en.wikipedia.org' + sublink.get('href')\n",
        "                    sublink_description = sublink.parent.text.replace(sublink_title, '').replace(link_title, '').strip()\n",
        "                    links.append({'title': sublink_title, 'url': sublink_url, 'description': sublink_description})\n",
        "        #print(links)\n",
        "        return links\n",
        "    else:\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "5VwT46tA0CZz"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Seitenaufrufe\n",
        "url = 'https://pageviews.toolforge.org/?project=en.wikipedia.org&pages=Formula1|Michael_Schumacher'\n",
        "headers, data = get_table_info(url)\n",
        "print(headers)\n",
        "print(data)\n"
      ],
      "metadata": {
        "id": "6oWcLmN24z9s",
        "outputId": "c1b187a8-d3fb-49ee-ca18-f14cdbb0abc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-e274fc943da5>:12: DeprecationWarning: headless property is deprecated, instead use add_argument('-headless')\n",
            "  options.headless = True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['', 'Page title', 'Class', 'Views', 'Daily average', 'Edits', 'Editors', 'Size', 'Protection', 'Watchers', 'Links']\n",
            "[['', 'Michael Schumacher', '', '208,471', '9,927', '4', '4', '224,390', 'none', '742', 'All languages\\n        •\\n        Redirects'], ['', 'Formula1', '', '100', '5', '0', '0', '25', 'none', 'Unknown', 'All languages\\n        •\\n        Redirects'], []]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#When did Michael Schumacher win his first F1 World Drivers Title? -> 1994\n",
        "#First 1994 is searched on wikipedia, then all the links from the event sections of that page are filtered and written into a dict together with their pageviews.\n",
        "#Now order these lists such that the entry with most pageviews is on top\n",
        "# THIS WORKS\n",
        "\n",
        "#6:33\n",
        "#\n",
        "\n",
        "year=1994\n",
        "url = 'https://en.wikipedia.org/wiki/2004'\n",
        "\n",
        "links_events_section = get_links_in_section_with_sublinks(url, 'Events')\n",
        "print(len(links_events_section))\n",
        "\n",
        "for link in links_events_section:\n",
        "    t_title =  link.get('title') \n",
        "    t_url = create_url_from_title(t_title)\n",
        "    link[\"pageview_url\"] = t_url\n",
        "    link[\"pageviews\"] = extract_visitorNumber_from_link(t_url)\n",
        "\n",
        "#sort the list links_event_section after the pageviews such that the entity with the highest pageviews is first\n",
        "sorted_list = sorted(links_events_section, key=lambda x: int(x['pageviews'].replace(',', '')), reverse=True)\n",
        "\n",
        "print(sorted_list) \n",
        "#print(links_events_section)"
      ],
      "metadata": {
        "id": "8hZTwIILMaNL",
        "outputId": "06a2c69a-d7b6-4b02-c9ca-7dc5c5f50901",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-17eb1ddd7e4e>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlink\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlinks_events_section\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mt_title\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mlink\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mt_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_url_from_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_title\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mlink\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pageview_url\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mlink\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pageviews\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_visitorNumber_from_link\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'create_url_from_title' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sort the list links_event_section after the pageviews such that the entity with the highest pageviews is first\n",
        "sorted_list = sorted(links_events_section, key=lambda x: int(x['pageviews'].replace(',', '')), reverse=True)\n",
        "\n",
        "print(sorted_list)"
      ],
      "metadata": {
        "id": "jE1OFZJ9HjeW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "abab2460-e58a-4cd7-c81a-1c1c2de80ed7"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-42ba5c94814d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#sort the list links_event_section after the pageviews such that the entity with the highest pageviews is first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msorted_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinks_events_section\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pageviews'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-49-42ba5c94814d>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#sort the list links_event_section after the pageviews such that the entity with the highest pageviews is first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msorted_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinks_events_section\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pageviews'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'pageviews'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TEST\n",
        "\n",
        "#get all wiki links on a page -> discard the one we dont need -> analyse the ones that could be usefull -> sort them after the backlinks\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def get_events_links(title):\n",
        "    \"\"\"\n",
        "    Gets all the links in the \"Events\" section of a Wikipedia page.\n",
        "    Returns a list of strings containing the URLs of each event.\n",
        "    \"\"\"\n",
        "    url = f\"https://en.wikipedia.org/w/api.php?action=parse&format=json&page={title}&prop=text\"\n",
        "    response = requests.get(url)\n",
        "    data = response.json()\n",
        "\n",
        "    soup = BeautifulSoup(data['parse']['text']['*'], 'html.parser')\n",
        "    events_section = soup.find('span', {'class': 'mw-headline', 'id': 'Events'})\n",
        "    if events_section:\n",
        "        events_list = events_section.find_next('ul')\n",
        "        if events_list:\n",
        "            events = events_list.find_all('a')\n",
        "            event_links = [f\"https://en.wikipedia.org{event['href']}\" for event in events]\n",
        "            return event_links\n",
        "    \n",
        "    return None\n",
        "\n",
        "\n",
        "print(get_events_links(2004))"
      ],
      "metadata": {
        "id": "dYFPOhzAVotw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39cec41f-0da2-433a-a309-4f70fa82cdc1"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['https://en.wikipedia.org/wiki/January_3', 'https://en.wikipedia.org/wiki/Flash_Airlines_Flight_604', 'https://en.wikipedia.org/wiki/Red_Sea', 'https://en.wikipedia.org#cite_note-3', 'https://en.wikipedia.org/wiki/January_4', 'https://en.wikipedia.org/wiki/Spirit_(rover)', 'https://en.wikipedia.org/wiki/Mars', 'https://en.wikipedia.org#cite_note-4', 'https://en.wikipedia.org/wiki/January_6', 'https://en.wikipedia.org/wiki/Burj_Khalifa', 'https://en.wikipedia.org/wiki/Dubai', 'https://en.wikipedia.org/wiki/UAE', 'https://en.wikipedia.org/wiki/January_8', 'https://en.wikipedia.org/wiki/Queen_Mary_2', 'https://en.wikipedia.org/wiki/Elizabeth_II', 'https://en.wikipedia.org#cite_note-5', 'https://en.wikipedia.org/wiki/January_25', 'https://en.wikipedia.org/wiki/Opportunity_(rover)', 'https://en.wikipedia.org#cite_note-6', 'https://en.wikipedia.org/wiki/January_26', 'https://en.wikipedia.org/wiki/Mydoom', 'https://en.wikipedia.org#cite_note-7', 'https://en.wikipedia.org/wiki/January_28', 'https://en.wikipedia.org/wiki/Winx_Club', 'https://en.wikipedia.org/wiki/Rai_2']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Search the most popular births and deaths in that year and then order them \n",
        "\n",
        "url1 = 'https://en.wikipedia.org/wiki/Category:1994_births'\n",
        "url2 = 'https://en.wikipedia.org/wiki/Category:1994_deaths'\n",
        "url3 = 'https://en.wikipedia.org/wiki/1994#Events'\n",
        "\n",
        "links_events_section = get_links_in_section_with_sublinks(url3, 'Events')\n",
        "print(len(links_events_section))\n",
        "\n",
        "print(links_events_section)\n",
        "\n",
        "#print(get_births(1994))"
      ],
      "metadata": {
        "id": "DcYhrh1mPfmP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5017cd22-d03c-4433-e21c-200d5a2335da"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30\n",
            "[{'title': 'January 1', 'url': 'https://en.wikipedia.org/wiki/January_1', 'description': '– The North American Free Trade Agreement (NAFTA) is established.'}, {'title': 'January 1', 'url': 'https://en.wikipedia.org/wiki/January_1', 'description': '– The North American Free Trade Agreement (NAFTA) is established.'}, {'title': 'North American Free Trade Agreement', 'url': 'https://en.wikipedia.org/wiki/North_American_Free_Trade_Agreement', 'description': '– The  (NAFTA) is established.'}, {'title': 'January 8', 'url': 'https://en.wikipedia.org/wiki/January_8', 'description': '– Soyuz TM-18: Valeri Polyakov begins his 437.7-day orbit of the Earth, eventually setting the world record for days spent in orbit.'}, {'title': 'January 8', 'url': 'https://en.wikipedia.org/wiki/January_8', 'description': '– Soyuz TM-18: Valeri Polyakov begins his 437.7-day orbit of the Earth, eventually setting the world record for days spent in orbit.'}, {'title': 'Valeri Polyakov', 'url': 'https://en.wikipedia.org/wiki/Valeri_Polyakov', 'description': '– Soyuz TM-18:  begins his 437.7-day orbit of the Earth, eventually setting the world record for days spent in orbit.'}, {'title': 'Earth', 'url': 'https://en.wikipedia.org/wiki/Earth', 'description': '– Soyuz TM-18: Valeri Polyakov begins his 437.7-day orbit of the , eventually setting the world record for days spent in orbit.'}, {'title': 'January 11', 'url': 'https://en.wikipedia.org/wiki/January_11', 'description': '– The Irish government announces the end of a 15-year broadcasting ban on the Provisional Irish Republican Army and its political arm Sinn Féin.'}, {'title': 'January 11', 'url': 'https://en.wikipedia.org/wiki/January_11', 'description': '– The Irish government announces the end of a 15-year broadcasting ban on the Provisional Irish Republican Army and its political arm Sinn Féin.'}, {'title': 'Provisional Irish Republican Army', 'url': 'https://en.wikipedia.org/wiki/Provisional_Irish_Republican_Army', 'description': '– The Irish government announces the end of a 15-year broadcasting ban on the  and its political arm Sinn Féin.'}, {'title': 'Sinn Féin', 'url': 'https://en.wikipedia.org/wiki/Sinn_F%C3%A9in', 'description': '– The Irish government announces the end of a 15-year broadcasting ban on the Provisional Irish Republican Army and its political arm .'}, {'title': 'January 14', 'url': 'https://en.wikipedia.org/wiki/January_14', 'description': \"– U.S. President Bill Clinton and Russian President Boris Yeltsin sign the Kremlin accords, which stop the preprogrammed aiming of nuclear missiles toward each country's targets, and also provide for the dismantling of the nuclear arsenal in Ukraine.\"}, {'title': 'January 14', 'url': 'https://en.wikipedia.org/wiki/January_14', 'description': \"– U.S. President Bill Clinton and Russian President Boris Yeltsin sign the Kremlin accords, which stop the preprogrammed aiming of nuclear missiles toward each country's targets, and also provide for the dismantling of the nuclear arsenal in Ukraine.\"}, {'title': 'Bill Clinton', 'url': 'https://en.wikipedia.org/wiki/Bill_Clinton', 'description': \"– U.S. President  and Russian President Boris Yeltsin sign the Kremlin accords, which stop the preprogrammed aiming of nuclear missiles toward each country's targets, and also provide for the dismantling of the nuclear arsenal in Ukraine.\"}, {'title': 'Boris Yeltsin', 'url': 'https://en.wikipedia.org/wiki/Boris_Yeltsin', 'description': \"– U.S. President Bill Clinton and Russian President  sign the Kremlin accords, which stop the preprogrammed aiming of nuclear missiles toward each country's targets, and also provide for the dismantling of the nuclear arsenal in Ukraine.\"}, {'title': 'Kremlin accords', 'url': 'https://en.wikipedia.org/wiki/Kremlin_accords', 'description': \"– U.S. President Bill Clinton and Russian President Boris Yeltsin sign the , which stop the preprogrammed aiming of nuclear missiles toward each country's targets, and also provide for the dismantling of the nuclear arsenal in Ukraine.\"}, {'title': 'Intercontinental ballistic missile', 'url': 'https://en.wikipedia.org/wiki/Intercontinental_ballistic_missile', 'description': \"– U.S. President Bill Clinton and Russian President Boris Yeltsin sign the Kremlin accords, which stop the preprogrammed aiming of nuclear missiles toward each country's targets, and also provide for the dismantling of the nuclear arsenal in Ukraine.\"}, {'title': 'Nuclear arsenal', 'url': 'https://en.wikipedia.org/wiki/Nuclear_arsenal', 'description': \"– U.S. President Bill Clinton and Russian President Boris Yeltsin sign the Kremlin accords, which stop the preprogrammed aiming of nuclear missiles toward each country's targets, and also provide for the dismantling of the nuclear arsenal in Ukraine.\"}, {'title': 'Ukraine', 'url': 'https://en.wikipedia.org/wiki/Ukraine', 'description': \"– U.S. President Bill Clinton and Russian President Boris Yeltsin sign the Kremlin accords, which stop the preprogrammed aiming of nuclear missiles toward each country's targets, and also provide for the dismantling of the nuclear arsenal in .\"}, {'title': 'January 17', 'url': 'https://en.wikipedia.org/wiki/January_17', 'description': '– The 6.7 Mw Northridge earthquake strikes the Greater Los Angeles Area with a maximum Mercalli intensity of IX (Violent), leaving 57 people dead and more than 8,700 injured.'}, {'title': 'January 17', 'url': 'https://en.wikipedia.org/wiki/January_17', 'description': '– The 6.7 Mw Northridge earthquake strikes the Greater Los Angeles Area with a maximum Mercalli intensity of IX (Violent), leaving 57 people dead and more than 8,700 injured.'}, {'title': 'Seismic magnitude scales', 'url': 'https://en.wikipedia.org/wiki/Seismic_magnitude_scales#Mw', 'description': '– The 6.7 Mw Northridge earthquake strikes the Greater Los Angeles Area with a maximum Mercalli intensity of IX (Violent), leaving 57 people dead and more than 8,700 injured.'}, {'title': '1994 Northridge earthquake', 'url': 'https://en.wikipedia.org/wiki/1994_Northridge_earthquake', 'description': '– The 6.7 Mw Northridge earthquake strikes the Greater Los Angeles Area with a maximum Mercalli intensity of IX (Violent), leaving 57 people dead and more than 8,700 injured.'}, {'title': 'Greater Los Angeles Area', 'url': 'https://en.wikipedia.org/wiki/Greater_Los_Angeles_Area', 'description': '– The 6.7 Mw Northridge earthquake strikes the  with a maximum Mercalli intensity of IX (Violent), leaving 57 people dead and more than 8,700 injured.'}, {'title': 'Mercalli intensity scale', 'url': 'https://en.wikipedia.org/wiki/Mercalli_intensity_scale', 'description': '– The 6.7 Mw Northridge earthquake strikes the Greater Los Angeles Area with a maximum Mercalli intensity of IX (Violent), leaving 57 people dead and more than 8,700 injured.'}, {'title': 'January 19', 'url': 'https://en.wikipedia.org/wiki/January_19', 'description': '– Record cold temperatures hit the eastern United States. The coldest temperature ever measured in Indiana state history, −36\\xa0°F (−38\\xa0°C), is recorded in New Whiteland, Indiana.'}, {'title': 'January 19', 'url': 'https://en.wikipedia.org/wiki/January_19', 'description': '– Record cold temperatures hit the eastern United States. The coldest temperature ever measured in Indiana state history, −36\\xa0°F (−38\\xa0°C), is recorded in New Whiteland, Indiana.'}, {'title': '1994 North American cold wave', 'url': 'https://en.wikipedia.org/wiki/1994_North_American_cold_wave', 'description': '– Record cold temperatures hit the eastern United States. The coldest temperature ever measured in Indiana state history, −36\\xa0°F (−38\\xa0°C), is recorded in New Whiteland, Indiana.'}, {'title': 'Indiana', 'url': 'https://en.wikipedia.org/wiki/Indiana', 'description': '– Record cold temperatures hit the eastern United States. The coldest temperature ever measured in  state history, −36\\xa0°F (−38\\xa0°C), is recorded in New Whiteland, .'}, {'title': 'New Whiteland, Indiana', 'url': 'https://en.wikipedia.org/wiki/New_Whiteland,_Indiana', 'description': '– Record cold temperatures hit the eastern United States. The coldest temperature ever measured in Indiana state history, −36\\xa0°F (−38\\xa0°C), is recorded in .'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def create_url_from_title(title):\n",
        "    \"\"\"\n",
        "    Given a list of dictionaries containing href, title, and description for each entry, extracts the title from each link and constructs a new URL using the title.\n",
        "    \"\"\"\n",
        "    urls = []\n",
        "    if title != None: \n",
        "      # Replace spaces with underscores in the title\n",
        "      title = title.replace(' ', '_')\n",
        "\n",
        "      # Construct a new URL using the title\n",
        "      url = 'https://pageviews.wmcloud.org/redirectviews/?project=en.wikipedia.org&platform=all-access&agent=user&range=all-time&sort=views&direction=1&view=list&page=' + title\n",
        "  \n",
        "    return url\n",
        "\n",
        "\n",
        "def extract_visitorNumber_from_link(link):\n",
        "    \"\"\"\n",
        "    extract the first entry of the table\n",
        "    \"\"\"\n",
        "    headers, data = get_table_info(link)\n",
        "\n",
        "    if data[1]: \n",
        "      return data[1][2]\n",
        "    else:\n",
        "      return 0\n",
        "\n",
        "def get_visitor_numbers(pageviews_url):\n",
        "    response = requests.get(pageviews_url)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()['items']\n",
        "        visitors = {}\n",
        "        for item in data:\n",
        "            visitors[item['timestamp']] = item['views']\n",
        "        return visitors\n",
        "    else:\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "i02UxHSSpmpz"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "link = 'https://pageviews.wmcloud.org/redirectviews/?project=en.wikipedia.org&platform=all-access&agent=user&range=all-time&sort=views&direction=1&view=list&page=North_American_Free_Trade_Agreement'\n",
        "\n",
        "print(extract_visitorNumber_from_link(link))"
      ],
      "metadata": {
        "id": "_FYBzxhEIIHo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26a7c9e8-78fd-4aef-e964-0c5a2e606017"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-e274fc943da5>:12: DeprecationWarning: headless property is deprecated, instead use add_argument('-headless')\n",
            "  options.headless = True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7,131,455\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test for year questions (try to implement an alternative way vie pageview.toolsforge) "
      ],
      "metadata": {
        "id": "oq8Zj9_w-aS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import requests\n",
        "\n",
        "from selenium import webdriver\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def get_table_info(url):\n",
        "    \"\"\"\n",
        "    Given a URL, this function opens the url and retrieves the information stored in a table.\n",
        "    \"\"\"\n",
        "    options = webdriver.FirefoxOptions()\n",
        "    #options.headless = True\n",
        "    options.add_argument('--headless')\n",
        "\n",
        "    driver = webdriver.Firefox(options=options)\n",
        "    driver.get(url)\n",
        "    time.sleep(5) # Wait for the page to load completely\n",
        "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "    driver.quit()\n",
        "    table = soup.find('table')\n",
        "    rows = table.find_all('tr')\n",
        "    headers = [header.text.strip() for header in rows[0].find_all('th')]\n",
        "    data = []\n",
        "    for row in rows[1:]:\n",
        "        data.append([cell.text.strip() for cell in row.find_all('td')])\n",
        "    return (headers, data)\n",
        "\n",
        "\n",
        "def get_table_info_requests(url):\n",
        "    \"\"\"\n",
        "    Given a URL, this function opens the url and retrieves the information stored in a table.\n",
        "    \"\"\"\n",
        "    headers = []\n",
        "    data = []\n",
        "\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    table = soup.find('table')\n",
        "    rows = table.find_all('tr')\n",
        "    headers = [header.text.strip() for header in rows[0].find_all('th')]\n",
        "    data = []\n",
        "    for row in rows[1:]:\n",
        "        data.append([cell.text.strip() for cell in row.find_all('td')])\n",
        "\n",
        "    return (headers, data)\n",
        "\n",
        "\n",
        "\n",
        "def get_links_in_section(wikipedia_url, section_heading):\n",
        "    \"\"\"\n",
        "    Given a Wikipedia URL, and a section of this article, returns an array of dictionaries containing the href, title, and description of each link on that section of the page.\n",
        "    \"\"\"\n",
        "    response = requests.get(wikipedia_url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    section = soup.find('span', {'id': section_heading})\n",
        "    if section is None:\n",
        "        return None\n",
        "    section_content = section.parent.find_next_sibling('ul')\n",
        "    if section_content is None:\n",
        "        return None\n",
        "    links = []\n",
        "    for item in section_content.find_all('li'):\n",
        "        link = item.find('a')\n",
        "        if link is not None and link.has_attr('href') and link['href'].startswith('/wiki/'):\n",
        "            title = link.get('title', '')\n",
        "            description = item.text.strip()\n",
        "            url = f\"https://en.wikipedia.org{link['href']}\"\n",
        "            links.append({'title': title, 'description': description, 'url': url})\n",
        "    print(links)\n",
        "    return links\n",
        "\n",
        "\n",
        "def get_links_in_section_with_sublinks(wikipedia_url, section_title):\n",
        "    \"\"\"\n",
        "    Given a Wikipedia URL, and a section of this article, returns an array of dictionaries containing the href, title, and description of each link  with its sublinks as well.\n",
        "    \"\"\"\n",
        "    response = requests.get(wikipedia_url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        section_heading = soup.find('span', {'id': section_title})\n",
        "        if section_heading is None:\n",
        "            return None\n",
        "        section = section_heading.parent\n",
        "        section_content = section.find_next_sibling('ul')\n",
        "        if section_content is None:\n",
        "            return None\n",
        "        links = []\n",
        "        for item in section_content.find_all('li'):\n",
        "            link = item.find('a')\n",
        "            if link is not None:\n",
        "                link_title = link.get('title')\n",
        "                link_url = 'https://en.wikipedia.org' + link.get('href')\n",
        "                link_description = item.text.replace(link_title, '').strip()\n",
        "                links.append({'title': link_title, 'url': link_url, 'description': link_description})\n",
        "                for sublink in item.find_all('a', href=True, recursive=False):\n",
        "                    sublink_title = sublink.get('title')\n",
        "                    sublink_url = 'https://en.wikipedia.org' + sublink.get('href')\n",
        "                    sublink_description = sublink.parent.text.replace(sublink_title, '').replace(link_title, '').strip()\n",
        "                    links.append({'title': sublink_title, 'url': sublink_url, 'description': sublink_description})\n",
        "        #print(links)\n",
        "        return links\n",
        "    else:\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "wzL3elbnOlYl"
      },
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "\"\"\"\n",
        "Returns all backlinks located in the thumbcaption section\n",
        "\"\"\"\n",
        "#IMPORTANT\n",
        "\n",
        "def get_wikipedia_backlinks_thumbcaption(url):\n",
        "    # Load the Wikipedia page HTML\n",
        "    page_html = requests.get(url).text\n",
        "    soup = BeautifulSoup(page_html, 'html.parser')\n",
        "\n",
        "    # Find the image caption on the page\n",
        "    caption = soup.find('div', class_='thumbcaption')\n",
        "\n",
        "    if caption is not None:\n",
        "      # Extract the caption text and any backlinks\n",
        "      backlink_sentences = {}\n",
        "      sentences = caption.text.strip().split('.')\n",
        "      for sentence in sentences:\n",
        "        links = caption.find_all('a', href=True, string=re.compile(sentence))\n",
        "        if len(links) > 0:\n",
        "            backlinks = []\n",
        "            for link in links:\n",
        "                backlink_url = 'https://en.wikipedia.org' + link['href']\n",
        "                backlink_title = link.get('title', '')\n",
        "                backlinks.append((backlink_url, backlink_title))\n",
        "            backlink_sentences[sentence] = backlinks\n",
        "\n",
        "    return backlink_sentences"
      ],
      "metadata": {
        "id": "ivgHDbkdgfJF"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This function correctly prunes the links to only include the string after the last / character\n",
        "\"\"\"\n",
        "def prune_links(links):\n",
        "    pruned_links = []\n",
        "    for url, title in links:\n",
        "        pruned_url = url.split('/')[-1]\n",
        "        pruned_links.append((pruned_url, title))\n",
        "    return pruned_links\n",
        "\n",
        "\"\"\"\n",
        "This function first initializes an empty list combined_list that will hold the combined strings. Then, it uses a for loop to iterate through the input list in increments of 10 tuples at a time.\n",
        "For each sub-list of up to 10 tuples, it uses the list comprehension and join() method as before to combine the first elements of each tuple into a single string separated by '|'. \n",
        "Finally, the function appends each combined string to the combined_list and returns it at the end.\n",
        "\"\"\"\n",
        "def combine_first_elements(my_list):\n",
        "    combined_list = []\n",
        "    num_tuples = len(my_list)\n",
        "    for i in range(0, num_tuples, 10):\n",
        "        sub_list = my_list[i:i+10]\n",
        "        combined_str = '|'.join([tup[0] for tup in sub_list])\n",
        "        combined_list.append(combined_str)\n",
        "    return combined_list\n",
        "\n",
        "\"\"\"\n",
        "This function first initializes an empty list url_list that will hold the modified URLs. Then, it uses a for loop to iterate through each combined string in the input list. \n",
        "For each combined string, it concatenates the base URL with a forward slash and the combined string, and appends the resulting URL to the url_list. Finally, the function returns the url_list at the end.\n",
        "\"\"\"\n",
        "def add_combined_strings_to_url(base_url, combined_strings):\n",
        "    url_list = []\n",
        "    for string in combined_strings:\n",
        "        url_list.append(base_url + '/' + string)\n",
        "    return url_list"
      ],
      "metadata": {
        "id": "DQ8oiiIkl7Kt"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\"\"\"\n",
        "This will output a list of all the sentences in the thumbcaption, split by ';'.\n",
        "\"\"\"\n",
        "def get_thumbcaption_sentences(url):\n",
        "    # Get the HTML content of the page\n",
        "    page = requests.get(url)\n",
        "    soup = BeautifulSoup(page.content, 'html.parser')\n",
        "\n",
        "    # Find the thumbcaption element\n",
        "    thumbcaption = soup.find('div', class_='thumbcaption')\n",
        "\n",
        "    # Get all the sentences in the thumbcaption\n",
        "    sentences = thumbcaption.text.split('; ')\n",
        "\n",
        "    return sentences\n",
        "\n",
        "\n",
        "def list_to_dict(lst):\n",
        "    result = {}\n",
        "    for sublist in lst:\n",
        "        if len(sublist) >= 4:\n",
        "            key = sublist[1]\n",
        "            value = int(sublist[3].replace(',', ''))\n",
        "            result[key] = value\n",
        "    return result\n",
        "\n",
        "\"\"\"\n",
        "This function takes a list of links, opens each link to get the data, discards the header and converts the data into a dictionary using the list_to_dict() function, \n",
        "and then updates a combined dictionary with the resulting dictionary from each link. Finally, it returns the combined dictionary.\n",
        "\"\"\"\n",
        "def combine_dicts_from_links(link_list):\n",
        "    combined_dict = {}\n",
        "    for link in link_list:\n",
        "        header, data = get_table_info(link)\n",
        "        #print(data)\n",
        "        link_dict = list_to_dict(data)\n",
        "        combined_dict.update(link_dict)\n",
        "    return combined_dict\n",
        "\n",
        "\"\"\"\n",
        "This sorts the items in the dictionary based on the integer value of the second element in each key-value tuple (i.e. item[1]), in descending order (reverse=True).\n",
        "\"\"\"\n",
        "def sort_dict_desc(d):\n",
        "    return {k: v for k, v in sorted(d.items(), key=lambda item: int(item[1]), reverse=True)}\n",
        "\n",
        "\"\"\"\n",
        "This function takes in the ord dictionary and the sentences list as arguments.\n",
        "It initializes an empty list called result that we will append the found sentences to. It then iterates over each key in the ord dictionary and for each key, it iterates over each sentence in the sentences list. \n",
        "If the key is found in the sentence, the sentence is appended to the result list\n",
        "\"\"\"\n",
        "def find_sentences(ord, sentences):\n",
        "    result = []\n",
        "    for key in ord:\n",
        "        for sentence in sentences:\n",
        "            if key in sentence:\n",
        "                result.append(sentence)\n",
        "    return result\n",
        "\n",
        "\"\"\"\n",
        "This function takes a list of sentences and a keyword, removes the keyword from each sentence in the list, and returns the updated list.\n",
        "\"\"\"\n",
        "def remove_keyword(sentences, keyword):\n",
        "    updated_sentences = []\n",
        "    for sentence in sentences:\n",
        "        updated_sentence = sentence.replace(keyword, \"\")\n",
        "        updated_sentences.append(updated_sentence)\n",
        "    return updated_sentences\n",
        "\n",
        "\"\"\"\n",
        "This function takes a list of sentences and a list of keyword, removes the keyword from each sentence in the list, and returns the updated list. MAYBE NOT WORKING\n",
        "\"\"\"\n",
        "def remove_keywords(sentences, keywords):\n",
        "    \"\"\"\n",
        "    Removes one or more keywords from each sentence in the list of sentences.\n",
        "    \"\"\"\n",
        "    result = sentences\n",
        "    for entry in keywords:\n",
        "        result = remove_keyword(result, entry)\n",
        "    \n",
        "    return result\n",
        "\n",
        "    #for i, sentence in enumerate(sentences):\n",
        "     #   for keyword in keywords:\n",
        "      #      sentences[i] = sentence.replace(keyword, '')\n",
        "    #return sentences\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "This function prepends a given string to each sentence in a list.\n",
        "\"\"\"\n",
        "def prepend_string(sentences, prepend_str):\n",
        "    new_sentences = []\n",
        "    for sentence in sentences:\n",
        "        new_sentence = f\"{prepend_str}{sentence}\"\n",
        "        new_sentences.append(new_sentence)\n",
        "    return new_sentences\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "This function creates a new list new_sentences and loops over each sentence in the input sentences list. \n",
        "It then uses a list comprehension and the difflib.SequenceMatcher class to compare the sentence to each sentence already in new_sentences. \n",
        "If the ratio of similarity between the two sentences is greater than 0.8 (adjust this threshold as needed), it considers the sentence to be similar and skips it. Otherwise, it adds the sentence to new_sentences. \n",
        "Finally, it returns the new list of unique sentences.\n",
        "\"\"\"\n",
        "import difflib\n",
        "\n",
        "def remove_similar(sentences):\n",
        "    new_sentences = []\n",
        "    for sentence in sentences:\n",
        "        if not any(difflib.SequenceMatcher(None, sentence, s).ratio() > 0.8 for s in new_sentences):\n",
        "            new_sentences.append(sentence)\n",
        "    return new_sentences\n",
        "\n"
      ],
      "metadata": {
        "id": "iQNo1UjzstYW"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#IMPORTANT\n",
        "\n",
        "years_key = '2004'\n",
        "\n",
        "wiki_base_link= 'https://en.wikipedia.org/wiki/'\n",
        "test_link = wiki_base_link + years_key\n",
        "#url = 'https://pageviews.toolforge.org/?project=en.wikipedia.org&pages='\n",
        "pageviews_range_url = 'https://pageviews.wmcloud.org/?project=en.wikipedia.org&platform=all-access&agent=user&redirects=0&range=all-time&pages='\n",
        "\n",
        "sentences_of_thumbcaption = get_thumbcaption_sentences(test_link) #just gets the sentences from the thumbcaption section of a wikipedi years page \n",
        "#print(sentences_of_thumbcaption)\n",
        "thumbcaption = get_wikipedia_backlinks_thumbcaption(test_link) #get all backlinks of the thumbcapture of the year (those are the most known events)\n",
        "#print(thumbcaption) \n",
        "thumbcaption_key = next(iter(thumbcaption))\n",
        "thumbcaption_val = thumbcaption[thumbcaption_key]\n",
        "#print(thumbcaption_val)\n",
        "pruned = prune_links(thumbcaption_val) #prune those backlinks such that only the important part remains\n",
        "#print(pruned)\n",
        "com = combine_first_elements(pruned) #combine up to 10 of these links to create a request to pageview\n",
        "#print(com)\n",
        "url_list = add_combined_strings_to_url(pageviews_range_url, com) #now we have a list of pageview links with all the backlinks of the thumbcaption part of the wiki page\n",
        "#print(url_list)\n",
        "data=combine_dicts_from_links(url_list) #now we called the links and retreieved the pageviews; saved them as a dictionary\n",
        "#print(data)\n",
        "ord = sort_dict_desc(data) #now the list is ordered in ascending order\n",
        "#print(ord)\n",
        "tmp = find_sentences(ord,sentences_of_thumbcaption) #search the corresponding sentence to the keyword (USA and school shooting for example)\n",
        "#remove the years number from the hints OBVIOUSNESS\n",
        "keywords_list = [years_key, 'clockwise ', 'Clockwise ', 'From top left', 'from top-left', 'from top left', 'From top-left', 'from top-left: ', ':'] #list of keywords that should be removed from the sentences\n",
        "t4=remove_keywords(tmp, keywords_list)\n",
        "prepend_str = 'In the same year, '\n",
        "hints = prepend_string(t4, prepend_str)\n",
        "\n",
        "final_hints = remove_similar(hints) #before adjusting the sentences\n",
        "print(final_hints)\n",
        "\n",
        "#some of these could work as a hint but not all of them\n",
        "# I will implement a method to create better hint-sentences, because at the moment sometimes they are not really sentences\n",
        "# in addition to these I will add hint for popular events such as the most important sports events of the year (Euros, WorldCup, F1, ChampionsLeague, Superowl, ... )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvcHIpCheNpL",
        "outputId": "9ac0eb91-292d-45e3-fd53-4a8d700314a3"
      },
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Clockwise from top-left: Facebook, originally called TheFacebook, is launched by Mark Zuckerberg', 'the 2004 transit of Venus, the first such occurrence since 1882', 'NASA lands the Opportunity Rover on Mars', 'the 2004 Summer Olympics are held in Athens', 'Al-Qaeda bombs multiple trains in Madrid, killing 193 people', 'the European Union adds 10 new member-states', '333 people are killed in the Beslan school siege, carried out by Chechen terrorists', 'a massive megathrust earthquake off the coast of Sumatra and the resultant tsunami kill over 227,000 people—one of the worst natural disasters in recorded history.']\n",
            "['In the same year,  Facebook, originally called TheFacebook, is launched by Mark Zuckerberg', 'In the same year, the European Union adds 10 new member-states', 'In the same year, NASA lands the Opportunity Rover on Mars', 'In the same year, Al-Qaeda bombs multiple trains in Madrid, killing 193 people', 'In the same year, the  Summer Olympics are held in Athens', 'In the same year, 333 people are killed in the Beslan school siege, carried out by Chechen terrorists', 'In the same year, a massive megathrust earthquake off the coast of Sumatra and the resultant tsunami kill over 227,000 people—one of the worst natural disasters in recorded history.', 'In the same year, the  transit of Venus, the first such occurrence since 1882']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test creation of question Pools for popular events for year questions (via pageview.toolsforge) "
      ],
      "metadata": {
        "id": "O37okmzfPCcM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def get_all_tables(url):\n",
        "    \"\"\"\n",
        "    Given a URL, this function opens the url and retrieves all tables on the page.\n",
        "    \"\"\"\n",
        "    options = webdriver.FirefoxOptions()\n",
        "    #options.headless = True\n",
        "    options.add_argument('--headless')\n",
        "\n",
        "\n",
        "    driver = webdriver.Firefox(options=options)\n",
        "    driver.get(url)\n",
        "    time.sleep(1) # Wait for the page to load completely\n",
        "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "    driver.quit()\n",
        "\n",
        "    tables = soup.find_all('table')\n",
        "    all_tables = []\n",
        "    for table in tables:\n",
        "        rows = table.find_all('tr')\n",
        "        headers = [header.text.strip() for header in rows[0].find_all('th')]\n",
        "        data = []\n",
        "        for row in rows[1:]:\n",
        "            data.append([cell.text.strip() for cell in row.find_all('td')])\n",
        "        all_tables.append({'headers': headers, 'data': data})\n",
        "\n",
        "    return all_tables\n",
        "\n",
        "def get_first_elements(data_dict):\n",
        "    \"\"\"\n",
        "    Extracts the first element from each sublist in the 'data' list of lists in a dictionary and returns them in a separate list.\n",
        "    \n",
        "    Args:\n",
        "    - data_dict (dict): A dictionary containing a 'data' key with a list of lists as its value.\n",
        "    \n",
        "    Returns:\n",
        "    - A list containing the first element of each sublist in the 'data' list of lists.\n",
        "    \"\"\"\n",
        "    data_list = data_dict['data']\n",
        "    result = []\n",
        "    for sublist in data_list:\n",
        "        if len(sublist) > 0:\n",
        "            if len(sublist[0]) > 0:\n",
        "              result.append(sublist[0])\n",
        "    return result\n",
        "\n",
        "#prune the list of the hole wiki page down to the important table\n",
        "def prune_dict_list(dict_list, keyw):\n",
        "    for d in dict_list:\n",
        "        if 'headers' in d and d['headers'] == [keyw]:\n",
        "            return d\n",
        "    return None  # if no dict with the desired key-value pair is found\n",
        "\n",
        "#replace the \\n and create a list of lists \n",
        "def create_list_from_list_of_lists_key(lst, keyw):\n",
        "    result = []\n",
        "    for sublst in lst:\n",
        "        if sublst:\n",
        "          new_test = [item.replace(keyw, '') for item in sublst[0].split(keyw)]\n",
        "          result.append(new_test)\n",
        "    return result\n",
        "\n",
        "#replace the : and create a dict; {'year': 'CL-winner'}\n",
        "def create_dict_from_list_of_lists(lst):\n",
        "    result_dict = {}\n",
        "    for sublist in lst:\n",
        "        for item in sublist:\n",
        "            if \":\" in item:\n",
        "                key, value = item.split(\":\")\n",
        "                key = key.strip().replace(\"–\", \"-\")\n",
        "                value = value.strip()\n",
        "                result_dict[key] = value\n",
        "            else:\n",
        "                continue\n",
        "    return result_dict\n",
        "\n",
        "#split on the : and creates a dict\n",
        "def create_dict_from_list(lst):\n",
        "    result = {}\n",
        "    for item in lst:\n",
        "        parts = item.split(\":\")\n",
        "       # parts = parts.strip().replace(\":\", \"\")\n",
        "        result[parts[0]] = parts[1]\n",
        "    return result\n",
        "\n",
        "#splits on [ and creates a dict 1950: 'Farina'\n",
        "def get_year_with_driver(race_results):\n",
        "    results_dict = {}\n",
        "    for result in race_results:\n",
        "        if result:\n",
        "            results_dict[result[0].split(\"[\")[0]] = result[1]\n",
        "    return results_dict\n",
        "\n",
        "#get rid of the links ('Alberto Ascari[20]' => 'Alberto Ascari')\n",
        "def clean_driver_names(results_dict):\n",
        "    for year, driver in results_dict.items():\n",
        "        results_dict[year] = driver.split('[')[0].strip()\n",
        "    return results_dict\n",
        "\n",
        "#deletes the : from the key\n",
        "def clean_dict_keys(dict_to_clean):\n",
        "    cleaned_dict = {}\n",
        "    for key, value in dict_to_clean.items():\n",
        "        cleaned_dict[key.rstrip(':')] = value\n",
        "    return cleaned_dict\n",
        "\n",
        "#function to split on \\n but that lets the city names stay together\n",
        "def create_city_dict(city_list):\n",
        "    city_dict = {}\n",
        "    cities = city_list[0].split('\\n')\n",
        "    for city in cities:\n",
        "        if city:\n",
        "            year, *city_name = city.strip().split()\n",
        "            city_dict[year] = ' '.join(city_name)\n",
        "    return city_dict\n",
        "\n",
        "\n",
        "#get the dict of all the champions league winners \n",
        "def champions_league_winners_list():\n",
        "    champions_league_url = 'https://en.wikipedia.org/wiki/List_of_European_Cup_and_UEFA_Champions_League_finals#List_of_finals'\n",
        "    all = get_all_tables(champions_league_url) #gets all tables of wiki page\n",
        "\n",
        "    #first prune of that huge dict\n",
        "    keyw= 'showvteEuropean Cup and UEFA Champions League winners'\n",
        "    pruned_dict = prune_dict_list(all,keyw)\n",
        "\n",
        "    #second prune\n",
        "    inter = pruned_dict.get('data') \n",
        "    cl_list = inter[2:5] + inter[7:11]\n",
        "    tmp1 = create_list_from_list_of_lists_key(cl_list, '\\n')\n",
        "    tmp2 = create_dict_from_list_of_lists(tmp1)\n",
        "\n",
        "    return tmp2\n",
        "\n",
        "#get the dict of all the euros winners \n",
        "def uefa_euros_winners_list():\n",
        "    euros_url = 'https://en.wikipedia.org/wiki/List_of_UEFA_European_Championship_finals#List_of_finals'\n",
        "    all = get_all_tables(euros_url) #gets all tables of wiki page\n",
        "\n",
        "    #first prune of that huge dict\n",
        "    keyw= 'showvteUEFA European Championship winners'\n",
        "    pruned_dict = prune_dict_list(all,keyw)\n",
        "\n",
        "    #second prune\n",
        "    inter = pruned_dict.get('data') \n",
        "    tmp1 = create_list_from_list_of_lists_key(inter, '\\n')\n",
        "    tmp2 = create_dict_from_list_of_lists(tmp1)\n",
        "\n",
        "    return tmp2\n",
        "\n",
        "#get the dict of all the wold cup winners \n",
        "def uefa_worlds_winners_list():\n",
        "    worlds_url = 'https://en.wikipedia.org/wiki/List_of_FIFA_World_Cup_finals#List_of_final_matches'\n",
        "    all = get_all_tables(worlds_url) #gets all tables of wiki page\n",
        "\n",
        "    tmp3=get_first_elements(all[3])\n",
        "\n",
        "    #first prune of that huge dict\n",
        "    keyw= 'showvteFIFA World Cup'\n",
        "    pruned_dict = prune_dict_list(all,keyw)\n",
        "  \n",
        "    #second prune\n",
        "    inter = pruned_dict.get('data') \n",
        "    inter = inter[2]\n",
        "\n",
        "    years = [s for s in inter[0].split('\\n')]\n",
        "\n",
        "    my_dict = dict(zip(years, tmp3))\n",
        "\n",
        "    return my_dict\n",
        "\n",
        "#get the dict of all the F1 drivers world champions\n",
        "def f1_winners_list():\n",
        "    euros_url = 'https://en.wikipedia.org/wiki/List_of_Formula_One_World_Drivers%27_Champions#By_season'\n",
        "    all = get_all_tables(euros_url) #gets all tables of wiki page\n",
        "\n",
        "    lst = all[2].get('data')\n",
        "\n",
        "    tmp2 = get_year_with_driver(lst)\n",
        "    tmp2 = clean_driver_names(tmp2)\n",
        "\n",
        "    return tmp2\n",
        "  \n",
        "#get the dict of all summer olympics host cities\n",
        "def summer_olympics_hosts_list():\n",
        "    summerO_url = 'https://en.wikipedia.org/wiki/Summer_Olympic_Games#List_of_Summer_Olympic_Games'\n",
        "    all = get_all_tables(summerO_url) #gets all tables of wiki page\n",
        "    lst = all[10].get('data')\n",
        "\n",
        "    tmp1 = create_list_from_list_of_lists_key(lst, '\\n')\n",
        "    tmp1 = tmp1[0]\n",
        "    tmp1 = create_dict_from_list(tmp1)\n",
        "    tmp1 = clean_dict_keys(tmp1)\n",
        "    tmp1 = clean_driver_names(tmp1)\n",
        "\n",
        "    return tmp1\n",
        "\n",
        "\n",
        "#get the dict of all winter olympics host cities\n",
        "def winter_olympics_hosts_list():\n",
        "    winterO_url = 'https://en.wikipedia.org/wiki/Winter_Olympic_Games#List_of_Winter_Olympic_Games'\n",
        "    all = get_all_tables(winterO_url) #gets all tables of wiki page\n",
        "    lst = all[8].get('data')\n",
        "\n",
        "    tmp1 = create_list_from_list_of_lists_key(lst, '\\n')\n",
        "    tmp1 = tmp1[0]\n",
        "    tmp1 = create_dict_from_list(tmp1)\n",
        "    tmp1 = clean_dict_keys(tmp1)\n",
        "    tmp1 = clean_driver_names(tmp1)\n",
        "\n",
        "    return tmp1\n"
      ],
      "metadata": {
        "id": "ARMx75Z0QkVh"
      },
      "execution_count": 361,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Years-Question-Pool\n",
        "\n",
        "#For the Champions league\n",
        "cl_all = champions_league_winners_list()\n",
        "print(cl_all)\n",
        "\n",
        "#For Football-Euros\n",
        "euro_all = uefa_euros_winners_list()\n",
        "print(euro_all)\n",
        "\n",
        "#For Football-Worlds\n",
        "worlds_all = uefa_worlds_winners_list()\n",
        "print(worlds_all)\n",
        "\n",
        "#For Fromula1\n",
        "f1_all = f1_winners_list()\n",
        "print(f1_all)\n",
        "\n",
        "#For OlympicSummerGames \n",
        "summer_olympics_all = summer_olympics_hosts_list()\n",
        "print(summer_olympics_all)\n",
        "\n",
        "#For OlympicWinterGames \n",
        "winter_olympics_all = winter_olympics_hosts_list()\n",
        "print(winter_olympics_all)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuWB4hrxPCcV",
        "outputId": "bb4e0bd3-8a62-4266-ef7c-e68870e2fb3a"
      },
      "execution_count": 362,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'1955-56': 'Real Madrid', '1956-57': 'Real Madrid', '1957-58': 'Real Madrid', '1958-59': 'Real Madrid', '1959-60': 'Real Madrid', '1960-61': 'Benfica', '1961-62': 'Benfica', '1962-63': 'AC Milan', '1963-64': 'Internazionale', '1964-65': 'Internazionale', '1965-66': 'Real Madrid', '1966-67': 'Celtic', '1967-68': 'Manchester United', '1968-69': 'AC Milan', '1969-70': 'Feyenoord', '1970-71': 'Ajax', '1971-72': 'Ajax', '1972-73': 'Ajax', '1973-74': 'Bayern Munich', '1974-75': 'Bayern Munich', '1975-76': 'Bayern Munich', '1976-77': 'Liverpool', '1977-78': 'Liverpool', '1978-79': 'Nottingham Forest', '1979-80': 'Nottingham Forest', '1992-93': 'Marseille', '1993-94': 'AC Milan', '1994-95': 'Ajax', '1995-96': 'Juventus', '1996-97': 'Borussia Dortmund', '1997-98': 'Real Madrid', '1998-99': 'Manchester United', '1999-2000': 'Real Madrid', '2000-01': 'Bayern Munich', '2001-02': 'Real Madrid', '2002-03': 'AC Milan', '2003-04': 'Porto', '2004-05': 'Liverpool', '2005-06': 'Barcelona', '2006-07': 'AC Milan', '2007-08': 'Manchester United', '2008-09': 'Barcelona', '2009-10': 'Internazionale', '2010-11': 'Barcelona', '2011-12': 'Chelsea', '2012-13': 'Bayern Munich', '2013-14': 'Real Madrid', '2014-15': 'Barcelona', '2015-16': 'Real Madrid', '2016-17': 'Real Madrid', '2017-18': 'Real Madrid', '2018-19': 'Liverpool', '2019-20': 'Bayern Munich', '2020-21': 'Chelsea', '2021-22': 'Real Madrid'}\n",
            "{'1960': 'Soviet Union', '1964': 'Spain', '1968': 'Italy', '1972': 'West Germany', '1976': 'Czechoslovakia', '1980': 'West Germany', '1984': 'France', '1988': 'Netherlands', '1992': 'Denmark', '1996': 'Germany', '2000': 'France', '2004': 'Greece', '2008': 'Spain', '2012': 'Spain', '2016': 'Portugal', '2020': 'Italy'}\n",
            "{'1930': 'Uruguay', '1934': 'Italy', '1938': 'Italy', '19502': 'Uruguay', '1954': 'West Germany', '1958': 'Brazil', '1962': 'Brazil', '1966': 'England', '1970': 'Brazil', '1974': 'West Germany', '1978': 'Argentina', '1982': 'Italy', '1986': 'Argentina', '1990': 'West Germany', '1994': 'Brazil', '1998': 'France', '2002': 'Brazil', '2006': 'Italy', '2010': 'Spain', '2014': 'Germany', '2018': 'France', '2022': 'Argentina'}\n",
            "{'1950': 'Giuseppe Farina', '1951': 'Juan Manuel Fangio', '1952': 'Alberto Ascari', '1953': 'Alberto Ascari', '1954': 'Juan Manuel Fangio', 'Mercedes': 'Mercedes', '1955': 'Juan Manuel Fangio', '1956': 'Juan Manuel Fangio', '1957': 'Juan Manuel Fangio', '1958': 'Mike Hawthorn', '1959': 'Jack Brabham', '1960': 'Jack Brabham', '1961': 'Phil Hill', '1962': 'Graham Hill', '1963': 'Jim Clark', '1964': 'John Surtees', '1965': 'Jim Clark', '1966': 'Jack Brabham', '1967': 'Denny Hulme', '1968': 'Graham Hill', '1969': 'Jackie Stewart', '1970': 'Jochen Rindt', '1971': 'Jackie Stewart', '1972': 'Emerson Fittipaldi', '1973': 'Jackie Stewart', '1974': 'Emerson Fittipaldi', '1975': 'Niki Lauda', '1976': 'James Hunt', '1977': 'Niki Lauda', '1978': 'Mario Andretti', '1979': 'Jody Scheckter', '1980': 'Alan Jones', '1981': 'Nelson Piquet', '1982': 'Keke Rosberg', '1983': 'Nelson Piquet', '1984': 'Niki Lauda', '1985': 'Alain Prost', '1986': 'Alain Prost', '1987': 'Nelson Piquet', '1988': 'Ayrton Senna', '1989': 'Alain Prost', '1990': 'Ayrton Senna', '1991': 'Ayrton Senna', '1992': 'Nigel Mansell', '1993': 'Alain Prost', '1994': 'Michael Schumacher', '1995': 'Michael Schumacher', '1996': 'Damon Hill', '1997': 'Jacques Villeneuve', '1998': 'Mika Häkkinen', '1999': 'Mika Häkkinen', '2000': 'Michael Schumacher', '2001': 'Michael Schumacher', '2002': 'Michael Schumacher', '2003': 'Michael Schumacher', '2004': 'Michael Schumacher', '2005': 'Fernando Alonso', '2006': 'Fernando Alonso', '2007': 'Kimi Räikkönen', '2008': 'Lewis Hamilton', '2009': 'Jenson Button', '2010': 'Sebastian Vettel', '2011': 'Sebastian Vettel', '2012': 'Sebastian Vettel', '2013': 'Sebastian Vettel', '2014': 'Lewis Hamilton', '2015': 'Lewis Hamilton', '2016': 'Nico Rosberg', '2017': 'Lewis Hamilton', '2018': 'Lewis Hamilton', '2019': 'Lewis Hamilton', '2020': 'Lewis Hamilton', '2021': 'Max Verstappen', '2022': 'Max Verstappen'}\n",
            "{'1896': 'Athens', '1900': 'Paris', '1904': 'St. Louis', '1908': 'London', '1912': 'Stockholm', '1916': 'None', '1920': 'Antwerp', '1924': 'Paris', '1928': 'Amsterdam', '1932': 'Los Angeles', '1936': 'Berlin', '1940': 'None', '1944': 'None', '1948': 'London', '1952': 'Helsinki', '1956': 'Melbourne', '1960': 'Rome', '1964': 'Tokyo', '1968': 'Mexico City', '1972': 'Munich', '1976': 'Montreal', '1980': 'Moscow', '1984': 'Los Angeles', '1988': 'Seoul', '1992': 'Barcelona', '1996': 'Atlanta', '2000': 'Sydney', '2004': 'Athens', '2008': 'Beijing', '2012': 'London', '2016': 'Rio de Janeiro', '2020': 'Tokyo', '2024': 'Paris', '2028': 'Los Angeles', '2032': 'Brisbane'}\n",
            "{'1924': 'Chamonix', '1928': 'St. Moritz', '1932': 'Lake Placid', '1936': 'Garmisch-Partenkirchen', '1940': 'None', '1944': 'None', '1948': 'St. Moritz', '1952': 'Oslo', '1956': \"Cortina d'Ampezzo\", '1960': 'Squaw Valley', '1964': 'Innsbruck', '1968': 'Grenoble', '1972': 'Sapporo', '1976': 'Innsbruck', '1980': 'Lake Placid', '1984': 'Sarajevo', '1988': 'Calgary', '1992': 'Albertville', '1994': 'Lillehammer', '1998': 'Nagano', '2002': 'Salt Lake City', '2006': 'Turin', '2010': 'Vancouver', '2014': 'Sochi', '2018': 'Pyeongchang', '2022': 'Beijing', '2026': \"Milan and Cortina d'Ampezzo\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5rwSDisuN9s"
      },
      "source": [
        "# All imports"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "%rm -r optimizationOfHintGeneration\n",
        "!git clone https://github.com/AlexWalcher/optimizationOfHintGeneration.git\n",
        "%cd /content/"
      ],
      "metadata": {
        "id": "b_jGbTrbz8x2",
        "outputId": "7d98242b-d448-40ea-8261-b164e5795de7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "rm: cannot remove 'optimizationOfHintGeneration': No such file or directory\n",
            "Cloning into 'optimizationOfHintGeneration'...\n",
            "remote: Enumerating objects: 51, done.\u001b[K\n",
            "remote: Counting objects: 100% (51/51), done.\u001b[K\n",
            "remote: Compressing objects: 100% (49/49), done.\u001b[K\n",
            "remote: Total 51 (delta 25), reused 6 (delta 1), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (51/51), 6.94 MiB | 4.90 MiB/s, done.\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install selenium\n",
        "!apt-get update \n",
        "!apt-get install firefox\n",
        "!apt install firefox-geckodriver"
      ],
      "metadata": {
        "id": "98Zh7IC7mSWI",
        "outputId": "99d6e15d-acde-4431-bcde-eee0dc427358",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.8.3-py3-none-any.whl (6.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trio-websocket~=0.9\n",
            "  Downloading trio_websocket-0.10.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.9/dist-packages (from selenium) (2022.12.7)\n",
            "Collecting trio~=0.17\n",
            "  Downloading trio-0.22.0-py3-none-any.whl (384 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m384.9/384.9 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3[socks]~=1.26 in /usr/local/lib/python3.9/dist-packages (from selenium) (1.26.15)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in /usr/local/lib/python3.9/dist-packages (from trio~=0.17->selenium) (1.1.1)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.9/dist-packages (from trio~=0.17->selenium) (3.4)\n",
            "Collecting outcome\n",
            "  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.9/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.9/dist-packages (from trio~=0.17->selenium) (1.3.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.9/dist-packages (from trio~=0.17->selenium) (22.2.0)\n",
            "Collecting async-generator>=1.9\n",
            "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
            "Collecting wsproto>=0.14\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.9/dist-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
            "Collecting h11<1,>=0.9.0\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: outcome, h11, async-generator, wsproto, trio, trio-websocket, selenium\n",
            "Successfully installed async-generator-1.10 h11-0.14.0 outcome-1.2.0 selenium-4.8.3 trio-0.22.0 trio-websocket-0.10.2 wsproto-1.2.0\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease [3,622 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease [1,581 B]\n",
            "Get:3 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
            "Hit:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  Packages [973 kB]\n",
            "Hit:8 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "Hit:9 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
            "Get:11 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1,028 kB]\n",
            "Hit:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "Get:13 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [2,065 kB]\n",
            "Hit:14 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n",
            "Get:15 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2,593 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1,324 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3,073 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [2,203 kB]\n",
            "Fetched 13.6 MB in 3s (4,385 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libdbus-glib-1-2 libdbusmenu-glib4 libdbusmenu-gtk3-4 libxtst6\n",
            "  xul-ext-ubufox\n",
            "Suggested packages:\n",
            "  fonts-lyx\n",
            "The following NEW packages will be installed:\n",
            "  firefox libdbus-glib-1-2 libdbusmenu-glib4 libdbusmenu-gtk3-4 libxtst6\n",
            "  xul-ext-ubufox\n",
            "0 upgraded, 6 newly installed, 0 to remove and 30 not upgraded.\n",
            "Need to get 60.8 MB of archives.\n",
            "After this operation, 243 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal/main amd64 libdbus-glib-1-2 amd64 0.110-5fakssync1 [59.1 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu focal/main amd64 libxtst6 amd64 2:1.2.3-1 [12.8 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 firefox amd64 111.0.1+build2-0ubuntu0.20.04.1 [60.7 MB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu focal/main amd64 libdbusmenu-glib4 amd64 16.04.1+18.10.20180917-0ubuntu6 [41.2 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu focal/main amd64 libdbusmenu-gtk3-4 amd64 16.04.1+18.10.20180917-0ubuntu6 [27.7 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu focal/main amd64 xul-ext-ubufox all 3.4-0ubuntu1.17.10.1 [3,320 B]\n",
            "Fetched 60.8 MB in 2s (29.1 MB/s)\n",
            "Selecting previously unselected package libdbus-glib-1-2:amd64.\n",
            "(Reading database ... 122349 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libdbus-glib-1-2_0.110-5fakssync1_amd64.deb ...\n",
            "Unpacking libdbus-glib-1-2:amd64 (0.110-5fakssync1) ...\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "Preparing to unpack .../1-libxtst6_2%3a1.2.3-1_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1) ...\n",
            "Selecting previously unselected package firefox.\n",
            "Preparing to unpack .../2-firefox_111.0.1+build2-0ubuntu0.20.04.1_amd64.deb ...\n",
            "Unpacking firefox (111.0.1+build2-0ubuntu0.20.04.1) ...\n",
            "Selecting previously unselected package libdbusmenu-glib4:amd64.\n",
            "Preparing to unpack .../3-libdbusmenu-glib4_16.04.1+18.10.20180917-0ubuntu6_amd64.deb ...\n",
            "Unpacking libdbusmenu-glib4:amd64 (16.04.1+18.10.20180917-0ubuntu6) ...\n",
            "Selecting previously unselected package libdbusmenu-gtk3-4:amd64.\n",
            "Preparing to unpack .../4-libdbusmenu-gtk3-4_16.04.1+18.10.20180917-0ubuntu6_amd64.deb ...\n",
            "Unpacking libdbusmenu-gtk3-4:amd64 (16.04.1+18.10.20180917-0ubuntu6) ...\n",
            "Selecting previously unselected package xul-ext-ubufox.\n",
            "Preparing to unpack .../5-xul-ext-ubufox_3.4-0ubuntu1.17.10.1_all.deb ...\n",
            "Unpacking xul-ext-ubufox (3.4-0ubuntu1.17.10.1) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1) ...\n",
            "Setting up libdbusmenu-glib4:amd64 (16.04.1+18.10.20180917-0ubuntu6) ...\n",
            "Setting up libdbus-glib-1-2:amd64 (0.110-5fakssync1) ...\n",
            "Setting up xul-ext-ubufox (3.4-0ubuntu1.17.10.1) ...\n",
            "Setting up libdbusmenu-gtk3-4:amd64 (16.04.1+18.10.20180917-0ubuntu6) ...\n",
            "Setting up firefox (111.0.1+build2-0ubuntu0.20.04.1) ...\n",
            "update-alternatives: using /usr/bin/firefox to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/firefox to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "Please restart all running instances of firefox, or you will experience problems.\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Processing triggers for mime-support (3.64ubuntu1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  firefox-geckodriver\n",
            "0 upgraded, 1 newly installed, 0 to remove and 30 not upgraded.\n",
            "Need to get 1,277 kB of archives.\n",
            "After this operation, 4,431 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 firefox-geckodriver amd64 111.0.1+build2-0ubuntu0.20.04.1 [1,277 kB]\n",
            "Fetched 1,277 kB in 1s (2,166 kB/s)\n",
            "Selecting previously unselected package firefox-geckodriver.\n",
            "(Reading database ... 122476 files and directories currently installed.)\n",
            "Preparing to unpack .../firefox-geckodriver_111.0.1+build2-0ubuntu0.20.04.1_amd64.deb ...\n",
            "Unpacking firefox-geckodriver (111.0.1+build2-0ubuntu0.20.04.1) ...\n",
            "Setting up firefox-geckodriver (111.0.1+build2-0ubuntu0.20.04.1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__DepwTLww00",
        "outputId": "0a88a8f6-d075-4752-dd79-03810486f29e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install sparqlwrapper"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sparqlwrapper\n",
            "  Downloading SPARQLWrapper-2.0.0-py3-none-any.whl (28 kB)\n",
            "Collecting rdflib>=6.1.1\n",
            "  Downloading rdflib-6.3.2-py3-none-any.whl (528 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m528.1/528.1 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.9/dist-packages (from rdflib>=6.1.1->sparqlwrapper) (3.0.9)\n",
            "Collecting isodate<0.7.0,>=0.6.0\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from isodate<0.7.0,>=0.6.0->rdflib>=6.1.1->sparqlwrapper) (1.16.0)\n",
            "Installing collected packages: isodate, rdflib, sparqlwrapper\n",
            "Successfully installed isodate-0.6.1 rdflib-6.3.2 sparqlwrapper-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pageviewapi"
      ],
      "metadata": {
        "id": "66w0wd3eu-_z",
        "outputId": "643d11eb-bd84-4b4d-8a11-150b4c2b5bbd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pageviewapi\n",
            "  Downloading pageviewapi-0.4.0-py2.py3-none-any.whl (6.9 kB)\n",
            "Collecting attrdict\n",
            "  Downloading attrdict-2.0.1-py2.py3-none-any.whl (9.9 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from pageviewapi) (2.27.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from attrdict->pageviewapi) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->pageviewapi) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->pageviewapi) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->pageviewapi) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->pageviewapi) (3.4)\n",
            "Installing collected packages: attrdict, pageviewapi\n",
            "Successfully installed attrdict-2.0.1 pageviewapi-0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "id": "LMeyegyfgdd_",
        "outputId": "04dce372-af1c-42fa-fc74-a5a13a646282",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.27.4-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (2.0.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (0.15.1+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (1.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (3.8.1)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.11.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence-transformers) (2.0.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence-transformers) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (16.0.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m119.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk->sentence-transformers) (8.1.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->sentence-transformers) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.12)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125942 sha256=04356254752c2174487d8d37bd3c46dcd165190a3a9fb69e37ec04aa995c77e5\n",
            "  Stored in directory: /root/.cache/pip/wheels/71/67/06/162a3760c40d74dd40bc855d527008d26341c2b0ecf3e8e11f\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: tokenizers, sentencepiece, huggingface-hub, transformers, sentence-transformers\n",
            "Successfully installed huggingface-hub-0.13.4 sentence-transformers-2.2.2 sentencepiece-0.1.97 tokenizers-0.13.3 transformers-4.27.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia"
      ],
      "metadata": {
        "id": "_KjIV0guhBIT",
        "outputId": "e70410cf-9a3b-409d-a14e-567206201fde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from wikipedia) (4.11.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from wikipedia) (2.27.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.12)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4->wikipedia) (2.4)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11696 sha256=fdaab30a051c129eb08290a6038727d06c2282ebe00d507d82d84b4e8c0837a3\n",
            "  Stored in directory: /root/.cache/pip/wheels/c2/46/f4/caa1bee71096d7b0cdca2f2a2af45cacf35c5760bee8f00948\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COpv1aim8jLJ"
      },
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth',1000)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7j16mqC9L83"
      },
      "source": [
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "\n",
        "sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests"
      ],
      "metadata": {
        "id": "n07hKh23g9dG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4\n",
        "from bs4 import BeautifulSoup, NavigableString, Tag"
      ],
      "metadata": {
        "id": "PyfC4F2Ya6sh"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "pQIQvzqoaqbS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFQ0BQ8dnv50"
      },
      "source": [
        "import wikipedia"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random"
      ],
      "metadata": {
        "id": "-6NVlM8dhEdI"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3KDOw-yx4JR"
      },
      "source": [
        "import pageviewapi"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QT41X2h85r7j"
      },
      "source": [
        "#!pip install -U pip setuptools wheel\n",
        "#!pip install -U spacy\n",
        "#!python -m spacy download en_core_web_sm\n",
        "#!python -m spacy link en_core_web_sm en\n",
        "import spacy"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "du5Un8VufnlG"
      },
      "source": [
        "# Functions for reuse"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8o8t6UzdAaKj"
      },
      "source": [
        "#get Wikidata ID\n",
        "def getQitemOfName(entity):\n",
        "  \n",
        "  searched = \"'\"+entity+\"'\"\n",
        "  sparql.setQuery('''\n",
        "  SELECT ?item ?itemLabel WHERE {\n",
        "    ?item rdfs:label '''+searched+'''@en.\n",
        "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
        "\n",
        "  }\n",
        "  ''')\n",
        "  \n",
        "  sparql.setReturnFormat(JSON)\n",
        "  entities = sparql.query().convert()\n",
        "  entities_df = pd.json_normalize(entities['results']['bindings'])\n",
        "  words = entities_df[[\"item.value\"]].iloc[0].str.split(\"/\")[0]\n",
        "  return words[-1]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4tNPxs6Cp8f"
      },
      "source": [
        "#get name of entity from Wikidata ID\n",
        "\n",
        "def getLabelOfQitems(entities):\n",
        "\n",
        "  finalLabels = []\n",
        "  for item in entities:\n",
        "    #searched = \"'\"+item+\"'\"\n",
        "    sparql.setQuery('''\n",
        "    SELECT * WHERE {\n",
        "        wd:'''+item+ ''' rdfs:label ?label .\n",
        "        FILTER (langMatches( lang(?label), \"EN\" ) )\n",
        "    } LIMIT 1\n",
        "    ''')\n",
        "    sparql.setReturnFormat(JSON)\n",
        "    entities = sparql.query().convert()\n",
        "    entities_df = pd.json_normalize(entities['results']['bindings'])\n",
        "    label = entities_df[\"label.value\"].iloc[0]\n",
        "    finalLabels.append(label)\n",
        "\n",
        "  return finalLabels  "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gK0zkvbXxRpU"
      },
      "source": [
        "#function to get all backlinks to wikipage and saves backlinks >= 1000 in array\n",
        "#source: https://www.mediawiki.org/wiki/API:Backlinks#Response\n",
        "\n",
        "import requests\n",
        "\n",
        "def getBacklinks(entities):\n",
        "  S = requests.Session()\n",
        "\n",
        "  URL = \"https://en.wikipedia.org/w/api.php\"\n",
        "\n",
        "  listWithBacklink500 = []\n",
        "  dictWithAllEntries = {}\n",
        "\n",
        "  for val in entities:\n",
        "    #print(row[\"itemLabel.value\"])\n",
        "    PARAMS = {\n",
        "        \"action\": \"query\",\n",
        "        \"format\": \"json\",\n",
        "        \"list\": \"backlinks\",\n",
        "        \"bltitle\": val,\n",
        "        \"bllimit\": \"max\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "      R = S.get(url=URL, params=PARAMS)\n",
        "      DATA = R.json()\n",
        "   \n",
        "\n",
        "      BACKLINKS = DATA[\"query\"][\"backlinks\"]\n",
        "\n",
        "      count = 0\n",
        "      for links in BACKLINKS:\n",
        "        count +=1\n",
        "\n",
        "      \n",
        "      while \"continue\" in DATA:\n",
        "        if (count >= 1000):\n",
        "          break;\n",
        "        blcontinue = DATA[\"continue\"][\"blcontinue\"]\n",
        "        PARAMS[\"blcontinue\"] = blcontinue\n",
        "\n",
        "        R = S.get(url=URL, params=PARAMS)\n",
        "        DATA = R.json()\n",
        "        BACKLINKS = DATA[\"query\"][\"backlinks\"]\n",
        "\n",
        "        for links in BACKLINKS:\n",
        "          count += 1\n",
        "      \n",
        "      dictWithAllEntries[val] = count\n",
        "      #print(val + \": \" + str(count))\n",
        "\n",
        "    except ValueError:\n",
        "      print(\"Value Error\")\n",
        "\n",
        "    if (count >= 1000):\n",
        "      listWithBacklink500.append(val) \n",
        "\n",
        "  if (len(listWithBacklink500) > 0):\n",
        "    #return getPageViews(listWithBacklink500)\n",
        "    return listWithBacklink500\n",
        "\n",
        "  else:\n",
        "    listMostBacklinks = []\n",
        "    sortedDict = dict(sorted(dictWithAllEntries.items(), key=lambda item:item[1]))\n",
        "\n",
        "    for k, v in sortedDict.items():\n",
        "      listMostBacklinks.append(k)\n",
        "    \n",
        "    numberToPass = len(listMostBacklinks)//3\n",
        "        \n",
        "    return listMostBacklinks[-numberToPass:]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-0fPfmxUZSW"
      },
      "source": [
        "#function to get all backlinks to wikipage and saves backlink == 500 in array\n",
        "#source: https://www.mediawiki.org/wiki/API:Backlinks#Response\n",
        "\n",
        "import requests\n",
        "\n",
        "def getBacklinksDict(entities):\n",
        "  S = requests.Session()\n",
        "\n",
        "  URL = \"https://en.wikipedia.org/w/api.php\"\n",
        "\n",
        "  dictWithBacklink500 = {}\n",
        "  dictWithAllEntries = {}\n",
        "\n",
        "  for key, val in entities.items():\n",
        "    print(key, val)\n",
        "\n",
        "    PARAMS = {\n",
        "        \"action\": \"query\",\n",
        "        \"format\": \"json\",\n",
        "        \"list\": \"backlinks\",\n",
        "        \"bltitle\": key,\n",
        "        \"bllimit\": \"max\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "      R = S.get(url=URL, params=PARAMS)\n",
        "      DATA = R.json()\n",
        "   \n",
        "\n",
        "      BACKLINKS = DATA[\"query\"][\"backlinks\"]\n",
        "\n",
        "      count = 0\n",
        "      for links in BACKLINKS:\n",
        "        count +=1\n",
        "\n",
        "      \n",
        "      while \"continue\" in DATA:\n",
        "        if (count >= 1000):\n",
        "          break;\n",
        "        blcontinue = DATA[\"continue\"][\"blcontinue\"]\n",
        "        PARAMS[\"blcontinue\"] = blcontinue\n",
        "\n",
        "        R = S.get(url=URL, params=PARAMS)\n",
        "        DATA = R.json()\n",
        "        BACKLINKS = DATA[\"query\"][\"backlinks\"]\n",
        "\n",
        "        for links in BACKLINKS:\n",
        "          count += 1\n",
        "      \n",
        "      dictWithAllEntries[key] = count\n",
        "      #print(val + \": \" + str(count))\n",
        "\n",
        "    except ValueError:\n",
        "      print(\"Value Error\")\n",
        "    except KeyError:\n",
        "      print(\"Key Error\")\n",
        "\n",
        "    if (count >= 1000):\n",
        "      dictWithBacklink500.update({key: val}) \n",
        "\n",
        "  if (len(dictWithBacklink500) > 0):\n",
        "    #return getPageViews(listWithBacklink500)\n",
        "    return dictWithBacklink500\n",
        "\n",
        "    #print(listMostBacklinks[-numberToPass:])\n",
        "\n",
        "    \n",
        "    return listMostBacklinks[-numberToPass:]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hz4SZXwl1j7d"
      },
      "source": [
        "#get avg pageViews of entities\n",
        "\n",
        "def getPageViewsList(listEntities):\n",
        "  avgDict = {}\n",
        "  \n",
        "  for val in listEntities:\n",
        "    try:\n",
        "      resp = pageviewapi.per_article('en.wikipedia', val, '20191106', '20201120', access='all-access', agent='all-agents', granularity='monthly')\n",
        "    #print(resp)\n",
        "      avgViews = 0\n",
        "      count = 0\n",
        "      for i in resp.get(\"items\"):\n",
        "        #print(i.get(\"views\"))\n",
        "        avgViews += i.get(\"views\")\n",
        "        count += 1\n",
        "      avgDict.update({val: avgViews//count})\n",
        "    except:\n",
        "      print(\"Page not found to check pageViews for page: \" + val)\n",
        "      \n",
        "  #return (max(avgDict, key=avgDict.get))\n",
        "  return avgDict\n",
        "\n",
        "\n",
        "  \n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhL5jdCN-NdY"
      },
      "source": [
        "#get avg pageViews of entity\n",
        "import pageviewapi\n",
        "\n",
        "\n",
        "def getPageViews(entity):\n",
        "  avgDict = {}\n",
        "  \n",
        "  try:\n",
        "    resp = pageviewapi.per_article('en.wikipedia', entity, '20191106', '20201120', access='all-access', agent='all-agents', granularity='monthly')\n",
        "\n",
        "    avgViews = 0\n",
        "    count = 0\n",
        "    for i in resp.get(\"items\"):\n",
        "      #print(i.get(\"views\"))\n",
        "      avgViews += i.get(\"views\")\n",
        "      count += 1\n",
        "\n",
        "    pageViews = avgViews//count\n",
        "    #avgDict.update({val: avgViews//count})\n",
        "\n",
        "    #return (max(avgDict, key=avgDict.get))\n",
        "    return pageViews\n",
        "  #print(resp)\n",
        "  except:\n",
        "    print(\"Page not found to check pageViews for page: \" + entity)\n",
        "    return -1\n",
        "    \n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check if entity is a person\n",
        "#return list of persons\n",
        "\n",
        "def checkIfHuman(entity):\n",
        "\n",
        "  #listHumans = []\n",
        "\n",
        "  #for test in entity:\n",
        "    #print(item)\n",
        "  searched = \"'\"+entity+\"'\"\n",
        "\n",
        "  #print(searched)\n",
        "\n",
        "  try:\n",
        "\n",
        "    sparql.setQuery('''\n",
        "        SELECT distinct ?item ?itemLabel \n",
        "        WHERE{\n",
        "          ?item ?label '''+searched+'''.  \n",
        "          ?item wdt:P31 wd:Q5.\n",
        "          ?article schema:about ?item .\n",
        "          ?article schema:inLanguage \"en\" .\n",
        "          ?article schema:isPartOf <https://en.wikipedia.org/>.\t\n",
        "          SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }    \n",
        "        }\n",
        "    ''')\n",
        "    sparql.setReturnFormat(JSON)\n",
        "    entities = sparql.query().convert()\n",
        "    entities_df = pd.json_normalize(entities['results']['bindings'])\n",
        "\n",
        "    #print(entities)\n",
        "    #print(entities_df[\"itemLabel.value\"].item())\n",
        "\n",
        "    #for index, row in entities_df.iterrows():\n",
        "    return entities_df[\"itemLabel.value\"].item()\n",
        "    \n",
        "    \n",
        "  except:\n",
        "    return None\n"
      ],
      "metadata": {
        "id": "QHdp21OCwtgj"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOJJtksOgQYk"
      },
      "source": [
        "#if parantheses in entity -> remove\n",
        "\n",
        "def removeParanthesesDict(inputDict):\n",
        "\n",
        "  newDict = {}\n",
        "  for key, val in inputDict.items():\n",
        "    if ('(' in key):\n",
        "      newElem = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", key).rstrip()\n",
        "      newDict.update({newElem: val})\n",
        "    else:\n",
        "      newDict.update({key: val})\n",
        "\n",
        "  return newDict"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lhmuuUNQ__8"
      },
      "source": [
        "#Hints if answer is a Location"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-T2fLcTRD54"
      },
      "source": [
        "#Query with all properties to get data\n",
        "\n",
        "def allHintsLocation(entity):\n",
        "  sparql.setQuery('''\n",
        "  SELECT ?itemLabel ?property\n",
        "  WHERE\n",
        "  {  \n",
        "    VALUES ?property {wdt:P35 wdt:P30 wdt:P36 wdt:P610 wdt:P1082 wdt:P421 wdt:P38 wdt:P17 wdt:P1376 wdt:P131 wdt:P6 wdt:P1830 wdt:P47 wdt:P206 wdt:P37 wdt:P463 wdt:793}\n",
        "    wd:'''+entity+''' ?property ?item.\n",
        "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
        "  }\n",
        "  ''')\n",
        "  sparql.setReturnFormat(JSON)\n",
        "  hintsLocation = sparql.query().convert()\n",
        "\n",
        "  hintsLocation_df = pd.json_normalize(hintsLocation['results']['bindings'])\n",
        "\n",
        "  return hintsLocation_df"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "co1A0iK-sery"
      },
      "source": [
        "#create template based hints\n",
        "\n",
        "def getFormatedHintsLocation(queryResults_df):\n",
        "  timeZone = []\n",
        "  isLocatedIn = []\n",
        "  ownerOf = []\n",
        "  shareBorder = []\n",
        "  bodyOfWater = []\n",
        "  languages = []\n",
        "  memberOf = []\n",
        "  significantEvents = []\n",
        "\n",
        "  listHintsLocation = []\n",
        "\n",
        "  searchString = \"http://www.wikidata.org/prop/direct/\"\n",
        "  \n",
        "  for index, row in queryResults_df.iterrows():\n",
        "    if(not location in row[\"itemLabel.value\"]):\n",
        "      if(row[\"property.value\"] == searchString + \"P35\"):\n",
        "        listHintsLocation.append(\"Head of state of searched country is \" + row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P30\"):\n",
        "        listHintsLocation.append(\"The searched location is on continent \" + row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P36\"):\n",
        "        listHintsLocation.append(\"The capital of searched country is \" + row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"\"):\n",
        "        listHintsLocation.append(\"The highest point in searched location is \" + row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P1082\"):\n",
        "        listHintsLocation.append(\"The searched location has a population of \" + row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P38\"):\n",
        "        listHintsLocation.append(\"Currency in searched location is \" + row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P17\"):\n",
        "        if(not location in row[\"itemLabel.value\"]):\n",
        "          listHintsLocation.append(\"The searched location is in country \"+ row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P1376\"):\n",
        "        listHintsLocation.append(\"The searched city is capital of \" + row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P131\"):\n",
        "        listHintsLocation.append(\"The searched location is located in \" + row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P6\"):\n",
        "        listHintsLocation.append(\"Head of government of searched location is \" + row[\"itemLabel.value\"])\n",
        "      \n",
        "      #if get multiple values save all in list and pick one afterwards\n",
        "      elif(row[\"property.value\"] == searchString + \"P1830\"):\n",
        "        if(not location in row[\"itemLabel.value\"]):\n",
        "          ownerOf.append(row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P47\"):\n",
        "        shareBorder.append(row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P206\"):\n",
        "        bodyOfWater.append(row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P37\"):\n",
        "        if(not location in row[\"itemLabel.value\"]):\n",
        "          languages.append(row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P421\"):\n",
        "        timeZone.append(row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P463\"):\n",
        "        memberOf.append(row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P793\"):\n",
        "        if(not location in row[\"itemLabel.value\"]):\n",
        "          significantEvents.append(row[\"itemLabel.value\"])\n",
        "\n",
        "  #getting more than one result with one query, pick one for output\n",
        "  if(len(ownerOf) > 0):\n",
        "    listHintsLocation.append(\"The searched location is owner of \" + ownerOf[0])\n",
        "  if(len(shareBorder) > 0):\n",
        "    listHintsLocation.append(\"The searched location shares border with \" + shareBorder[0])\n",
        "  if(len(bodyOfWater) > 0):\n",
        "    listHintsLocation.append(\"The next body of water of searched location is \" + bodyOfWater[0])\n",
        "  if(len(memberOf) > 0):\n",
        "    listBacklinks = getBacklinks(memberOf)\n",
        "    dictPageViews = getPageViewsList(listBacklinks)\n",
        "    listHintsLocation.append(\"Searched location is member of \" + (max(dictPageViews, key=dictPageViews.get)))\n",
        "  if(len(significantEvents) > 0):\n",
        "    listBacklinks = getBacklinks(significantEvents)\n",
        "    dictPageViews = getPageViewsList(listBacklinks)\n",
        "    listHintsLocation.append(\"A significant event happened in this location was \" + (max(dictPageViews, key=dictPageViews.get)))\n",
        "\n",
        "  #if getting more than one language concatenate all\n",
        "  commaCounter = len(languages)-1\n",
        "  stringLanguages = \"\"\n",
        "  for language in languages:\n",
        "    if(commaCounter > 0):\n",
        "      stringLanguages += language + \", \"\n",
        "      commaCounter -= 1\n",
        "    else:\n",
        "      stringLanguages += language\n",
        "  if(stringLanguages != \"\"):\n",
        "    listHintsLocation.append(\"Spoken language(s) in searched location is/are \" + stringLanguages)\n",
        "  if(len(timeZone) > 0):\n",
        "    listHintsLocation.append(\"The searched location is in time Zone \" + timeZone[0])\n",
        "\n",
        "  \n",
        "  return listHintsLocation\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g65hUdVTRbXU"
      },
      "source": [
        "def writeLocationHintsInFile(listHintsLocation):\n",
        "  randomHintsList = random.sample(listHintsLocation, len(listHintsLocation))\n",
        "  \n",
        "  for item in randomHintsList:\n",
        "    with open(\"hintsLocation.txt\", 'a') as writefile:\n",
        "      writefile.write(item + \"\\n\")\n",
        "      writefile.close()"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Yf6Fi_34ye5"
      },
      "source": [
        "#Hints if answer is a Person"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Query with all properties to get data\n",
        "\n",
        "def allHintsPerson(entity):\n",
        "  sparql.setQuery('''\n",
        "  SELECT ?itemLabel ?property\n",
        "  WHERE\n",
        "  {  \n",
        "    VALUES ?property {wdt:P21 wdt:P27 wdt:P569 wdt:P19 wdt:P1971 wdt:P106 wdt:P1340 wdt:P1884 wdt:P2048 wdt:P39 wdt:P69 wdt:P512 wdt:P102 wdt:P3602 wdt:P800 wdt:P166 wdt:P3373 wdt:P1412 wdt:P413 wdt:P118 wdt:P54}\n",
        "    wd:'''+entity+''' ?property ?item.\n",
        "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
        "  }\n",
        "  ''')\n",
        "  sparql.setReturnFormat(JSON)\n",
        "  hintsPerson = sparql.query().convert()\n",
        "\n",
        "  hintsPerson_df = pd.json_normalize(hintsPerson['results']['bindings'])\n",
        "  \n",
        "  return hintsPerson_df"
      ],
      "metadata": {
        "id": "Lm8zYu5C3xSO"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qS1ZtKwL5hM-"
      },
      "source": [
        "#create temolate based hints\n",
        "\n",
        "def getFormatedHintsPerson(queryResults_df):\n",
        "\n",
        "  listOccupation = [\"Searched person was occuupied as \"]\n",
        "  listPositionHeld = [\"Searched person held the position(s) \"]\n",
        "  listEducatedAt = [\"Searched person was educated at \"]\n",
        "  listAcademicDegrees = [\"Searched person has academic degrees \"]\n",
        "  listPoliticParty = [\"Searched was/is member of politic party \"]\n",
        "  listCandidacyElection = [\"Searched person was candidacy at election(s) \"]\n",
        "  listNotableWork = [\"Searched person did notable work \"]\n",
        "  listAwardsReceived = [\"Searched person received awards \"]\n",
        "  listLanguages = [\"Searched person speaks language(s) \"]\n",
        "  listPlayedPositions = [\"Searched person played position \"]\n",
        "  listLeague = [\"Searched person played in leagues \"]\n",
        "  listTeams = [\"Searched person played in teams \"]\n",
        "  listCitizenship = [\"Searched person has following citizenships \"]\n",
        "  listSiblings = []\n",
        "  listHair = []\n",
        "\n",
        "  listHintsPerson = []\n",
        "\n",
        "  for index, row in queryResults_df.iterrows():\n",
        "    if(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P21\"):\n",
        "      listHintsPerson.append(\"Gender of searched person is \" + row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P569\"):\n",
        "      listHintsPerson.append(\"Searched person was born in \" + row[\"itemLabel.value\"][:4])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P19\"):\n",
        "      listHintsPerson.append(\"Searched person was born in \" + row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P1971\"):\n",
        "      listHintsPerson.append(\"Number of children of seared person is \" + row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P1340\"):\n",
        "      listHintsPerson.append(\"Searched person has \" + row[\"itemLabel.value\"] + \" eyes\")    \n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P2048\"):\n",
        "      listHintsPerson.append(\"Searched persons height is \" + row[\"itemLabel.value\"])    \n",
        "\n",
        "    \n",
        "    #if get multiple values save all in list and pick one afterwards\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P27\"):\n",
        "      listCitizenship.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P106\"):\n",
        "      listOccupation.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P39\"):\n",
        "      listPositionHeld.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P69\"):\n",
        "      listEducatedAt.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P512\"):\n",
        "      listAcademicDegrees.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P102\"):\n",
        "      listPoliticParty.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P3602\"):\n",
        "      listCandidacyElection.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P800\"):\n",
        "      listNotableWork.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P166\"):\n",
        "      listAwardsReceived.append(row[\"itemLabel.value\"])\n",
        "\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P1412\"):\n",
        "      listLanguages.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P413\"):\n",
        "      listPlayedPositions.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P118\"):\n",
        "      listLeague.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P54\"):\n",
        "      listTeams.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P3373\"):\n",
        "      listSiblings.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P1884\"):\n",
        "      listHair.append(row[\"itemLabel.value\"])\n",
        "\n",
        "  #if too many results could be return check\n",
        "  #if more thane 3 -> cut\n",
        "  listResultsCut = [listPositionHeld, listAcademicDegrees, listPoliticParty, \n",
        "                 listCandidacyElection, listNotableWork, listLanguages, \n",
        "                 listPlayedPositions, listLeague, listTeams, listCitizenship, listAwardsReceived, listEducatedAt, listOccupation]\n",
        "  for singleList in listResultsCut:\n",
        "    cutResults(singleList, listHintsPerson)\n",
        "\n",
        "  #just get one hair color\n",
        "  if(len(listHair) > 0):\n",
        "    listHintsPerson.append(\"The searched person has \" + listHair[0])\n",
        "\n",
        "  #get number of siblings\n",
        "  if(len(listSiblings) > 0):\n",
        "    listHintsPerson.append(\"The searched person has \" + str(len(listSiblings)) + \" siblings\")  \n",
        "  \n",
        "  return listHintsPerson"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#if more results than 3 -> print 3 results + \"and x others\"\n",
        "\n",
        "def cutResults(listResults, listHintsPerson):\n",
        "  \n",
        "  numResults = len(listResults)-1\n",
        "  resultsLeft = numResults-3\n",
        "\n",
        "  if (len(listResults) > 1):\n",
        "    if(resultsLeft > 0):\n",
        "      listHintsPerson.append(getConcatenationMultipleResults(listResults[0:4])+ \" and \" + str(resultsLeft) + \" others\")\n",
        "    else:\n",
        "      listHintsPerson.append(getConcatenationMultipleResults(listResults))"
      ],
      "metadata": {
        "id": "KDP4yoRpB_Sq"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIOq3XcwIr2a"
      },
      "source": [
        "#concatenate multiple results with comma\n",
        "\n",
        "def getConcatenationMultipleResults(resultList):\n",
        "  concatenation = \"\"\n",
        "  commaCounter = len(resultList)-1\n",
        "  for item in resultList:\n",
        "    if(commaCounter == len(resultList)-1):\n",
        "      concatenation += item\n",
        "      commaCounter-=1\n",
        "    elif(commaCounter > 0):\n",
        "      concatenation += item + \", \"\n",
        "      commaCounter-=1\n",
        "    elif(commaCounter == 0):\n",
        "      concatenation += item\n",
        "  \n",
        "  return concatenation\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B74PSzscAzl8"
      },
      "source": [
        "def writePersonHintsInFile(listHintsPerson):\n",
        "  with open(\"hintsPerson.txt\", 'a') as writefile:\n",
        "    #writefile.write(\"\\n\\n---Hints for Person \" + person + \"---\\n\\n\")\n",
        "    randomHintsList = random.sample(listHintsPerson, len(listHintsPerson))\n",
        "    for item in randomHintsList:\n",
        "      writefile.write(item + \"\\n\")\n",
        "    writefile.close()"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LORwHLog-t_"
      },
      "source": [
        "# Hints if answer is a Year"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dil5f183hhHJ"
      },
      "source": [
        "**Get all outlinks of wikipedia year page and pass to getBacklinks. Than look for return values on page.content and print line**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQFplu-7AF1L"
      },
      "source": [
        "import spacy\n",
        "def hintsYear(year):\n",
        "\n",
        "  page = wikipedia.page(year, auto_suggest=False)\n",
        "  pageContent = page.content\n",
        "\n",
        "  #get all links from year page\n",
        "  dictPageLinks = getOutlinks2(year)\n",
        "  #print(dictPageLinks)\n",
        "\n",
        "  #get rid of unnesercery links\n",
        "  filteredPageLinks = filterDict(dictPageLinks)\n",
        "\n",
        "  \n",
        "  #get backlinks of all links\n",
        "  dictBacklink = getBacklinksDict(filteredPageLinks)\n",
        "  #first200 = {k: filteredPageLinks[k] for k in list(filteredPageLinks)[:200]}\n",
        "  #dictBacklink = getBacklinksDict(first200)\n",
        "\n",
        "  #remove parantheses of links\n",
        "  dictAdjusted = removeParanthesesDict(dictBacklink)\n",
        "\n",
        "  #get all possible hint (all lines with a entity #backlinks > 1000)\n",
        "  dictPossibleHints = findLineInTextEvent(dictAdjusted, year)\n",
        "\n",
        "  with open('./resultsEvents.txt', 'a') as writefile:\n",
        "    writefile.write(\"Question: \" + questionYear + \"(\"+ str(year) + \")---\\n\")\n",
        "\n",
        "  #check if entity because of which line was found is subject\n",
        "  #if it is not the subject -> drop\n",
        "  dictFinalHints = getFinalHints(dictPossibleHints)\n",
        "\n",
        "  #get rid of number of foundevents at end of line\n",
        "  #need to put number at the and because it is dict and would be overwritten\n",
        "  dictFinalHintsAdjusted = {}\n",
        "  for key, val in dictFinalHints.items():\n",
        "    dictFinalHintsAdjusted.update({key.split('///', 1)[0]: val})  \n",
        "\n",
        "  #returns dict sorted after PageViews\n",
        "  dictSortedPV = sortHintsNumberPVSubject(dictFinalHintsAdjusted)\n",
        "\n",
        "  #get highest PageView\n",
        "  maxPV = list(dictSortedPV.values())[0]\n",
        "\n",
        "  #sort hints PageViews and Simscore combined\n",
        "  sortHintsUtilityScore(questionYear, dictSortedPV, maxPV)\n",
        "\n",
        "  listHumansNames = []\n",
        "\n",
        "  for key, val in dictAdjusted.items():\n",
        "    result = checkIfHuman(key)\n",
        "    if(result != None):\n",
        "      listHumansNames.append(result)\n",
        "\n",
        "  #findLineInTextBirthDeath(listHumansNames, year)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-dJk5vrhq6M"
      },
      "source": [
        "**Function to find line of most important link in year content**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0GonOTeikJm"
      },
      "source": [
        "import re\n",
        "\n",
        "def findLineInTextEvent(entitiesEvent, year):\n",
        "\n",
        "  page = wikipedia.page(year, auto_suggest=False)\n",
        "  pageText = page.content\n",
        "  splitText = []\n",
        "\n",
        "  splitText = (re.split(\"== Events ==|== Births ==\", pageText ))\n",
        "\n",
        "  contentEvents = splitText[1]\n",
        "\n",
        "  foundEvent = 0\n",
        "\n",
        "  dictHintsEntity = {}\n",
        "\n",
        "  for key, val in entitiesEvent.items():\n",
        "    #if (foundEvent == 20):\n",
        "      #break;\n",
        "\n",
        "    for line in contentEvents.split(\"\\n\"):\n",
        "      #if (foundEvent == 20):\n",
        "        #break;\n",
        "\n",
        "      if val in line:\n",
        "        words = line.split()\n",
        "        #counterEntitiesInLine = 0\n",
        "        if(len(words) < 15):\n",
        "          dictHintsEntity.update({line + \"///\" + str(foundEvent): val})\n",
        "          #listHints.append(line)\n",
        "          foundEvent += 1\n",
        "          #break;\n",
        "  return dictHintsEntity"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZAM4MxB-8By"
      },
      "source": [
        "def findLineInTextBirthDeath(entitiesHuman, year):\n",
        "\n",
        "  page = wikipedia.page(year, auto_suggest=False)\n",
        "  pageText = page.content\n",
        "  splitText = []\n",
        "\n",
        "  splitText = (re.split(\"== Births ==|== Deaths ==\", pageText ))\n",
        "\n",
        "  print(splitText)\n",
        "\n",
        "  contentBirths = splitText[1]\n",
        "  contentDeaths = splitText[2]\n",
        "\n",
        "  foundBirth = 0\n",
        "  foundDeath = 0\n",
        "\n",
        "  #open file and append results\n",
        "  with open('./resultsBirthDeath.txt', 'a') as writefile:\n",
        "    writefile.write(\"---Birth/Death in year \" + str(year) + \"---\")\n",
        "\n",
        "    for entity in entitiesHuman:\n",
        "      #if (foundBirth == 3 and foundDeath == 3):\n",
        "        #writefile.write(\"\\n\")\n",
        "        #break;\n",
        "\n",
        "      for line in contentBirths.split(\"\\n\"):\n",
        "        #if (foundBirth == 3):\n",
        "          #break;\n",
        "\n",
        "        if entity in line:\n",
        "          writefile.write(\"Found with entity \" + entity + \"\\n\")\n",
        "          writefile.write(\"Birth: \" + line + \"\\n\")\n",
        "          writefile.write(\"\\n\")\n",
        "          foundBirth +=1\n",
        "          break;\n",
        "\n",
        "      for line in contentDeaths.split(\"\\n\"):\n",
        "        #if (foundDeath == 3):\n",
        "          #break;\n",
        "        if entity in line:\n",
        "          writefile.write(\"Found with entity \" + entity + \"\\n\")\n",
        "          writefile.write(\"Death: \" + line + \"\\n\")\n",
        "          writefile.write(\"\\n\")\n",
        "          foundDeath += 1\n",
        "          break;\n",
        "    writefile.write(\"\\n\")\n",
        "      "
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUy44eeb0EKg"
      },
      "source": [
        "def sortHintsNumberPVSubject(dictHints):\n",
        "\n",
        "  with open('./resultsEvents.txt', 'a') as writefile: \n",
        "\n",
        "    dictFinalHintsPV = {}\n",
        "\n",
        "    for key, val in dictHints.items():\n",
        "      dictFinalHintsPV.update({key: getPageViews(val)})\n",
        "\n",
        "    dictFinalHintsSortedPV = dict(sorted(dictFinalHintsPV.items(), key=lambda item: item[1], reverse=True))\n",
        "    \"\"\"writefile.write(\"\\n--Final Hints sorted #PV---\\n\")\n",
        "    for key, val in dictFinalHintsSortedPV.items():\n",
        "      writefile.write(key + \"(\" + str(val) + \" PageViews)\\n\")\"\"\"\n",
        "\n",
        "  return dictFinalHintsSortedPV\n",
        "\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4YpAjm5B6CM"
      },
      "source": [
        "\n",
        "\n",
        "def sortHintsUtilityScore(question, dictFinalHintsSortedPV, maxPV):\n",
        "  model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "  question_embedding = model.encode(question)\n",
        "\n",
        "  alpha = 0.5\n",
        "\n",
        "  #sentence_embeddings = model.encode(hints)\n",
        "\n",
        "  dictHintsBERT = {}\n",
        "\n",
        "  for key, val in dictFinalHintsSortedPV.items():\n",
        "    sentence_embedding = model.encode(key)\n",
        "    simScore = cosine_similarity([question_embedding], [sentence_embedding]).item()\n",
        "    impScore = alpha * (val/maxPV) + (1-alpha) * simScore\n",
        "    dictHintsBERT.update({key: impScore})\n",
        "\n",
        "  with open('./resultsEvents.txt', 'a') as writefile:\n",
        "\n",
        "    dictHintsBERT = dict(sorted(dictHintsBERT.items(), key=lambda item: item[1], reverse=True))\n",
        "    writefile.write(\"\\n--Final Hints sorted Utility Score---\\n\")\n",
        "    for key, val in dictHintsBERT.items():\n",
        "      writefile.write(key + \"(\" + str(val) + \" Utility Score)\\n\")\n",
        "\n",
        "  return dictHintsBERT\n",
        " "
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxuRQ-FsZdQ_"
      },
      "source": [
        "def get_subject_phrase(doc):\n",
        "    for token in doc:\n",
        "        if (\"subj\" in token.dep_):\n",
        "            subtree = list(token.subtree)\n",
        "            start = subtree[0].i\n",
        "            end = subtree[-1].i + 1\n",
        "            return doc[start:end]"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBif9Lc8_sg3"
      },
      "source": [
        "def getFinalHints(dictPossibleHints):\n",
        "\n",
        "  #nlp = spacy.load(\"en\")\n",
        "  nlp = spacy.load(\"en_core_web_sm\")\n",
        "  dictFinalHintsSubject = {}\n",
        "  \n",
        "  for key, val in dictPossibleHints.items():\n",
        "    doc = nlp(key)\n",
        "    subject = get_subject_phrase(doc)\n",
        "    if (val in str(subject)):\n",
        "      dictFinalHintsSubject.update({key: val})\n",
        "      #listFinalHints.append(key)\n",
        "\n",
        "  return dictFinalHintsSubject"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JUt_0x6jD-x"
      },
      "source": [
        "\n",
        "def getOutlinks2(year):\n",
        "\n",
        "  URL = 'https://en.wikipedia.org/wiki/' + str(year)\n",
        "\n",
        "  # Fetch all the HTML source from the url\n",
        "  response = requests.get(URL)\n",
        "\n",
        "\n",
        "  soup = bs4.BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "  listTextOfLinks = []\n",
        "  listLinks = []\n",
        "\n",
        "  dictLinkName = {}\n",
        "\n",
        "\n",
        "  for link in soup.find_all(\"a\"):\n",
        "    listTextOfLinks.append(link.get_text())\n",
        "    dictLinkName.update({str(link.get(\"href\")).split('/')[-1].replace('_', ' '): link.get_text()})\n",
        "\n",
        "  #newDict = filterDict(dictLinkName)\n",
        "\n",
        "  #print(len(dictLinkName))\n",
        "  #print(len(newDict))\n",
        "  #print(newDict)\n",
        "\n",
        "  return dictLinkName"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAuFgVXomCdf"
      },
      "source": [
        "def filterDict(dictNameLink):\n",
        "\n",
        "  patternYear = r\"[1-3][0-9]{3}\"\n",
        "  patternCentury = r\"[1-2][0-9]\\w+.century\"\n",
        "  patternAd = r\"AD.\\d\\d\\d\\d\"\n",
        "  patternMonth = r\"[a-zA-Z]+\\s\\d+\"\n",
        "  patternMillennium = r\".+millennium\"\n",
        "  patternHashtag = r\"#\"\n",
        "  patternCalendar = r\"calendar\"\n",
        "  patternCategory = r\"category\"\n",
        "  patternList = r\"list\"\n",
        "  patternTemplate = r\"template\"\n",
        "  patternWikipedia = r\"wikipedia\"\n",
        "  patternIndex = r\"index\"\n",
        "  patternHtml = r\"html\"\n",
        "  patternPercent = r\"%\"\n",
        "\n",
        "  newDict = {}\n",
        "\n",
        "  for key, val in dictNameLink.items():\n",
        "    if(not re.match(patternYear, key) and\n",
        "       not re.match(patternCentury, key) and \n",
        "       not re.match(patternAd, key) and \n",
        "       not re.match(patternMonth, key) and\n",
        "       not re.search(patternHashtag, key) and\n",
        "       not re.search(patternCalendar, key, re.IGNORECASE) and\n",
        "       not re.search(patternCategory, key, re.IGNORECASE) and\n",
        "       not re.search(patternList, key, re.IGNORECASE) and\n",
        "       not re.search(patternTemplate, key, re.IGNORECASE) and\n",
        "       not re.search(patternWikipedia, key, re.IGNORECASE) and\n",
        "       not re.search(patternIndex, key, re.IGNORECASE) and\n",
        "       not re.search(patternHtml, key, re.IGNORECASE) and\n",
        "       not re.search(patternPercent, key) and\n",
        "       not re.match(patternMillennium, key) and\n",
        "       not val == \"\" and not val == \" \"):\n",
        "      \n",
        "\n",
        "      newDict.update({key: val})\n",
        "\n",
        "\n",
        "  print(newDict)\n",
        "\n",
        "  return newDict\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleanup"
      ],
      "metadata": {
        "id": "94cf7FEdL-3p"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R87Y1LPF55PL"
      },
      "source": [
        "**Clean GitHub directory**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "%rm -r optimizationOfHintGeneration\n",
        "%cd /content/\n",
        "%rm hintsLocation.txt\n",
        "%rm hintsPerson.txt\n",
        "%rm resultsEvents.txt"
      ],
      "metadata": {
        "id": "2cREJk4T5v7S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}