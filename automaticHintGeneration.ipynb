{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "cxaEE5DFfr_v",
        "WckjgVs-fZto",
        "s5rwSDisuN9s",
        "du5Un8VufnlG",
        "_lhmuuUNQ__8",
        "0Yf6Fi_34ye5",
        "0LORwHLog-t_"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexWalcher/automaticHintGeneration/blob/Test/automaticHintGeneration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##How to use:\n",
        "1. Run **All imports**\n",
        "2. Run **Functions for unexpected prediction methods**\n",
        "3. Run **Generate hints - (putting everything together)**"
      ],
      "metadata": {
        "id": "sdufKBvPeQbe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wqMxtJncEqQT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5rwSDisuN9s"
      },
      "source": [
        "# **All imports**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# This cell will not display any output\n",
        "%cd /content/\n",
        "%rm -r automaticHintGeneration\n",
        "!git clone https://github.com/AlexWalcher/automaticHintGeneration.git\n",
        "%cd /content/"
      ],
      "metadata": {
        "id": "b_jGbTrbz8x2"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# This cell will not display any output\n",
        "!pip install selenium\n",
        "!apt-get update\n",
        "!apt-get install firefox\n",
        "!apt install firefox-geckodriver\n",
        "!pip install sparqlwrapper\n",
        "!pip install pageviewapi\n",
        "!pip install sentence-transformers\n",
        "!pip install wikipedia\n",
        "!pip install requests"
      ],
      "metadata": {
        "id": "98Zh7IC7mSWI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# This cell will not display any output\n",
        "\n",
        "#new imports\n",
        "import random\n",
        "import spacy\n",
        "import time\n",
        "import requests\n",
        "import re\n",
        "import difflib\n",
        "import pprint\n",
        "import itertools\n",
        "import wikipedia\n",
        "# import wikipediaapi\n",
        "!pip install lxml\n",
        "#import xml.etree.ElementTree as ET\n",
        "import lxml.etree as ET\n",
        "import xml.etree.ElementTree as ET\n",
        "import urllib.request\n",
        "\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth',1000)\n",
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
        "import bs4\n",
        "from bs4 import BeautifulSoup, NavigableString, Tag\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "#!pip install -U pip setuptools wheel\n",
        "#!pip install -U spacy\n",
        "#!python -m spacy download en_core_web_sm\n",
        "#!python -m spacy link en_core_web_sm en\n",
        "from selenium import webdriver\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse, parse_qs\n",
        "from collections import OrderedDict\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "#from collections import OrderedDict\n",
        "import collections.abc as collections\n",
        "from collections.abc import Mapping\n",
        "#from collections import Mapping\n",
        "\n",
        "!pip install git+https://github.com/Commonists/pageview-api.git\n",
        "!pip install pageviewapi\n",
        "\n",
        "#import pageviewapi"
      ],
      "metadata": {
        "id": "chGS_8doxwwE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# This cell will not display any output\n",
        "!pip install Wikipedia-API\n",
        "import wikipediaapi\n",
        "!pip install wikidata\n",
        "import wikidata"
      ],
      "metadata": {
        "id": "pTRRIW0tnxd4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load File**"
      ],
      "metadata": {
        "id": "WckjgVs-fZto"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_K8U-ZpsRss_"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "#this cell was reused from the old prediction methods\n",
        "\n",
        "df = pd.ExcelFile(\"./automaticHintGeneration/testSet.xlsx\").parse(\"Sheet1\")\n",
        "x = []\n",
        "x.append(df[\"Answer\"])\n",
        "\n",
        "dataPerson = []\n",
        "dataYear = []\n",
        "dataLocation = []\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "  if(row[\"Category\"] == \"Person\"):\n",
        "    dataPerson.append([row[\"Question\"], row[\"Answer\"]])\n",
        "  elif(row[\"Category\"] == \"Year\"):\n",
        "    dataYear.append([row[\"Question\"], row[\"Answer\"]])\n",
        "  elif(row[\"Category\"] == \"Location\"):\n",
        "    dataLocation.append([row[\"Question\"], row[\"Answer\"]])\n",
        "\n",
        "person_df = pd.DataFrame(dataPerson, columns=[\"Question\", \"Answer\"])\n",
        "year_df = pd.DataFrame(dataYear, columns=[\"Question\", \"Answer\"])\n",
        "location_df = pd.DataFrame(dataLocation, columns=[\"Question\", \"Answer\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Functions for unexpected prediction methods**"
      ],
      "metadata": {
        "id": "S6IVsfX-fQ3n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **New prediction method for years**"
      ],
      "metadata": {
        "id": "mKh1YgDQt3BC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We tried different ways of finding pieces of information that most people remember about a certain year. Three different approaches have been taken: pre-choosen \"most important\" events from wikipedia thumbcaption description, sports events and historical events.\n",
        "\n",
        "---\n",
        "\n",
        "Short explenation of the functions:\n",
        "*   **Thumbcaption** part: picture description of the wiki-years-page converted into hint-sentences; ranked after entry with the most pageviews\n",
        "*   Popular **sports events**: create hint-sentences with pre determined popular reoccuring sports events; ranked after pageviews of the wiki-sports-page\n",
        "*   **Vizgr** events: historical events from vizgr.org; ranked after a NLP-Bernt similarity score between the question and the historical event\n",
        "\n"
      ],
      "metadata": {
        "id": "McNzssrA4lza"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dictionary for thumbcaption part of a year\n",
        "\n"
      ],
      "metadata": {
        "id": "oq8Zj9_w-aS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Given a URL, this function opens the url and retrieves the information stored in a table.\n",
        "\"\"\"\n",
        "def get_table_info(url):\n",
        "  options = webdriver.FirefoxOptions()\n",
        "  #options.headless = True\n",
        "  options.add_argument('--headless')\n",
        "  #print(\"get_table_info TEST\")\n",
        "  try:\n",
        "    driver = webdriver.Firefox(options=options)\n",
        "    driver.get(url)\n",
        "    time.sleep(10) # Wait for the page to load completely\n",
        "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "    driver.quit()\n",
        "    table = soup.find('table')\n",
        "    rows = table.find_all('tr')\n",
        "    headers = [header.text.strip() for header in rows[0].find_all('th')]\n",
        "    data = []\n",
        "  except Exception as e:\n",
        "    # Exception handling code\n",
        "    pass\n",
        "    #print(f\"An exception occurred: {e}\")\n",
        "\n",
        "  for row in rows[1:]:\n",
        "      data.append([cell.text.strip() for cell in row.find_all('td')])\n",
        "  return (headers, data)\n",
        "\n",
        "\"\"\"\n",
        "Given a URL, this function opens the url and retrieves the information stored in a table.\n",
        "\"\"\"\n",
        "def get_table_info_requests(url):\n",
        "  headers = []\n",
        "  data = []\n",
        "\n",
        "  response = requests.get(url)\n",
        "  soup = BeautifulSoup(response.content, 'html.parser')\n",
        "  table = soup.find('table')\n",
        "  rows = table.find_all('tr')\n",
        "  headers = [header.text.strip() for header in rows[0].find_all('th')]\n",
        "  data = []\n",
        "  for row in rows[1:]:\n",
        "    data.append([cell.text.strip() for cell in row.find_all('td')])\n",
        "  return (headers, data)\n",
        "\n",
        "\"\"\"\n",
        "Given a Wikipedia URL, and a section of this article, returns an array of dictionaries containing the href, title, and description of each link on that section of the page.\n",
        "\"\"\"\n",
        "def get_links_in_section(wikipedia_url, section_heading):\n",
        "\n",
        "  response = requests.get(wikipedia_url)\n",
        "  soup = BeautifulSoup(response.content, 'html.parser')\n",
        "  section = soup.find('span', {'id': section_heading})\n",
        "  if section is None:\n",
        "    return None\n",
        "  section_content = section.parent.find_next_sibling('ul')\n",
        "  if section_content is None:\n",
        "    return None\n",
        "  links = []\n",
        "  for item in section_content.find_all('li'):\n",
        "    link = item.find('a')\n",
        "    if link is not None and link.has_attr('href') and link['href'].startswith('/wiki/'):\n",
        "      title = link.get('title', '')\n",
        "      description = item.text.strip()\n",
        "      url = f\"https://en.wikipedia.org{link['href']}\"\n",
        "      links.append({'title': title, 'description': description, 'url': url})\n",
        "  #print(links)\n",
        "  return links\n",
        "\n",
        "\"\"\"\n",
        "Given a Wikipedia URL, and a section of this article, returns an array of dictionaries containing the href, title, and description of each link  with its sublinks as well.\n",
        "\"\"\"\n",
        "def get_links_in_section_with_sublinks(wikipedia_url, section_title):\n",
        "\n",
        "  response = requests.get(wikipedia_url)\n",
        "  if response.status_code == 200:\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    section_heading = soup.find('span', {'id': section_title})\n",
        "    if section_heading is None:\n",
        "      return None\n",
        "    section = section_heading.parent\n",
        "    section_content = section.find_next_sibling('ul')\n",
        "    if section_content is None:\n",
        "      return None\n",
        "    links = []\n",
        "    for item in section_content.find_all('li'):\n",
        "      link = item.find('a')\n",
        "      if link is not None:\n",
        "        link_title = link.get('title')\n",
        "        link_url = 'https://en.wikipedia.org' + link.get('href')\n",
        "        link_description = item.text.replace(link_title, '').strip()\n",
        "        links.append({'title': link_title, 'url': link_url, 'description': link_description})\n",
        "        for sublink in item.find_all('a', href=True, recursive=False):\n",
        "          sublink_title = sublink.get('title')\n",
        "          sublink_url = 'https://en.wikipedia.org' + sublink.get('href')\n",
        "          sublink_description = sublink.parent.text.replace(sublink_title, '').replace(link_title, '').strip()\n",
        "          links.append({'title': sublink_title, 'url': sublink_url, 'description': sublink_description})\n",
        "    return links\n",
        "  else:\n",
        "    return None\n",
        "\n",
        "\"\"\"\n",
        "Returns all backlinks located in the thumbcaption section\n",
        "\"\"\"\n",
        "def get_wikipedia_backlinks_thumbcaption(url):\n",
        "  # Load the Wikipedia page HTML\n",
        "  page_html = requests.get(url).text\n",
        "  soup = BeautifulSoup(page_html, 'html.parser')\n",
        "\n",
        "  # Find the image caption on the page\n",
        "  caption = soup.find('div', class_='thumbcaption')\n",
        "\n",
        "  if caption is not None:\n",
        "    # Extract the caption text and any backlinks\n",
        "    backlink_sentences = {}\n",
        "    sentences = caption.text.strip().split('.')\n",
        "    for sentence in sentences:\n",
        "      links = caption.find_all('a', href=True, string=re.compile(sentence))\n",
        "      if len(links) > 0:\n",
        "        backlinks = []\n",
        "        for link in links:\n",
        "          backlink_url = 'https://en.wikipedia.org' + link['href']\n",
        "          backlink_title = link.get('title', '')\n",
        "          backlinks.append((backlink_url, backlink_title))\n",
        "        backlink_sentences[sentence] = backlinks\n",
        "  return backlink_sentences\n",
        "\n",
        "\"\"\"\n",
        "This function correctly prunes the links to only include the string after the last / character\n",
        "\"\"\"\n",
        "def prune_links(links):\n",
        "  pruned_links = []\n",
        "  for url, title in links:\n",
        "    pruned_url = url.split('/')[-1]\n",
        "    pruned_links.append((pruned_url, title))\n",
        "  return pruned_links\n",
        "\n",
        "\"\"\"\n",
        "This function first initializes an empty list combined_list that will hold the combined strings. Then, it uses a for loop to iterate through the input list in increments of 10 tuples at a time.\n",
        "For each sub-list of up to 10 tuples, it uses the list comprehension and join() method as before to combine the first elements of each tuple into a single string separated by '|'.\n",
        "Finally, the function appends each combined string to the combined_list and returns it at the end.\n",
        "\"\"\"\n",
        "def combine_first_elements(my_list):\n",
        "  combined_list = []\n",
        "  num_tuples = len(my_list)\n",
        "  for i in range(0, num_tuples, 10):\n",
        "    sub_list = my_list[i:i+10]\n",
        "    combined_str = '|'.join([tup[0] for tup in sub_list])\n",
        "    combined_list.append(combined_str)\n",
        "  return combined_list\n",
        "\n",
        "\"\"\"\n",
        "This function first initializes an empty list url_list that will hold the modified URLs. Then, it uses a for loop to iterate through each combined string in the input list.\n",
        "For each combined string, it concatenates the base URL with a forward slash and the combined string, and appends the resulting URL to the url_list. Finally, the function returns the url_list at the end.\n",
        "\"\"\"\n",
        "def add_combined_strings_to_url(base_url, combined_strings):\n",
        "  url_list = []\n",
        "  for string in combined_strings:\n",
        "    url_list.append(base_url + '/' + string)\n",
        "  return url_list\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "This will output a list of all the sentences in the thumbcaption, split by ';'.\n",
        "\"\"\"\n",
        "def get_thumbcaption_sentences(url):\n",
        "  # Get the HTML content of the page\n",
        "  page = requests.get(url)\n",
        "  soup = BeautifulSoup(page.content, 'html.parser')\n",
        "\n",
        "  # Find the thumbcaption element\n",
        "  thumbcaption = soup.find('div', class_='thumbcaption')\n",
        "\n",
        "  # Get all the sentences in the thumbcaption\n",
        "  sentences = thumbcaption.text.split('; ')\n",
        "\n",
        "  return sentences\n",
        "\n",
        "\"\"\"\n",
        "This function converts a list into a dict in the way that is needed.\n",
        "\"\"\"\n",
        "def list_to_dict(lst):\n",
        "  result = {}\n",
        "  for sublist in lst:\n",
        "    if len(sublist) >= 4:\n",
        "      key = sublist[1]\n",
        "      value = int(sublist[3].replace(',', ''))\n",
        "      result[key] = value\n",
        "  return result\n",
        "\n",
        "\"\"\"\n",
        "This function takes a list of links, opens each link to get the data, discards the header and converts the data into a dictionary using the list_to_dict() function,\n",
        "and then updates a combined dictionary with the resulting dictionary from each link. Finally, it returns the combined dictionary.\n",
        "\"\"\"\n",
        "def combine_dicts_from_links(link_list):\n",
        "  combined_dict = {}\n",
        "  for link in link_list:\n",
        "    header, data = get_table_info(link)\n",
        "    #print(data)\n",
        "    link_dict = list_to_dict(data)\n",
        "    combined_dict.update(link_dict)\n",
        "  return combined_dict\n",
        "\n",
        "\"\"\"\n",
        "This sorts the items in the dictionary based on the integer value of the second element in each key-value tuple (i.e. item[1]), in descending order (reverse=True).\n",
        "\"\"\"\n",
        "def sort_dict_desc(d):\n",
        "  return {k: v for k, v in sorted(d.items(), key=lambda item: int(item[1]), reverse=True)}\n",
        "\n",
        "\"\"\"\n",
        "This function takes in the ord dictionary and the sentences list as arguments.\n",
        "It initializes an empty list called result that we will append the found sentences to. It then iterates over each key in the ord dictionary and for each key, it iterates over each sentence in the sentences list.\n",
        "If the key is found in the sentence, the sentence is appended to the result list\n",
        "\"\"\"\n",
        "def find_sentences(ord, sentences):\n",
        "  result = []\n",
        "  for key in ord:\n",
        "    for sentence in sentences:\n",
        "      if key in sentence:\n",
        "        result.append(sentence)\n",
        "  return result\n",
        "\n",
        "\"\"\"\n",
        "This function takes a list of sentences and a keyword, removes the keyword from each sentence in the list, and returns the updated list.\n",
        "\"\"\"\n",
        "def remove_keyword(sentences, keyword):\n",
        "  updated_sentences = []\n",
        "  for sentence in sentences:\n",
        "    updated_sentence = sentence.replace(keyword, \"\")\n",
        "    updated_sentences.append(updated_sentence)\n",
        "  return updated_sentences\n",
        "\n",
        "\"\"\"\n",
        "This function takes a list of sentences and a list of keyword, removes the keyword from each sentence in the list, and returns the updated list. MAYBE NOT WORKING\n",
        "    Removes one or more keywords from each sentence in the list of sentences.\n",
        "\n",
        "\"\"\"\n",
        "def remove_keywords(sentences, keywords):\n",
        "  result = sentences\n",
        "  for entry in keywords:\n",
        "    result = remove_keyword(result, entry)\n",
        "  return result\n",
        "\n",
        "\"\"\"\n",
        "This function prepends a given string to each sentence in a list.\n",
        "\"\"\"\n",
        "def prepend_string(sentences, prepend_str):\n",
        "  new_sentences = []\n",
        "  for sentence in sentences:\n",
        "    new_sentence = f\"{prepend_str}{sentence}\"\n",
        "    new_sentences.append(new_sentence)\n",
        "  return new_sentences\n",
        "\n",
        "\"\"\"\n",
        "This function creates a new list new_sentences and loops over each sentence in the input sentences list.\n",
        "It then uses a list comprehension and the difflib.SequenceMatcher class to compare the sentence to each sentence already in new_sentences.\n",
        "If the ratio of similarity between the two sentences is greater than 0.8 (adjust this threshold as needed), it considers the sentence to be similar and skips it. Otherwise, it adds the sentence to new_sentences.\n",
        "Finally, it returns the new list of unique sentences.\n",
        "\"\"\"\n",
        "def remove_similar(sentences):\n",
        "  new_sentences = []\n",
        "  for sentence in sentences:\n",
        "    if not any(difflib.SequenceMatcher(None, sentence, s).ratio() > 0.8 for s in new_sentences):\n",
        "      new_sentences.append(sentence)\n",
        "  return new_sentences\n",
        "\n"
      ],
      "metadata": {
        "id": "wzL3elbnOlYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#IMPORTANT\n",
        "\"\"\"\n",
        "Calls the wiki years page, retrieve the thumbcaption part and rewrites it to become sentences. Return those sentences as hints.\n",
        "Args: years_list (list): The years we want the thumbcaption hints from.\n",
        "Returns:  dict: A dictionary with the most popular events of a year retrieved from the thumbcaption part of a wiki years page.\n",
        "\"\"\"\n",
        "def thumbcaption_hints_per_year(years_list):\n",
        "  thumbcaption_hints = {}\n",
        "  wiki_base_link= 'https://en.wikipedia.org/wiki/'\n",
        "  pageviews_range_url = 'https://pageviews.wmcloud.org/?project=en.wikipedia.org&platform=all-access&agent=user&redirects=0&range=all-time&pages='\n",
        "\n",
        "  for y in years_list:\n",
        "    years_key = str(y)\n",
        "    test_link = wiki_base_link + years_key\n",
        "    sentences_of_thumbcaption = get_thumbcaption_sentences(test_link) #just gets the sentences from the thumbcaption section of a wikipedi years page\n",
        "    thumbcaption = get_wikipedia_backlinks_thumbcaption(test_link) #get all backlinks of the thumbcapture of the year (those are the most known events)\n",
        "    thumbcaption_key = next(iter(thumbcaption))\n",
        "    thumbcaption_val = thumbcaption[thumbcaption_key]\n",
        "    pruned = prune_links(thumbcaption_val) #prune those backlinks such that only the important part remains\n",
        "    com = combine_first_elements(pruned) #combine up to 10 of these links to create a request to pageview\n",
        "    url_list = add_combined_strings_to_url(pageviews_range_url, com) #now we have a list of pageview links with all the backlinks of the thumbcaption part of the wiki page\n",
        "    #print(\"URL\")\n",
        "    #print(url_list)\n",
        "    data=combine_dicts_from_links(url_list) #now we called the links and retreieved the pageviews; saved them as a dictionary\n",
        "    ord = sort_dict_desc(data) #now the list is ordered in ascending order\n",
        "    tmp = find_sentences(ord,sentences_of_thumbcaption) #search the corresponding sentence to the keyword (USA and school shooting for example)\n",
        "    #remove the years number from the hints OBVIOUSNESS\n",
        "    keywords_list = [years_key, 'clockwise ', 'Clockwise ', 'From top left', 'from top-left', 'from top left', 'From top-left', 'from top-left: ', ':', 'from left, clockwise'] #list of keywords that should be removed from the sentences\n",
        "    t4=remove_keywords(tmp, keywords_list)\n",
        "    prepend_str = 'In the same year, '\n",
        "    hints = prepend_string(t4, prepend_str)\n",
        "\n",
        "    final_hints = remove_similar(hints) #before adjusting the sentences\n",
        "    thumbcaption_hints[y] = final_hints\n",
        "\n",
        "  return thumbcaption_hints\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Calls the thumbcaption_hints_per_year function for every year and combines those into a dict.\n",
        "Returns:  dict: A dictionary with the most popular events of a year retrieved from the thumbcaption part of a wiki years page.\n",
        "\"\"\"\n",
        "def get_year_thumbcaption_hints():\n",
        "  file_years_list = []\n",
        "  for index, row in year_df.iterrows():\n",
        "    file_years_list.append(row[\"Answer\"])\n",
        "\n",
        "  pop_thumb_hints = thumbcaption_hints_per_year(file_years_list)\n",
        "  return pop_thumb_hints\n",
        "\n",
        "#get_year_thumbcaption_hints()"
      ],
      "metadata": {
        "id": "cvcHIpCheNpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dictionary for popular sports events of a year"
      ],
      "metadata": {
        "id": "O37okmzfPCcM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Some global varaibles for faster execution\n",
        "pop_year_hints = {}\n",
        "pop_thumb_hints = {}\n",
        "\n",
        "\"\"\"\n",
        "Given a URL, this function opens the url and retrieves all tables on the page.\n",
        "\"\"\"\n",
        "def get_all_tables(url):\n",
        "  options = webdriver.FirefoxOptions()\n",
        "  options.add_argument('--headless')\n",
        "\n",
        "  driver = webdriver.Firefox(options=options)\n",
        "  driver.get(url)\n",
        "  time.sleep(1) # Wait for the page to load completely\n",
        "  soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "  driver.quit()\n",
        "\n",
        "  tables = soup.find_all('table')\n",
        "  all_tables = []\n",
        "  for table in tables:\n",
        "    rows = table.find_all('tr')\n",
        "    headers = [header.text.strip() for header in rows[0].find_all('th')]\n",
        "    data = []\n",
        "    for row in rows[1:]:\n",
        "      data.append([cell.text.strip() for cell in row.find_all('td')])\n",
        "    all_tables.append({'headers': headers, 'data': data})\n",
        "\n",
        "  return all_tables\n",
        "\n",
        "\"\"\"\n",
        "Extracts the first element from each sublist in the 'data' list of lists in a dictionary and returns them in a separate list.\n",
        "Args: data_dict (dict): A dictionary containing a 'data' key with a list of lists as its value.\n",
        "Returns: A list containing the first element of each sublist in the 'data' list of lists.\n",
        "\"\"\"\n",
        "def get_first_elements(data_dict):\n",
        "  data_list = data_dict['data']\n",
        "  result = []\n",
        "  for sublist in data_list:\n",
        "    if len(sublist) > 0:\n",
        "      if len(sublist[0]) > 0:\n",
        "        result.append(sublist[0])\n",
        "  return result\n",
        "\n",
        "#prune the list of the hole wiki page down to the important table\n",
        "def prune_dict_list(dict_list, keyw):\n",
        "  for d in dict_list:\n",
        "    if 'headers' in d and d['headers'] == [keyw]:\n",
        "      return d\n",
        "  return None  # if no dict with the desired key-value pair is found\n",
        "\n",
        "#replace the \\n and create a list of lists\n",
        "def create_list_from_list_of_lists_key(lst, keyw):\n",
        "  result = []\n",
        "  for sublst in lst:\n",
        "    if sublst:\n",
        "      new_test = [item.replace(keyw, '') for item in sublst[0].split(keyw)]\n",
        "      result.append(new_test)\n",
        "  return result\n",
        "\n",
        "#replace the : and create a dict; {'year': 'CL-winner'}\n",
        "def create_dict_from_list_of_lists(lst):\n",
        "  result_dict = {}\n",
        "  for sublist in lst:\n",
        "    for item in sublist:\n",
        "      if \":\" in item:\n",
        "        key, value = item.split(\":\")\n",
        "        key = key.strip().replace(\"–\", \"-\")\n",
        "        value = value.strip()\n",
        "        result_dict[key] = value\n",
        "      else:\n",
        "        continue\n",
        "  return result_dict\n",
        "\n",
        "#split on the : and creates a dict\n",
        "def create_dict_from_list(lst):\n",
        "  result = {}\n",
        "  for item in lst:\n",
        "    parts = item.split(\":\")\n",
        "    # parts = parts.strip().replace(\":\", \"\")\n",
        "    result[parts[0]] = parts[1]\n",
        "  return result\n",
        "\n",
        "#splits on [ and creates a dict 1950: 'Farina'\n",
        "def get_year_with_driver(race_results):\n",
        "  results_dict = {}\n",
        "  for result in race_results:\n",
        "    if result:\n",
        "      results_dict[result[0].split(\"[\")[0]] = result[1]\n",
        "  return results_dict\n",
        "\n",
        "#get rid of the links ('Alberto Ascari[20]' => 'Alberto Ascari')\n",
        "def clean_driver_names(results_dict):\n",
        "  for year, driver in results_dict.items():\n",
        "    results_dict[year] = driver.split('[')[0].strip()\n",
        "  return results_dict\n",
        "\n",
        "#deletes the : from the key\n",
        "def clean_dict_keys(dict_to_clean):\n",
        "  cleaned_dict = {}\n",
        "  for key, value in dict_to_clean.items():\n",
        "    cleaned_dict[key.rstrip(':')] = value\n",
        "  return cleaned_dict\n",
        "\n",
        "#function to split on \\n but that lets the city names stay together\n",
        "def create_city_dict(city_list):\n",
        "  city_dict = {}\n",
        "  cities = city_list[0].split('\\n')\n",
        "  for city in cities:\n",
        "    if city:\n",
        "      year, *city_name = city.strip().split()\n",
        "      city_dict[year] = ' '.join(city_name)\n",
        "  return city_dict\n",
        "\n",
        "\n",
        "#get the dict of all the champions league winners\n",
        "def champions_league_winners_list():\n",
        "  champions_league_url = 'https://en.wikipedia.org/wiki/List_of_European_Cup_and_UEFA_Champions_League_finals#List_of_finals'\n",
        "  all = get_all_tables(champions_league_url) #gets all tables of wiki page\n",
        "  #first prune of that huge dict\n",
        "  keyw= 'showvteEuropean Cup and UEFA Champions League winners'\n",
        "  pruned_dict = prune_dict_list(all,keyw)\n",
        "  #second prune\n",
        "  inter = pruned_dict.get('data')\n",
        "  cl_list = inter[2:5] + inter[7:11]\n",
        "  tmp1 = create_list_from_list_of_lists_key(cl_list, '\\n')\n",
        "  tmp2 = create_dict_from_list_of_lists(tmp1)\n",
        "\n",
        "  return tmp2\n",
        "\n",
        "#get the dict of all the euros winners\n",
        "def uefa_euros_winners_list():\n",
        "  euros_url = 'https://en.wikipedia.org/wiki/List_of_UEFA_European_Championship_finals#List_of_finals'\n",
        "  all = get_all_tables(euros_url) #gets all tables of wiki page\n",
        "  #first prune of that huge dict\n",
        "  keyw= 'showvteUEFA European Championship winners'\n",
        "  pruned_dict = prune_dict_list(all,keyw)\n",
        "  #second prune\n",
        "  inter = pruned_dict.get('data')\n",
        "  tmp1 = create_list_from_list_of_lists_key(inter, '\\n')\n",
        "  tmp2 = create_dict_from_list_of_lists(tmp1)\n",
        "\n",
        "  return tmp2\n",
        "\n",
        "#get the dict of all the wold cup winners\n",
        "def uefa_worlds_winners_list():\n",
        "  worlds_url = 'https://en.wikipedia.org/wiki/List_of_FIFA_World_Cup_finals#List_of_final_matches'\n",
        "  all = get_all_tables(worlds_url) #gets all tables of wiki page\n",
        "  tmp3=get_first_elements(all[3])\n",
        "  #first prune of that huge dict\n",
        "  keyw= 'showvteFIFA World Cup'\n",
        "  pruned_dict = prune_dict_list(all,keyw)\n",
        "  #second prune\n",
        "  inter = pruned_dict.get('data')\n",
        "  inter = inter[2]\n",
        "  years = [s for s in inter[0].split('\\n')]\n",
        "  my_dict = dict(zip(years, tmp3))\n",
        "\n",
        "  return my_dict\n",
        "\n",
        "#get the dict of all the F1 drivers world champions\n",
        "def f1_winners_list():\n",
        "  euros_url = 'https://en.wikipedia.org/wiki/List_of_Formula_One_World_Drivers%27_Champions#By_season'\n",
        "  all = get_all_tables(euros_url) #gets all tables of wiki page\n",
        "  lst = all[2].get('data')\n",
        "  tmp2 = get_year_with_driver(lst)\n",
        "  tmp2 = clean_driver_names(tmp2)\n",
        "\n",
        "  return tmp2\n",
        "\n",
        "#get the dict of all summer olympics host cities\n",
        "def summer_olympics_hosts_list():\n",
        "  summerO_url = 'https://en.wikipedia.org/wiki/Summer_Olympic_Games#List_of_Summer_Olympic_Games'\n",
        "  all = get_all_tables(summerO_url) #gets all tables of wiki page\n",
        "  lst = all[10].get('data')\n",
        "  tmp1 = create_list_from_list_of_lists_key(lst, '\\n')\n",
        "  tmp1 = tmp1[0]\n",
        "  tmp1 = create_dict_from_list(tmp1)\n",
        "  tmp1 = clean_dict_keys(tmp1)\n",
        "  tmp1 = clean_driver_names(tmp1)\n",
        "\n",
        "  return tmp1\n",
        "\n",
        "#get the dict of all winter olympics host cities\n",
        "def winter_olympics_hosts_list():\n",
        "  winterO_url = 'https://en.wikipedia.org/wiki/Winter_Olympic_Games#List_of_Winter_Olympic_Games'\n",
        "  all = get_all_tables(winterO_url) #gets all tables of wiki page\n",
        "  lst = all[8].get('data')\n",
        "  tmp1 = create_list_from_list_of_lists_key(lst, '\\n')\n",
        "  tmp1 = tmp1[0]\n",
        "  tmp1 = create_dict_from_list(tmp1)\n",
        "  tmp1 = clean_dict_keys(tmp1)\n",
        "  tmp1 = clean_driver_names(tmp1)\n",
        "\n",
        "  return tmp1"
      ],
      "metadata": {
        "id": "ARMx75Z0QkVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The basic sentences out of which we create the hint-sentences\n",
        "basic_sentences =  ['In the same year, ', 'In the previous year, ', 'In the following year, ']\n",
        "sport_sentences = [' has won the UEFA Champions League.', ' has won the UEFA Euro Football Championship.', ' has won the FIFA World Cup.', ' has won the F1 Drivers World Championship.', ]\n",
        "olympic_sentences = ['In the same year, the Summer Olympics were held in ', 'In the previous year, the Summer Olympics were held in ', 'In the following year, the Summer Olympics were held in ', 'In the same year, the Winter Olympics were held in ', 'In the previous year, the Winter Olympics were held in ', 'In the following year, the Winter Olympics were held in ']\n",
        "\n",
        "#write all of the winners of the different sports categories into lists\n",
        "cl_all = champions_league_winners_list() #For the Champions league,\n",
        "euro_all = uefa_euros_winners_list() #For Football-Euros\n",
        "worlds_all = uefa_worlds_winners_list() #For Football-Worlds\n",
        "f1_all = f1_winners_list() #For Fromula1\n",
        "summer_olympics_all = summer_olympics_hosts_list() #For OlympicSummerGames\n",
        "winter_olympics_all = winter_olympics_hosts_list() #For OlympicWinterGames\n",
        "\n",
        "'''\n",
        "takes a list of years and then creates a dict of dicts, where (if available) the most popular sports events of that year are saved as hints.\n",
        "returns a dict with the corresponding sports events from the years in years_list\n",
        "'''\n",
        "def popular_sports_per_year(years_list):\n",
        "\n",
        "  pop_sport_hints_year = {}\n",
        "  for index in years_list:\n",
        "    year = index\n",
        "    year_s = str(year)\n",
        "\n",
        "    year_dict = {\n",
        "        'cl': '', 'p_cl': '', 'f_cl': '',\n",
        "        'euros': '', 'p_euros': '', 'f_euros': '',\n",
        "        'worlds': '', 'p_worlds': '', 'f_worlds': '',\n",
        "        'f1': '', 'p_f1': '', 'f_f1': '',\n",
        "        'summer': '', 'p_summer': '', 'f_summer': '',\n",
        "        'winter': '', 'p_winter': '', 'f_winter': '',\n",
        "        }\n",
        "\n",
        "  # UEFA Champions League: Create the sentences like (In the same-, the following-, the previous-year)\n",
        "    for key in cl_all:\n",
        "      if int(key.split('-')[1]) == year % 100:\n",
        "        result = cl_all[key]\n",
        "        break\n",
        "    if result:\n",
        "      year_dict['cl'] = basic_sentences[0] + result + sport_sentences[0]\n",
        "    for key in cl_all:\n",
        "      if int(key.split('-')[1]) == (year - 1) % 100:\n",
        "        result = cl_all[key]\n",
        "        break\n",
        "    if result:\n",
        "      year_dict['p_cl'] = basic_sentences[1] + result + sport_sentences[0]\n",
        "    for key in cl_all:\n",
        "      if int(key.split('-')[1]) == (year + 1) % 100:\n",
        "        result = cl_all[key]\n",
        "        break\n",
        "    if result:\n",
        "      year_dict['f_cl'] = basic_sentences[2] + result + sport_sentences[0]\n",
        "\n",
        "  # UEFA EURO Football Championship: Create the sentences like (In the same-, the following-, the previous-year)\n",
        "    for d in euro_all:\n",
        "      if year_s in d:\n",
        "        year_dict['euros'] = basic_sentences[0] + euro_all[year_s] + sport_sentences[1]\n",
        "    for d in euro_all:\n",
        "      t = year - 1\n",
        "      year_int = str(t)\n",
        "      if year_int in d:\n",
        "        year_dict['p_euros'] = basic_sentences[1] + euro_all[year_int] + sport_sentences[1]\n",
        "    for d in euro_all:\n",
        "      t = year + 1\n",
        "      year_int = str(t)\n",
        "      if year_int in d:\n",
        "        year_dict['f_euros'] = basic_sentences[2] + euro_all[year_int] + sport_sentences[1]\n",
        "\n",
        "  # FIFA WORLD Football Championship: Create the sentences like (In the same-, the following-, the previous-year)\n",
        "    for d in worlds_all:\n",
        "      if year_s in d:\n",
        "        year_dict['worlds'] = basic_sentences[0] + worlds_all[year_s] + sport_sentences[2]\n",
        "    for d in worlds_all:\n",
        "      t = year - 1\n",
        "      year_int = str(t)\n",
        "      if year_int in d:\n",
        "        year_dict['p_worlds'] = basic_sentences[1] + worlds_all[year_int] + sport_sentences[2]\n",
        "    for d in worlds_all:\n",
        "      t = year + 1\n",
        "      year_int = str(t)\n",
        "      if year_int in d:\n",
        "        year_dict['f_worlds'] = basic_sentences[2] + worlds_all[year_int] + sport_sentences[2]\n",
        "\n",
        "  # F1 WORLD Drivers Championship: Create the sentences like (In the same-, the following-, the previous-year)\n",
        "    for d in f1_all:\n",
        "      if year_s in d:\n",
        "        year_dict['f1'] = basic_sentences[0] + f1_all[year_s] + sport_sentences[3]\n",
        "    for d in f1_all:\n",
        "      t = year - 1\n",
        "      year_int = str(t)\n",
        "      if year_int in d:\n",
        "        year_dict['p_f1'] = basic_sentences[1] + f1_all[year_int] + sport_sentences[3]\n",
        "    for d in f1_all:\n",
        "      t = year + 1\n",
        "      year_int = str(t)\n",
        "      if year_int in d:\n",
        "        year_dict['f_f1'] = basic_sentences[2] + f1_all[year_int] + sport_sentences[3]\n",
        "\n",
        "  # Summer Olympic Games: Create the sentences like (In the same-, the following-, the previous-year)\n",
        "    for d in summer_olympics_all:\n",
        "      if year_s in d:\n",
        "        year_dict['summer'] = olympic_sentences[0] + summer_olympics_all[year_s]\n",
        "    for d in summer_olympics_all:\n",
        "      t = year - 1\n",
        "      year_int = str(t)\n",
        "      if year_int in d:\n",
        "        year_dict['p_summer'] = olympic_sentences[1] + summer_olympics_all[year_int]\n",
        "    for d in summer_olympics_all:\n",
        "      t = year + 1\n",
        "      year_int = str(t)\n",
        "      if year_int in d:\n",
        "        year_dict['f_summer'] = olympic_sentences[2] + summer_olympics_all[year_int]\n",
        "\n",
        "  # Winter Olympic Games: Create the sentences like (In the same-, the following-, the previous-year)\n",
        "    for d in winter_olympics_all:\n",
        "      if year_s in d:\n",
        "        year_dict['winter'] = olympic_sentences[3] + winter_olympics_all[year_s]\n",
        "    for d in winter_olympics_all:\n",
        "      t = year - 1\n",
        "      year_int = str(t)\n",
        "      if year_int in d:\n",
        "        year_dict['p_winter'] = olympic_sentences[4] + winter_olympics_all[year_int]\n",
        "    for d in winter_olympics_all:\n",
        "      t = year + 1\n",
        "      year_int = str(t)\n",
        "      if year_int in d:\n",
        "        year_dict['f_winter'] = olympic_sentences[5] + winter_olympics_all[year_int]\n",
        "\n",
        "    #write the entry in the dict\n",
        "    pop_sport_hints_year[year] = year_dict\n",
        "\n",
        "  return pop_sport_hints_year\n",
        "\n",
        "#CALL FUNCTION\n",
        "#returns a dictionary of the years that occured in the xls file, with its corresponding hints from the most popular sports events of that year.\n",
        "def get_year_sports_hints():\n",
        "  file_years_list = []\n",
        "  for index, row in year_df.iterrows():\n",
        "    file_years_list.append(row[\"Answer\"])\n",
        "  pop_sport_hints = popular_sports_per_year(file_years_list)\n",
        "\n",
        "  return pop_sport_hints\n"
      ],
      "metadata": {
        "id": "kuWB4hrxPCcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# This cell will not display any output\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Load pre-trained model and tokenizer\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "def preprocess_text(text):\n",
        "  # Tokenize the text and add special tokens\n",
        "  tokens = tokenizer.encode(text, add_special_tokens=True)\n",
        "  # Convert the tokens to a tensor\n",
        "  token_tensor = torch.tensor(tokens).unsqueeze(0)\n",
        "  return token_tensor\n",
        "\n",
        "def get_similarity_score(text1, text2):\n",
        "  # Preprocess both texts\n",
        "  tensor1 = preprocess_text(text1)\n",
        "  tensor2 = preprocess_text(text2)\n",
        "\n",
        "  # Pass both tensors to the model to get the embeddings\n",
        "  with torch.no_grad():\n",
        "    output1 = model(tensor1)\n",
        "    output2 = model(tensor2)\n",
        "\n",
        "  # Compute the cosine similarity between the two embeddings\n",
        "  cosine_sim = torch.nn.functional.cosine_similarity(output1.last_hidden_state.mean(dim=1), output2.last_hidden_state.mean(dim=1), dim=1)\n",
        "  return cosine_sim.item()"
      ],
      "metadata": {
        "id": "OVdCk8gQQifE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dictionary for historical events from vizgr.org"
      ],
      "metadata": {
        "id": "D91xniD07ajS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Downloads an XML file from the specified URL and saves it to the specified filename.\n",
        "Args: url (str): The URL of the XML file to download.\n",
        "Returns: filename (str): The filename (including the path) to save the downloaded XML file.\n",
        "\"\"\"\n",
        "def download_xml_file(url, filename):\n",
        "  response = urllib.request.urlopen(url)\n",
        "  xml_data = response.read()\n",
        "  with open(filename, 'wb') as file:\n",
        "    file.write(xml_data)\n",
        "  return filename\n",
        "\n",
        "\"\"\"\n",
        "Parses an XML file and returns the root element.\n",
        "Args: filename (str): The filename (including the path) of the XML file to parse.\n",
        "Returns: xml.etree.ElementTree.Element: The root element of the parsed XML.\n",
        "\"\"\"\n",
        "def parse_xml_file(xml_file):\n",
        "  tree = ET.parse(xml_file)\n",
        "  root = tree.getroot()\n",
        "  result = {}\n",
        "  result['count'] = root.find('count').text\n",
        "  events = root.findall('event')\n",
        "  for event in events:\n",
        "    event_dict = {}\n",
        "    event_dict['date'] = event.find('date').text\n",
        "    event_dict['description'] = event.find('description').text\n",
        "    event_dict['lang'] = event.find('lang').text\n",
        "    event_dict['granularity'] = event.find('granularity').text\n",
        "    result[event_dict['date']] = event_dict\n",
        "  return result\n",
        "\n",
        "\"\"\"\n",
        "Retrieves historical information from the specified website using the given start and end dates.\n",
        "Args: start_date (str): The start date in the format 'YYYY/MM/DD'; end_date (str): The end date in the format 'YYYY/MM/DD'.\n",
        "Returns: str: The retrieved historical information as a xml file is saved and converted into a dict.\n",
        "\"\"\"\n",
        "def retrieve_historical_information(start_date, end_date):\n",
        "  url = f\"https://www.vizgr.org/historical-events/search.php?format=xml&begin_date={start_date.replace('/', '')}&end_date={end_date.replace('/', '')}\"\n",
        "  filename = \"./automaticHintGeneration/vizgr_events.xml\"\n",
        "  saved_file = download_xml_file(url, filename)\n",
        "  result = parse_xml_file(saved_file)\n",
        "\n",
        "  return result\n",
        "\n",
        "#put everything together\n",
        "\"\"\"\n",
        "Extracts the date and description from the XML string and returns a dictionary.\n",
        "Args: xml_string (str): The XML data in string format.\n",
        "Returns:  dict: A dictionary with the date as the key and the description up until the first dot as the value.\n",
        "\"\"\"\n",
        "def extract_description(xml_file):\n",
        "  extracted_data = {}\n",
        "  for key,value in xml_file.items():\n",
        "    try:\n",
        "      description = value.get('description')\n",
        "      pruned_description = '.'.join(description.split('.')[:1])\n",
        "      extracted_data[key] = pruned_description\n",
        "    except Exception as e:\n",
        "      continue\n",
        "      pass\n",
        "      #print('extract_description: ' , e)\n",
        "\n",
        "  return extracted_data\n",
        "\n",
        "\n",
        "def remove_string(sentence, string_to_remove):\n",
        "  return sentence.replace(string_to_remove, '')\n",
        "\n",
        "\"\"\"\n",
        "Function that retrieves the event information from the vizgr.com website.\n",
        "Returns:  dict: A dictionary with the date as the key and the event descriptions as values.\n",
        "\"\"\"\n",
        "def get_year_vizgr_hints():\n",
        "  file_years_list = []\n",
        "  for index, row in year_df.iterrows():\n",
        "    if row[\"Answer\"] < 2014:\n",
        "      file_years_list.append(row[\"Answer\"])\n",
        "  final_hints = {}\n",
        "\n",
        "  for year in file_years_list:\n",
        "    inter_start_date = str(year) + \"0101\"\n",
        "    inter_end_date = str(year) + \"1231\"\n",
        "    #print(inter_start_date, inter_end_date)\n",
        "    inter_solution = retrieve_historical_information(inter_start_date, inter_end_date)\n",
        "    vizgr_hint_descriptions = extract_description(inter_solution)\n",
        "    vizgr_hint_sentences = {}\n",
        "    for k,v in vizgr_hint_descriptions.items():\n",
        "      if len(v) > 15:\n",
        "        if str(year) in v:\n",
        "          v = remove_string(v,str(year))\n",
        "        if v[0].isspace():\n",
        "          vizgr_hint_sentences[k] = 'In the same year, '+ v[1].lower() + v[2:] + '.'\n",
        "        else:\n",
        "          vizgr_hint_sentences[k] = 'In the same year, '+ v[0].lower() + v[1:] + '.'\n",
        "    final_hints[year] = vizgr_hint_sentences\n",
        "\n",
        "  return final_hints"
      ],
      "metadata": {
        "id": "7k3PhVGpdZHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#biggest possible date: 2013/11/17 20131117\n",
        "\n",
        "pop_vizgr_hints = get_year_vizgr_hints()\n",
        "pprint.pprint(pop_vizgr_hints)"
      ],
      "metadata": {
        "id": "-QHt03PjdwEu",
        "outputId": "ab0602c2-7065-4b0f-fb8c-b41aa3695c90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1994: {'1994/01/06': 'In the same year, in Detroit, Michigan, Nancy Kerrigan '\n",
            "                      'is clubbed on the right leg by an assailant, under '\n",
            "                      \"orders from figure skating rival Tonya Harding's \"\n",
            "                      'ex-husband.',\n",
            "        '1994/01/08': \"In the same year, ''The Superhighway Summit'' is held \"\n",
            "                      \"at UCLA's Royce Hall.\",\n",
            "        '1994/01/15': \"In the same year, the ''SS American Star'' breaks tow \"\n",
            "                      'in the Atlantic Ocean and is beached at Fuerteventura '\n",
            "                      'in the Canary Islands a few days later.',\n",
            "        '1994/01/17': 'In the same year, the  Northridge earthquake, magnitude '\n",
            "                      '6.',\n",
            "        '1994/01/19': 'In the same year, record cold temperatures hit the '\n",
            "                      'eastern United States.',\n",
            "        '1994/01/20': 'In the same year, in South Carolina, Shannon Faulkner '\n",
            "                      'becomes the first female cadet to attend The Citadel, '\n",
            "                      'but soon drops out.',\n",
            "        '1994/01/21': 'In the same year, lorena Bobbitt is found not guilty by '\n",
            "                      'reason of insanity on charges of mutilating her husband '\n",
            "                      'John.',\n",
            "        '1994/01/26': 'In the same year, a man fires 2 blank shots at Charles, '\n",
            "                      'Prince of Wales in Sydney, Australia.',\n",
            "        '1994/02/01': \"In the same year, in Portland, Oregon, Tonya Harding's \"\n",
            "                      'ex-husband Jeff Gillooly pleads guilty for his role in '\n",
            "                      'attacking figure skater Nancy Kerrigan.',\n",
            "        '1994/02/04': 'In the same year, the Federal Open Market Committee '\n",
            "                      'raises the Fed Funds target rate for the first time '\n",
            "                      'since May 1989.',\n",
            "        '1994/02/05': 'In the same year, byron De La Beckwith is convicted of '\n",
            "                      'the 1963 murder of civil rights leader Medgar Evers.',\n",
            "        '1994/02/06': 'In the same year, markale massacres: A Bosnian Serb '\n",
            "                      'Army mortar shell kills 68 civilians and wounds about '\n",
            "                      '200 in a Sarajevo marketplace.',\n",
            "        '1994/02/09': 'In the same year, the  Winter Olympics begin in '\n",
            "                      'Lillehammer.',\n",
            "        '1994/02/22': 'In the same year, aldrich Ames and his wife are charged '\n",
            "                      'with spying for the Soviet Union by the United States '\n",
            "                      'Department of Justice.',\n",
            "        '1994/02/24': 'In the same year, in Gloucester, local police begin '\n",
            "                      'excavations at 25 Cromwell Street, the home of Fred '\n",
            "                      'West, a suspect in multiple murders.',\n",
            "        '1994/02/25': 'In the same year, israeli Kahanist Baruch Goldstein '\n",
            "                      'opens fire inside the Cave of the Patriarchs in the '\n",
            "                      'West Bank he kills 29 Muslims before worshippers beat '\n",
            "                      'him to death.',\n",
            "        '1994/02/27': 'In the same year, australian Federal Sports amp '\n",
            "                      'Environment Minister Ros Kelly resigns over quotThe '\n",
            "                      'Sports Rorts Affairquot, where it was alleged that she '\n",
            "                      'apportioned money for community sporting projects in a '\n",
            "                      'pork barreling fashion.',\n",
            "        '1994/02/28': 'In the same year, mary Ellen Withrow begins her term of '\n",
            "                      'office as Treasurer of the United States, serving under '\n",
            "                      'President Bill Clinton.',\n",
            "        '1994/03/06': 'In the same year, a referendum in Moldova results in '\n",
            "                      'the electorate voting against possible reunification '\n",
            "                      'with Romania.',\n",
            "        '1994/03/07': 'In the same year, the Church of England ordains its '\n",
            "                      'first female priests.',\n",
            "        '1994/03/14': 'In the same year, apple Computer, Inc.',\n",
            "        '1994/03/16': 'In the same year, in Portland, Oregon, Tonya Harding '\n",
            "                      'pleads guilty to conspiracy to hinder prosecution for '\n",
            "                      'trying to cover-up an attack on figure skating rival '\n",
            "                      'Nancy Kerrigan.',\n",
            "        '1994/03/20': 'In the same year, italian journalist Ilaria Alpi and TV '\n",
            "                      'cameraman Miran Hrovatin are assassinated in Somalia.',\n",
            "        '1994/03/21': 'In the same year, the 66th Academy Awards, hosted by '\n",
            "                      'Whoopi Goldberg, are held at the Dorothy Chandler '\n",
            "                      'Pavilion in Los Angeles, California.',\n",
            "        '1994/03/23': 'In the same year, the Eurofighter takes its first '\n",
            "                      'flight in Manching, Germany.',\n",
            "        '1994/03/28': 'In the same year, shell House Massacre: Inkatha Freedom '\n",
            "                      'Party and ANC supporters battle in central Johannesburg '\n",
            "                      'South Africa.',\n",
            "        '1994/03/31': \"In the same year, the journal ''Nature'' reports the \"\n",
            "                      'finding in Ethiopia of the first complete '\n",
            "                      \"''Australopithecus afarensis'' skull (see Human \"\n",
            "                      'evolution).',\n",
            "        '1994/04/06': 'In the same year, kurt Cobain, songwriter and frontman '\n",
            "                      'for the band Nirvana, is found dead at his Lake '\n",
            "                      'Washington home, apparently of a single self-inflicted '\n",
            "                      'gunshot wound.',\n",
            "        '1994/04/16': 'In the same year, voters in Finland decide to join the '\n",
            "                      'European Union in a referendum.',\n",
            "        '1994/04/19': 'In the same year, a Los Angeles jury awards $3.',\n",
            "        '1994/04/20': 'In the same year, paul Touvier is found guilty of '\n",
            "                      'ordering the execution of 7 Jews when he  served in the '\n",
            "                      'Vichy France Milice.',\n",
            "        '1994/04/21': 'In the same year, china Airlines Flight 140, an Airbus '\n",
            "                      'A300, crashes while landing at Nagoya, Japan, killing '\n",
            "                      '264 people.',\n",
            "        '1994/04/27': 'In the same year, south Africa holds its first fully '\n",
            "                      'multiracial elections, marking the final end of '\n",
            "                      'apartheid.',\n",
            "        '1994/04/29': 'In the same year, commodore International declares '\n",
            "                      'bankruptcy.',\n",
            "        '1994/04/30': 'In the same year, formula One driver Roland '\n",
            "                      'Ratzenberger is killed while qualifying for the  San '\n",
            "                      'Marino Grand Prix.',\n",
            "        '1994/05/01': 'In the same year, three-time Formula One world champion '\n",
            "                      'Ayrton Senna is killed in an accident during the San '\n",
            "                      'Marino Grand Prix in Imola, Italy.',\n",
            "        '1994/05/05': 'In the same year, the Bishkek Protocol between Armenia '\n",
            "                      'and Azerbaijan was signed, effectively freezing the '\n",
            "                      'Nagorno-Karabakh conflict.',\n",
            "        '1994/05/06': 'In the same year, an annular eclipse of the sun is '\n",
            "                      'visible across much of North America.',\n",
            "        '1994/05/12': \"In the same year, ice hockey becomes Canada's official \"\n",
            "                      'winter sport.',\n",
            "        '1994/05/17': 'In the same year, malawi holds its first multiparty '\n",
            "                      'elections.',\n",
            "        '1994/05/20': 'In the same year, after a funeral in Cluny Parish '\n",
            "                      'Church, Edinburgh attended by 900 people and after '\n",
            "                      'which 3,000 people lined the streets, John Smith is '\n",
            "                      'buried in a private family funeral on the island of '\n",
            "                      'Iona, at the sacred burial ground of Reilig Odhráin, '\n",
            "                      'which contains the graves of several Scottish kings as '\n",
            "                      'well as monarchs of Ireland, Norway and France.',\n",
            "        '1994/05/21': 'In the same year, italian former minister and Christian '\n",
            "                      'Democrat leader Giulio Andreotti is accused of Mafia '\n",
            "                      'allegiance by the court of Palermo.',\n",
            "        '1994/05/22': 'In the same year, pope John Paul II issues the '\n",
            "                      \"Apostolic Letter ''Ordinatio Sacerdotalis'' from the \"\n",
            "                      \"Vatican, expounding the Catholic Church's position \"\n",
            "                      'requiring quotthe reservation of priestly ordination to '\n",
            "                      'men alone.',\n",
            "        '1994/06/01': 'In the same year, republic of South Africa rejoins the '\n",
            "                      'British Commonwealth after the first democratic '\n",
            "                      'election.',\n",
            "        '1994/06/06': 'In the same year, june 8 ampndash Ceasefire '\n",
            "                      'negotiations for the Yugoslav War begin in Geneva they '\n",
            "                      'agree to a 1-month cessation of hostilities (which does '\n",
            "                      'not last more than a few days).',\n",
            "        '1994/06/12': 'In the same year, nicole Brown Simpson and Ronald '\n",
            "                      'Goldman are murdered outside the Simpson home in Los '\n",
            "                      'Angeles, California.',\n",
            "        '1994/06/14': 'In the same year, the New York Rangers win the Stanley '\n",
            "                      \"Cup, in '''7''' Games over the Vancouver Canucks.\",\n",
            "        '1994/06/15': 'In the same year, israel and the Vatican establish full '\n",
            "                      'diplomatic relations.',\n",
            "        '1994/06/20': 'In the same year, dean Mellberg, an ex-U.',\n",
            "        '1994/06/23': 'In the same year, the International Olympic Committee '\n",
            "                      'celebrates their first centennial.',\n",
            "        '1994/06/26': 'In the same year, microsoft announces it will no longer '\n",
            "                      'sell or support the MS-DOS operating system, which had '\n",
            "                      'been its mainstay since 1980.',\n",
            "        '1994/06/28': 'In the same year, members of the Aum Shinrikyo cult '\n",
            "                      'execute the first sarin gas attack at Matsumoto, Japan, '\n",
            "                      'killing 7 and injuring 660.',\n",
            "        '1994/06/30': 'In the same year, an Airbus A330 crashes during a test '\n",
            "                      'flight near Toulouse, France, where Airbus is based, '\n",
            "                      'killing the seven-person crew.',\n",
            "        '1994/07/02': 'In the same year, colombian footballer Andrés Escobar, '\n",
            "                      '27, is shot dead in Medellín.',\n",
            "        '1994/07/06': 'In the same year, fourteen firefighters die in the '\n",
            "                      'South Canyon wildfire on Storm King Mountain in '\n",
            "                      'Colorado.',\n",
            "        '1994/07/07': 'In the same year,  civil war in Yemen: Aden is occupied '\n",
            "                      'by troops from North Yemen.',\n",
            "        '1994/07/15': 'In the same year, july 21 ampndash The planet Jupiter '\n",
            "                      'is hit by 21 large fragments of Comet Shoemaker-Levy 9 '\n",
            "                      'over the course of 6 days.',\n",
            "        '1994/07/17': 'In the same year, brazil wins the  FIFA World Cup, '\n",
            "                      'defeating Italy by 3–2 in penalties (full time 0–0).',\n",
            "        '1994/07/18': 'In the same year, in Buenos Aires, a terrorist attack '\n",
            "                      'destroys a building housing several Jewish '\n",
            "                      'organizations, killing 85 and injuring many more (see '\n",
            "                      'AMIA Bombing).',\n",
            "        '1994/07/19': 'In the same year, four 26-pound ceiling tiles fall from '\n",
            "                      'the roof of the Kingdome in Seattle, Washington, just '\n",
            "                      'hours before a scheduled Seattle Mariners game.',\n",
            "        '1994/07/20': \"In the same year, comet Shoemaker-Levy 9's Fragment Q1 \"\n",
            "                      'hits Jupiter.',\n",
            "        '1994/07/25': 'In the same year, the University of London founds the '\n",
            "                      'School of Advanced Study, a group of postgraduate '\n",
            "                      'research institutes.',\n",
            "        '1994/08/05': \"In the same year, woodstock '94 begins in Saugerties, \"\n",
            "                      'New York.',\n",
            "        '1994/08/18': 'In the same year, irish mobster Martin Cahill is '\n",
            "                      'assassinated in Dublin.',\n",
            "        '1994/08/20': 'In the same year, in Honolulu, Hawaii, during a circus '\n",
            "                      'international performance, an elephant named Tyke '\n",
            "                      'crushes her trainer Allen Campbell to death before '\n",
            "                      'hundreds of horrified spectators, at the Neal Blaisdell '\n",
            "                      'Arena.',\n",
            "        '1994/08/23': 'In the same year, the Russian army leaves Estonia.',\n",
            "        '1994/09/03': \"In the same year, cold War: Russia and the People's \"\n",
            "                      'Republic of China agree to de-target their nuclear '\n",
            "                      'weapons against each other.',\n",
            "        '1994/09/04': 'In the same year, kansai International Airport in '\n",
            "                      'Osaka, Japan opens.',\n",
            "        '1994/09/05': 'In the same year, new South Wales State MP for '\n",
            "                      'Cabramatta John Newman is shot outside his home, in '\n",
            "                      \"Australia's first political assassination since 1977.\",\n",
            "        '1994/09/08': 'In the same year, uSAir Flight 427, a Boeing 737 with '\n",
            "                      '132 people on board, crashes on approach to Pittsburgh '\n",
            "                      'International Airport there are no survivors.',\n",
            "        '1994/09/10': \"In the same year, ''Wollemia nobilis'' (the 'Wollemi \"\n",
            "                      \"Pine'), previously known only from fossils, is \"\n",
            "                      'discovered living in remote rainforest gorges in the '\n",
            "                      'Wollemi National Park of New South Wales by canyoner '\n",
            "                      \"David Noble, 150ampnbspkm from Australia's largest \"\n",
            "                      'city.',\n",
            "        '1994/09/13': 'In the same year, president Bill Clinton signs the '\n",
            "                      'Assault Weapons Ban, which bans the manufacture of new '\n",
            "                      'weapons with certain features for a period of 10 years.',\n",
            "        '1994/09/16': 'In the same year, danish tour guide Louise Jensen is '\n",
            "                      'abducted, raped and murdered by three British soldiers '\n",
            "                      'in Cyprus.',\n",
            "        '1994/09/17': 'In the same year, heather Whitestone becomes the first '\n",
            "                      'hearing impaired contestant to win the Miss America '\n",
            "                      'entitlement.',\n",
            "        '1994/09/19': 'In the same year, american troops stage a bloodless '\n",
            "                      'invasion of Haiti in order to restore the legitimate '\n",
            "                      'elected leader, Jean-Bertrand Aristide, to power.',\n",
            "        '1994/09/28': 'In the same year, palau gains independence from the '\n",
            "                      'United Nations Trusteeship Council.',\n",
            "        '1994/10/04': 'In the same year, in Switzerland, 23 members of the '\n",
            "                      'Order of the Solar Temple cult are found dead, a day '\n",
            "                      'after 25 of their fellow cultists are similarly '\n",
            "                      'discovered in Morin Heights, Quebec.',\n",
            "        '1994/10/05': \"In the same year, uNESCO inaugurates World Teachers' \"\n",
            "                      'Day to celebrate and commemorate the signing of the '\n",
            "                      'Recommendation Concerning the Status of Teachers on '\n",
            "                      'October 5, 1966.',\n",
            "        '1994/10/08': 'In the same year, iraq disarmament crisis: The '\n",
            "                      'President of the United Nations Security Council says '\n",
            "                      'that Iraq must withdraw its troops from the Kuwait '\n",
            "                      'border, and immediately cooperate with weapons '\n",
            "                      'inspectors.',\n",
            "        '1994/10/12': 'In the same year, iraq disarmament crisis: Following '\n",
            "                      'threats by the U.',\n",
            "        '1994/10/29': 'In the same year, johan Heyns, an influential Afrikaner '\n",
            "                      'theologian and critic of apartheid, is assassinated.',\n",
            "        '1994/11/06': 'In the same year, a flood in Piedmont, Italy, kills '\n",
            "                      'dozens of people.',\n",
            "        '1994/11/07': 'In the same year, wXYC, the student radio station of '\n",
            "                      'the University of North Carolina at Chapel Hill, '\n",
            "                      \"provides the world's first internet radio broadcast.\",\n",
            "        '1994/11/08': 'In the same year, michael Schumacher wins his first '\n",
            "                      'Formula One World Championship in controversial '\n",
            "                      'circumstances at the Australian Grand Prix.',\n",
            "        '1994/11/16': 'In the same year, a Federal judge issues a temporary '\n",
            "                      'restraining order, prohibiting the State of California '\n",
            "                      'from implementing Proposition 187, that would have '\n",
            "                      'denied most public services to illegal aliens.',\n",
            "        '1994/11/19': 'In the same year, malawi recognizes the Sahrawi Arab '\n",
            "                      'Democratic Republic (SADR).',\n",
            "        '1994/11/20': 'In the same year, the Angolan government and UNITA '\n",
            "                      'rebels sign the Lusaka Protocol.',\n",
            "        '1994/11/28': 'In the same year, voters in Norway decide not to join '\n",
            "                      'the European Union in a referendum.',\n",
            "        '1994/11/30': 'In the same year, the National Football League '\n",
            "                      'announces that the Jacksonville Jaguars will become the '\n",
            "                      \"league's 30th franchise.\",\n",
            "        '1994/12/01': 'In the same year, ernesto Zedillo takes office as '\n",
            "                      'President of Mexico.',\n",
            "        '1994/12/02': 'In the same year, british Home Secretary Michael Howard '\n",
            "                      'announces that Myra Hindley will serve a whole life '\n",
            "                      'tariff for the Moors Murders of the 1960s.',\n",
            "        '1994/12/15': 'In the same year, civil unions between homosexuals are '\n",
            "                      'legalized in Sweden.',\n",
            "        '1994/12/26': 'In the same year, french anti-terrorist police storm a '\n",
            "                      'hijacked jet at Marseille and kill 4 Islamist '\n",
            "                      'terrorists.',\n",
            "        '1994/12/31': 'In the same year, pyroclastic flows – clouds of '\n",
            "                      'scalding gas, pumice, and ash – rapidly descend an '\n",
            "                      'erupting Mount Merapi volcano in central Java, causing '\n",
            "                      'sixty deaths.'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test the generate_hints_years() function:"
      ],
      "metadata": {
        "id": "6vIkujY8y8Dj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Function that checks and counts how many words two sentences have in common. (To know if hint is too opbvious/similar to question)\n",
        "Args: the years-question and the comparable sentence\n",
        "Returns:  list of common words\n",
        "\"\"\"\n",
        "def discrad_obvious_hints(question, sentence):\n",
        "  common_words = []\n",
        "  words1 = question.lower().split()\n",
        "  words2 = sentence.lower().split()\n",
        "  common_words = list(set(words1) & set(words2))\n",
        "  return common_words\n",
        "\n",
        "\"\"\"\n",
        "Function that order the hint sentences after their similarity score and discrads too obvious hints. Returns the top 5 most similar ones from each type of years-hint-generation.\n",
        "Args: the dictionary with all the different years hints sentences\n",
        "Returns:  OrderedDict\n",
        "\"\"\"\n",
        "def order_dictionary(my_dict):\n",
        "  #new_dict = OrderedDict()\n",
        "  question = ''\n",
        "  #new_dict = {}\n",
        "  keys_to_delete = []\n",
        "  ret = {}\n",
        "\n",
        "  for year, value in my_dict.items():\n",
        "    new_dict = {}\n",
        "    for typ, categories in value.items():\n",
        "      copy_inner_ordered_dict = OrderedDict()\n",
        "      question = value['question']\n",
        "      #print(question)\n",
        "      if typ == \"question\":\n",
        "        sorted_inner_ordered_dict = categories\n",
        "      else:\n",
        "        if typ == \"thumbcaption\":\n",
        "          sorted_inner_ordered_dict = categories\n",
        "        else: #for sports and vizgr\n",
        "          for a,b in categories.items():\n",
        "            for c,d in b.items():\n",
        "              #print(question, c)\n",
        "              common_word = discrad_obvious_hints(question, c)\n",
        "              if len(common_word) >= 4:\n",
        "                key = [typ, a]\n",
        "                keys_to_delete.append(key)\n",
        "              else:\n",
        "                if not c:\n",
        "                  continue\n",
        "                else:\n",
        "                  copy_inner_ordered_dict[a] = b\n",
        "          sorted_inner_ordered_dict = dict(sorted(copy_inner_ordered_dict.items(), key=lambda item: list(item[1].values())[0], reverse=True)[:5])\n",
        "      new_dict[typ] = sorted_inner_ordered_dict\n",
        "    ret[year] = new_dict\n",
        "  return ret\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Calls the three different function types of generating years hints, combines them into a single dict and calculate the similarity score between question and hint to rank them.\n",
        "Returns:  dict: A dictionary with the date as the key and the description up until the first dot as the value.\n",
        "Test for utility score of new questions; calculate score via BERT for each question,hint pair and write the score together with the question into the sim_scores dictionary.\n",
        "\"\"\"\n",
        "def generate_hints_years():\n",
        "\n",
        "  pop_year_hints = get_year_sports_hints()\n",
        "  pop_thumb_hints = get_year_thumbcaption_hints()\n",
        "  pop_vizgr_hints = get_year_vizgr_hints()\n",
        "  #pprint.pprint(pop_vizgr_hints)\n",
        "  years_hints = {}\n",
        "\n",
        "  for y in pop_year_hints:\n",
        "    year_dict = {\n",
        "        'sports': pop_year_hints[y],\n",
        "        'thumbcaption': pop_thumb_hints[y],\n",
        "        'vizgr' : pop_vizgr_hints[y]\n",
        "    }\n",
        "    years_hints[y] = year_dict\n",
        "  generated_hints_for_years = years_hints\n",
        "\n",
        "  #pprint.pprint(generated_hints_for_years)\n",
        "\n",
        "  qa_dict = dict(zip(year_df['Answer'], year_df['Question']))\n",
        "  sim_scores = years_hints\n",
        "  for y, q in qa_dict.items():\n",
        "    # for year, data in years_hints.items():\n",
        "    for year, data in generated_hints_for_years.items():\n",
        "      if y == year:\n",
        "        for category, subdata in data.items():\n",
        "          #sim_scores[year]['question'] = q\n",
        "          if category == 'sports':\n",
        "            for key, value in subdata.items():\n",
        "              sim_scores[year][category][key] = {}\n",
        "              similarity_score = get_similarity_score(q,value)\n",
        "              sim_scores[year][category][key][value] = similarity_score\n",
        "          elif category == 'thumbcaption':\n",
        "            for i in subdata:\n",
        "              sim_scores[year][category] = {}\n",
        "              similarity_score = get_similarity_score(q,i)\n",
        "              sim_scores[year][category][i] = similarity_score\n",
        "          elif category == 'vizgr':\n",
        "            for key, value in subdata.items():\n",
        "              sim_scores[year][category][key] = {}\n",
        "              similarity_score = get_similarity_score(q,value)\n",
        "              sim_scores[year][category][key][value] = similarity_score\n",
        "      else:\n",
        "        continue\n",
        "  for y, q in qa_dict.items():\n",
        "    # for year, data in years_hints.items():\n",
        "    #   if y == year:\n",
        "    #     sim_scores[year]['question'] = q\n",
        "    for year, data in generated_hints_for_years.items():\n",
        "      if y == year:\n",
        "        sim_scores[year]['question'] = q\n",
        "\n",
        "  #pprint.pprint(sim_scores, sort_dicts=False)\n",
        "  ordered_dict = order_dictionary(sim_scores)\n",
        "  #pprint.pprint(ordered_dict, sort_dicts=False)\n",
        "\n",
        "  return ordered_dict"
      ],
      "metadata": {
        "id": "-Pcx5uKity9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "years_hints = generate_hints_years()\n",
        "pprint.pprint(years_hints, indent=1,sort_dicts=False)\n",
        "\n",
        "#add that the year from the question cant appear in the hint sentence itself (too obvious)\n",
        "\n",
        "# ordered_dict = order_dictionary(years_hints)\n",
        "# pprint.pprint(ordered_dict, sort_dicts=False)"
      ],
      "metadata": {
        "id": "2MEy2MqT8TAc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d1c78c8-2f9f-44ba-c471-10d91e51677e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1994: {'sports': {'p_f1': {'In the previous year, Alain Prost has won the F1 Drivers World Championship.': 0.7816421985626221},\n",
            "                   'worlds': {'In the same year, Brazil has won the FIFA World Cup.': 0.6666070222854614},\n",
            "                   'cl': {'In the same year, AC Milan has won the UEFA Champions League.': 0.6647369861602783},\n",
            "                   'f_cl': {'In the following year, Ajax has won the UEFA Champions League.': 0.6391717791557312},\n",
            "                   'p_cl': {'In the previous year, Marseille has won the UEFA Champions League.': 0.6317457556724548}},\n",
            "        'thumbcaption': {'In the same year, the Kaiser Permanente building after the  Northridge earthquake': 0.5455725193023682},\n",
            "        'vizgr': {'1994/05/01': {'In the same year, three-time Formula One world champion Ayrton Senna is killed in an accident during the San Marino Grand Prix in Imola, Italy.': 0.7630124092102051},\n",
            "                  '1994/04/30': {'In the same year, formula One driver Roland Ratzenberger is killed while qualifying for the  San Marino Grand Prix.': 0.7324753999710083},\n",
            "                  '1994/04/06': {'In the same year, kurt Cobain, songwriter and frontman for the band Nirvana, is found dead at his Lake Washington home, apparently of a single self-inflicted gunshot wound.': 0.6706864833831787},\n",
            "                  '1994/09/05': {\"In the same year, new South Wales State MP for Cabramatta John Newman is shot outside his home, in Australia's first political assassination since 1977.\": 0.6559743881225586},\n",
            "                  '1994/07/19': {'In the same year, four 26-pound ceiling tiles fall from the roof of the Kingdome in Seattle, Washington, just hours before a scheduled Seattle Mariners game.': 0.6421981453895569}},\n",
            "        'question': 'When did Michael Schumacher win his first F1 World '\n",
            "                    'Drivers Title?'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **New prediction Methods for people:**"
      ],
      "metadata": {
        "id": "98m65vq_tyWL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We created a metric to find unexpected pieces of information of our searched-entity. This was done by searching for similar people to our searched-entity (same occupation), ranking the wiki-categories for all of the entities, comparing them and choosing the most well-known category, where only our searched-entity out of the similar-people-pool appers in. We can consider this an unexpected/unusual/surprising piece of information, because similar people are not connected to this category. This can be used as a hint, because often unusual information is more rememberable than obvious information.\n",
        "\n",
        "\n",
        "---\n",
        "Short explenation of the functions:\n",
        "\n",
        "*   Unexpected **categories** part: search for unusual pieces of information with the wiki-categories; rank them after the pageviews and compare them to similar people (same occupation) and choose the most unexpected one (the most popular one, where only the searched entity out of a list of similar entities appears in)\n",
        "*   Unexpected **predicates** part: create hint-sentences with properties of the searched entity; pre-rank them after a fixed ranking (created by hand) and a NLP-Bernt similarity score between the question and the hint-sentence\n"
      ],
      "metadata": {
        "id": "uRT1_8HpefRE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reusable functions to retrieve, compare and modify the needed data"
      ],
      "metadata": {
        "id": "EtAXhTJq6NK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Given a list of Wikipedia category names, creates the corresponding Wikipedia category links.\n",
        "Args: categories (list): A list of Wikipedia category names to create links for.\n",
        "Returns: category_links (list): A list of Wikipedia category links corresponding to the input categories.\n",
        "\"\"\"\n",
        "def get_category_links(categories):\n",
        "  category_links = []\n",
        "  for category in categories:\n",
        "    category_link = \"https://en.wikipedia.org/wiki/Category:\" + category.replace(\" \", \"_\")\n",
        "    #category_link = \"https://wikipedia.org/wiki/Category:\" + category.replace(\" \", \"_\")\n",
        "    category_links.append(category_link)\n",
        "  return category_links\n",
        "\n",
        "\"\"\"\n",
        "Given a list of Wikipedia category names, creates the corresponding Wikipedia category links.\n",
        "Args: categories (list): A list of Wikipedia category names to create links for.\n",
        "Returns: category_links (list): A list of Wikipedia category links corresponding to the input categories.\n",
        "\"\"\"\n",
        "def get_category_with_underscores(categories):\n",
        "\n",
        "  category_links = []\n",
        "  for category in categories:\n",
        "    category_link = category.replace(\" \", \"_\")\n",
        "    category_links.append(category_link)\n",
        "  return category_links\n",
        "\n",
        "\"\"\"\n",
        "Given a list of Wikipedia category names, retrieves the number of entries in each category and returns a dictionary\n",
        "with the category names as keys and the entry counts as values.\n",
        "Args: categories (list): A list of Wikipedia category names to retrieve entry counts for.\n",
        "Returns: category_entry_counts (dict): A dictionary of Wikipedia category names and their respective entry counts.\n",
        "\"\"\"\n",
        "def get_category_entry_counts(categories):\n",
        "  category_links = get_category_links(categories)\n",
        "  category_entry_counts = {}\n",
        "\n",
        "  for i, category_link in enumerate(category_links):\n",
        "    response = requests.get(category_link)\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "    try:\n",
        "      entry_count_text = soup.find(\"div\", {\"id\": \"catlinks\"}).find_all(\"a\")[1].text.strip()\n",
        "      entry_count = int(entry_count_text.split()[-2])\n",
        "      category_name = categories[i]\n",
        "      if category_name:\n",
        "        category_entry_counts[category_name] = entry_count\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "  return category_entry_counts\n",
        "\n",
        "# SPLIT OF 1. CELL\n"
      ],
      "metadata": {
        "id": "8Iqf7G5usErf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rewrite with wikiapi\n",
        "\"\"\"\n",
        "Retrieves the categories of a Wikipedia page using the wikipediaapi package.\n",
        "Args: title (str): The title of the Wikipedia page.\n",
        "Returns: categories (list): A list of categories associated with the page.\n",
        "\"\"\"\n",
        "def get_wikipedia_categories(title):\n",
        "  wikipedia = wikipediaapi.Wikipedia(\"en\")\n",
        "  page = wikipedia.page(title)\n",
        "  #print(page)\n",
        "  if not page.exists():\n",
        "    return []\n",
        "  categories = [c for c in page.categories]\n",
        "  return [cat.split(\":\")[1] for cat in categories]\n",
        "\n",
        "# SPLIT OF 2. CELL\n",
        "\n",
        "def get_category_subcategories(link):\n",
        "  #print(link)\n",
        "  # Create a Wikipedia API object\n",
        "  wiki_api = wikipediaapi.Wikipedia('en')\n",
        "  # Extract the category name from the link\n",
        "  category_name = link.split('/')[-1]\n",
        "  # Retrieve the category page\n",
        "  category_page = wiki_api.page(f\"Category:{category_name}\")\n",
        "  # Find the subcategories section of the page\n",
        "  subcategories_section = category_page.categorymembers\n",
        "  # Extract the subcategories from the section\n",
        "  subcategories = []\n",
        "  i=0\n",
        "  for subcategory in subcategories_section.values():\n",
        "    if i < 100: #threshold for how many entries per category\n",
        "      i +=1\n",
        "      if subcategory.ns == wikipediaapi.Namespace.CATEGORY:\n",
        "        subcategories.append(subcategory.fullurl)\n",
        "  return subcategories\n",
        "\n",
        "\n",
        "def get_category_pages(category_title, limit=100):\n",
        "  params = {\n",
        "    \"action\": \"query\",\n",
        "    \"format\": \"json\",\n",
        "    \"list\": \"categorymembers\",\n",
        "    \"cmtitle\": category_title,\n",
        "    \"cmlimit\": str(limit) # limit to 100 entries\n",
        "  }\n",
        "  #print(\"OK\")\n",
        "  pages = []\n",
        "  while True:\n",
        "    response = requests.get(\"https://en.wikipedia.org/w/api.php\", params=params).json()\n",
        "    pages += [p[\"title\"] for p in response[\"query\"][\"categorymembers\"]]\n",
        "    if \"continue\" in response:\n",
        "      params.update(response[\"continue\"])\n",
        "    else:\n",
        "      break\n",
        "    if len(pages) >= limit: # break the loop if the number of entries exceeds 100\n",
        "      break\n",
        "  return pages[:limit] # return only the first 100 entries\n",
        "\n",
        "# SPLIT OF 3. CELL\n"
      ],
      "metadata": {
        "id": "2QVAzJAoxFwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"\n",
        "Given a list of Wikipedia category names, retrieves the number of entries in each category and returns a dictionary\n",
        "with the category names as keys and the entry counts as values.\n",
        "Args: searched_location: The desired location for wich a hint should be created. (The answer to the question)\n",
        "Returns: dicto (dict): A dictionary of all categories with the subcategories and how many entries there are in each.\n",
        "\"\"\"\n",
        "def get_cat_with_all_subcats(searched_location):\n",
        "  #retrieves the list of categories from the location-wikipedia page\n",
        "  #print(\"1 \\n\")\n",
        "  try:\n",
        "    categories = get_wikipedia_categories(searched_location)\n",
        "    categories_with_underscore = get_category_with_underscores(categories)\n",
        "    categories_links = get_category_links(categories)\n",
        "  except Exception as e:\n",
        "      # Exception handling code\n",
        "      pass\n",
        "      #print(f\"An exception occurred: {e}\")\n",
        "\n",
        "  categories_with_links_dict = {}\n",
        "  for i in range(len(categories_with_underscore)):\n",
        "    key = categories_with_underscore[i]\n",
        "    link = categories_links[i]\n",
        "    categories_with_links_dict[key] = link\n",
        "  #print(\"4 \\n\")\n",
        "\n",
        "  cat_with_subcats_dict = {}\n",
        "  for category in categories_links:\n",
        "    try:\n",
        "      sub_cats = get_category_subcategories(category)\n",
        "      ct = get_category_title(category)\n",
        "      pages_list = get_category_pages(ct)\n",
        "    except Exception as e:\n",
        "      # Exception handling code\n",
        "      pass\n",
        "      #print(f\"An exception occurred: {e} in get_cat_with_all_subcats\")\n",
        "\n",
        "    filtered_list = [str(entry) for entry in pages_list if not entry.startswith(\"Category:\")]\n",
        "    new_list = [len(filtered_list), filtered_list]\n",
        "\n",
        "    if sub_cats is None:\n",
        "      continue\n",
        "    else:\n",
        "      cat_with_subcats_dict[category] = [[len(sub_cats), sub_cats], new_list]\n",
        "\n",
        "  return cat_with_subcats_dict\n",
        "\n",
        "#input a url of a category, this returns the tilte\n",
        "def get_category_title(category_url):\n",
        "  parts = category_url.split('/')\n",
        "  title = [part for part in parts if part.startswith('Category:')]\n",
        "  if title:\n",
        "    return title[0]\n",
        "  else:\n",
        "    return None\n",
        "\n",
        "#input searched location and returns a dict with number of pages and number of subcategories\n",
        "def get_categories_ranking(searched_location):\n",
        "  categories = get_wikipedia_categories(searched_location)\n",
        "  categories_links = []\n",
        "  cat_without_articles = []\n",
        "\n",
        "  bad_list = ['Articles with', 'CS1', 'Wikipedia', 'Webarchive', 'Short', 'Biography', 'Commons', 'Pages', 'Use', 'All', 'Articles', 'Coordinates', 'Engvar', 'Lang', 'Official']\n",
        "\n",
        "  for c in categories:\n",
        "    if not any(c.startswith(word) for word in bad_list):\n",
        "      cat_without_articles.append(c)\n",
        "  try:\n",
        "    categories_links = get_category_links(cat_without_articles)\n",
        "  except Exception as e:\n",
        "      # Exception handling code\n",
        "      pass\n",
        "      #print(f\"An exception occurred: {e} in get_categories_ranking\")\n",
        "  cat_with_amount = {}\n",
        "\n",
        "  for category in categories_links:\n",
        "    cat_with_amount[category] =  (0, 0)\n",
        "    # try:\n",
        "    #   sub_cats = get_category_subcategories(category)\n",
        "    #   ct = get_category_title(category)\n",
        "    #   pages_list = get_category_pages(ct)\n",
        "    # except Exception as e:\n",
        "    #   # Exception handling code\n",
        "    #   print(f\"An exception occurred: {e}\")\n",
        "\n",
        "    # if sub_cats is None:\n",
        "    #   continue\n",
        "    # else:\n",
        "    #   cat_with_amount[category] =  len(pages_list), len(sub_cats)\n",
        "\n",
        "  sorted_dict = dict(sorted(cat_with_amount.items(), key=lambda x: x[1], reverse=True))\n",
        "  return sorted_dict\n",
        "\n",
        "#NEW\n",
        "#extract the category part from the wiki links\n",
        "def extract_last_parts(links):\n",
        "  last_parts = []\n",
        "  for link in links:\n",
        "    last_part = link.split('/')[-1]\n",
        "    last_parts.append(last_part)\n",
        "  return last_parts\n",
        "\n",
        "#function to concatenate the category links to insert into a pageviews url\n",
        "def concatenate_elements(elements, n=10):\n",
        "  result = []\n",
        "  for i in range(0, len(elements), n):\n",
        "    group = elements[i:i+n]\n",
        "    result.append('|'.join(group))\n",
        "  return result\n",
        "\n",
        "\"\"\"\n",
        "This function first initializes an empty list url_list that will hold the modified URLs. Then, it uses a for loop to iterate through each combined string in the input list.\n",
        "For each combined string, it concatenates the base URL with a forward slash and the combined string, and appends the resulting URL to the url_list. Finally, the function returns the url_list at the end.\n",
        "\"\"\"\n",
        "def combine_pv_urls(base_url, combined_strings):\n",
        "    url_list = []\n",
        "    for strin in combined_strings:\n",
        "      url_list.append(base_url  + strin)\n",
        "    return url_list\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "This function takes a list of links, opens each link to get the data, discards the header and converts the data into a dictionary using the list_to_dict() function,\n",
        "and then updates a combined dictionary with the resulting dictionary from each link. Finally, it returns the combined dictionary.\n",
        "\"\"\"\n",
        "def combine_dicts_from_links(link_list):\n",
        "  combined_dict = {}\n",
        "  #pprint.pprint(link_list)\n",
        "  for link in link_list:\n",
        "    try:\n",
        "      header, data = get_table_info(link)\n",
        "      link_dict = list_to_dict(data)\n",
        "      combined_dict.update(link_dict)\n",
        "    except Exception as e:\n",
        "      # Exception handling code\n",
        "      pass\n",
        "      #print(f\"An exception occurred: {e} in combine_dicts_from_links\")\n",
        "  return combined_dict\n",
        "\n",
        "# SPLIT OF 4. CELL\n"
      ],
      "metadata": {
        "id": "Cc8wowZdxOm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_values_to_links(links_dict, values_dict):\n",
        "  new_dict = {}\n",
        "  for link, value in links_dict.items():\n",
        "    key = link.split(':')[-1]\n",
        "    if key in values_dict:\n",
        "      value += (values_dict[key],)\n",
        "    new_dict[link] = value\n",
        "  return new_dict\n",
        "\n",
        "def combine_catnumbers_pvs(ord_dict, norm_dict):\n",
        "  for key in ord_dict:\n",
        "    if key in norm_dict:\n",
        "      ord_dict[key] = ord_dict[key] + (norm_dict[key],)\n",
        "  return ord_dict\n",
        "\n",
        "def add_values_to_linkss(links_dict, values_dict):\n",
        "  new_dict = {}\n",
        "  for loc, ord_list in links_dict.items():\n",
        "    for loc2, ord_list_pv in values_dict.items():\n",
        "      if loc == loc2:\n",
        "        new_dict[loc] = combine_catnumbers_pvs(ord_list, ord_list_pv)\n",
        "  return new_dict\n",
        "\n",
        "#combines the pageviews of the categories of the location together with the sub-categories and pages of those subcategories\n",
        "def combine_pv_cats(cat_dict, pv_dict):\n",
        "  tmp = cat_dict\n",
        "\n",
        "  for key, value in cat_dict.items():\n",
        "    for key2, value2 in pv_dict.items():\n",
        "      category_name = key.split('/')[-1]\n",
        "      new_string = key2.replace(' ', '_')\n",
        "\n",
        "      if category_name == new_string:\n",
        "        org_tup = cat_dict[key]\n",
        "        new_tup = org_tup + (value2,0)\n",
        "        tmp[key] = new_tup\n",
        "\n",
        "        if len(new_tup) < 3:\n",
        "          #print(\"PROBLEM - combine_pv_cats\\n\")\n",
        "          continue\n",
        "  return tmp\n",
        "\n",
        "#combines the pageviews of the categories of the location together with the sub-categories and pages of those subcategories\n",
        "def get_dict_for_every_location(cat_ranking, cat_with_pv):\n",
        "  ret=cat_ranking\n",
        "\n",
        "  for location, value in cat_ranking.items():\n",
        "    for location1, value1 in cat_with_pv.items():\n",
        "      if location == location1:\n",
        "        #tmp = combine_pv_cats(value, value1)\n",
        "        #print(value, value1)\n",
        "        tmp = value1\n",
        "        ret[location] = tmp\n",
        "  return ret"
      ],
      "metadata": {
        "id": "WzIh8TdHxUC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipediaapi\n",
        "#import pageviewapi\n",
        "\n",
        "def get_categories(subject_dict):\n",
        "  categories_for_subject_dict= {}\n",
        "  rankings_for_categories_dict = {}\n",
        "  #creates a dict with all the dicts of each location with its categories and sub-categories\n",
        "  for subject, question in subject_dict.items():\n",
        "    try:\n",
        "      #dicto = get_cat_with_all_subcats(subject)\n",
        "      #categories_for_subject_dict[subject] = dicto\n",
        "      ranking = get_categories_ranking(subject)\n",
        "      ordict = OrderedDict(ranking)\n",
        "      rankings_for_categories_dict[subject] = ordict\n",
        "    except Exception as e:\n",
        "      #print(f\"An exception occurred in get_categories: {e}\")\n",
        "      pass\n",
        "\n",
        "  return rankings_for_categories_dict\n",
        "\n",
        "#given a dictionary with all the categories, the function returns the categories in a OrderedDict with the corresponding pageviews for each category\n",
        "def get_pageviews_for_categories(cat_dict):\n",
        "  pageviews_range_url = 'https://pageviews.wmcloud.org/?project=en.wikipedia.org&platform=all-access&agent=user&redirects=0&range=last-year&pages='\n",
        "\n",
        "  all_cats_with_pvs = {}\n",
        "  #if len(cat_dict) == 0:\n",
        "    #print(\"empty in get_pageviews_for_categories\")\n",
        "  for subject in cat_dict:\n",
        "    ord_dict = OrderedDict()\n",
        "    ordered_dict_sub =  cat_dict[subject]\n",
        "    links_list = [link for link in ordered_dict_sub.keys()]\n",
        "    pruned_link_parts_list = extract_last_parts(links_list)\n",
        "    concat_str_for_links = concatenate_elements(pruned_link_parts_list) #combine up to 10 of these links to create a request to pageview\n",
        "    pageviews_url_list = combine_pv_urls(pageviews_range_url, concat_str_for_links) #now we have a list of pageview links with all the backlinks of the thumbcaption part of the wiki page (REUSED FROM YEARS PART)\n",
        "    categories_with_pageviews =combine_dicts_from_links(pageviews_url_list) #now we called the links and retreieved the pageviews; saved them as a dictionary\n",
        "    ordered_categories_with_pageviews = sort_dict_desc(categories_with_pageviews) #now the list is ordered in ascending order\n",
        "    all_cats_with_pvs[subject] = OrderedDict(categories_with_pageviews)\n",
        "  return all_cats_with_pvs\n",
        "\n",
        "def sorting_dict(sor_dict):\n",
        "  ret = {}\n",
        "  l2 = sor_dict\n",
        "  for loc, value in l2.items():\n",
        "    if len(sor_dict) == 0:\n",
        "      #print(\"empty in sorting_dict\")\n",
        "      continue\n",
        "    try:\n",
        "      sorted_dict = OrderedDict(sorted(value.items(), key=lambda x: x[1][2], reverse=True))\n",
        "    except Exception as e:\n",
        "      #print(f\"Sorting failed for person {loc}: {e} in sorting_dict\")\n",
        "      #pprint.pprint(value)\n",
        "      sorted_dict = value\n",
        "    ret[loc] = sorted_dict\n",
        "    #sorted_dict = dict(sorted(value.items(), key=lambda x: x[2], reverse=True))\n",
        "  return ret\n",
        "\n",
        "from collections import OrderedDict\n",
        "# function that takes a dictionary with an ordered dictionary as the value and prunes the ordered dictionary to keep only the first n entries:\n",
        "def prune_ordered_dict(dictionary, n):\n",
        "  pruned_dict = OrderedDict()\n",
        "  for key, value in dictionary.items():\n",
        "    pruned_dict[key] = OrderedDict(list(value.items())[:n])\n",
        "  return pruned_dict\n",
        "\n",
        "# # function that takes a dictionary with an ordered dictionary as the value and prunes the ordered dictionary to keep only the first n entries and deltes certain categories:\n",
        "# def prune_and_ordered_dict(dictionary, n):\n",
        "#   pruned_dict = OrderedDict()\n",
        "#   inter1_dict= OrderedDict()\n",
        "#   for key, value in dictionary.items():\n",
        "#     inter3_dict= OrderedDict()\n",
        "#     for link, tuplee in value.items():\n",
        "#       link_str = str(link)\n",
        "#       if 'Living_people' not in link_str and 'Living people' not in link_str and '_births' not in link_str and 'births' not in link_str and '_deaths' not in link_str and 'Good_articles' not in link_str and 'Members' not in link_str and '20th' not in link_str and '21st' not in link_str:\n",
        "#         inter3_dict[link] = tuplee\n",
        "#     pruned_dict[key] = inter3_dict\n",
        "\n",
        "#   for key, value in pruned_dict.items():\n",
        "#     inter1_dict[key] = OrderedDict(list(value.items())[:n])\n",
        "#   return inter1_dict\n",
        "\n",
        "# function that takes a dictionary with an ordered dictionary as the value and prunes the ordered dictionary to keep only the first n entries and deltes certain categories:\n",
        "def prune_and_ordered_dict(dictionary, n):\n",
        "  pruned_dict = OrderedDict()\n",
        "  inter1_dict= OrderedDict()\n",
        "  bad_categories_list = ['Living_people', 'Living people', '_births', 'births', '_deaths', 'deaths', 'Good_articles', 'Good articles', 'Members','19th', '20th', '21st']\n",
        "  for key, value in dictionary.items():\n",
        "    inter3_dict= OrderedDict()\n",
        "    for link, tuplee in value.items():\n",
        "      link_str = str(link)\n",
        "      contains_bad_word = False\n",
        "      for word in bad_categories_list:\n",
        "        if word in link_str:\n",
        "          contains_bad_word = True\n",
        "          continue\n",
        "      if contains_bad_word == False:\n",
        "        inter3_dict[link] = tuplee\n",
        "    pruned_dict[key] = inter3_dict\n",
        "\n",
        "  for key, value in pruned_dict.items():\n",
        "    inter1_dict[key] = OrderedDict(list(value.items())[:n])\n",
        "  return inter1_dict\n",
        "\n",
        "\n",
        "#find the 20 most appearing categories\n",
        "def find_most_common_links(data_dict):\n",
        "  link_count = {}\n",
        "  for key, value in data_dict.items():\n",
        "    for link in value:\n",
        "      if link not in link_count:\n",
        "        link_count[link] = {\"count\": 1, \"keys\": [key]}\n",
        "      else:\n",
        "        link_count[link][\"count\"] += 1\n",
        "        link_count[link][\"keys\"].append(key)\n",
        "  # Sort the links by their count in descending order\n",
        "  sorted_links = sorted(link_count.items(), key=lambda x: x[1][\"count\"], reverse=True)\n",
        "  # Return a list of tuples with the link, count, and keys\n",
        "  return [(link, data[\"count\"], data[\"keys\"]) for link, data in sorted_links[:20]]"
      ],
      "metadata": {
        "id": "XfUS-hxe3mPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template_sentence_location = 'The location you are looking for, is a member of the category x'\n",
        "template_sentence_location2 = 'The location you are looking for, belongs to the category '\n",
        "\n",
        "template_sentence_person = 'The person you are looking for, is a member of the category x'\n",
        "template_sentence_person2 = 'The person you are looking for, belongs to the category '\n",
        "\n",
        "import itertools\n",
        "import requests\n",
        "\n",
        "def get_categories_with_pageviews_person(person_questions_dict):\n",
        "  cat_ranking_person = get_categories(person_questions_dict)\n",
        "  cat_with_pv_person = get_pageviews_for_categories(cat_ranking_person)\n",
        "  categories_with_subs_and_pageviews_person = get_dict_for_every_location(cat_ranking_person, cat_with_pv_person)\n",
        "  new_ordered_dict_person = sorting_dict(categories_with_subs_and_pageviews_person)\n",
        "  return new_ordered_dict_person\n",
        "\n",
        "#retrives the category name from the link\n",
        "def get_category_name(link):\n",
        "  category_prefix = \"Category:\"\n",
        "  if link.startswith(\"https://wikipedia.org/wiki/\"):\n",
        "    return link[len(\"https://wikipedia.org/wiki/\" + category_prefix):].replace(\"_\", \" \")\n",
        "  else:\n",
        "    if link.startswith(\"https://en.wikipedia.org/wiki/\"):\n",
        "      return link[len(\"https://en.wikipedia.org/wiki/\" + category_prefix):].replace(\"_\", \" \")\n",
        "    else:\n",
        "      return None\n",
        "\n",
        "#just get the first 3 categories without any special formula\n",
        "def get_first_three_categs_location(copy_categories_with_subs_and_pageviews_location):\n",
        "  first_three_categories_per_location = {}\n",
        "  for key, value in copy_categories_with_subs_and_pageviews_location.items():\n",
        "    first_three = dict(itertools.islice(value.items(), 3))\n",
        "    first_three_categories_per_location[key] = first_three\n",
        "\n",
        "  hint_sentence_location = {}\n",
        "  for key, value in first_three_categories_per_location.items():\n",
        "    inter_hints = []\n",
        "    for link, tup in value.items():\n",
        "      cat_name=get_category_name(link)\n",
        "      if cat_name is not None:\n",
        "        ok = template_sentence_location2 + cat_name\n",
        "      else:\n",
        "        ok = template_sentence_location2\n",
        "      inter_hints.append(ok)\n",
        "    hint_sentence_location[key] = inter_hints\n",
        "  return hint_sentence_location\n",
        "\n",
        "#just get the first 3 categories without any special formula\n",
        "def get_first_three_categs_person(copy_categories_with_subs_and_pageviews_location):\n",
        "  first_three_categories_per_location = {}\n",
        "  for key, value in copy_categories_with_subs_and_pageviews_location.items():\n",
        "    first_three = dict(itertools.islice(value.items(), 3))\n",
        "    first_three_categories_per_location[key] = first_three\n",
        "\n",
        "  hint_sentence_location = {}\n",
        "  for key, value in first_three_categories_per_location.items():\n",
        "    inter_hints = []\n",
        "    for link, tup in value.items():\n",
        "      cat_name=get_category_name(link)\n",
        "      if cat_name is not None:\n",
        "        ok = template_sentence_person2 + cat_name\n",
        "      else:\n",
        "        ok = template_sentence_person2\n",
        "      inter_hints.append(ok)\n",
        "    hint_sentence_location[key] = inter_hints\n",
        "  return hint_sentence_location\n",
        "\n",
        "#get predicates of a wiki page via wikidataAPI\n",
        "def get_wikidata_predicates(page_title):\n",
        "    # First, get the Wikidata item ID of the page\n",
        "    url = f\"https://en.wikipedia.org/w/api.php?action=query&prop=pageprops&ppprop=wikibase_item&redirects=1&titles={page_title}&format=json\"\n",
        "    response = requests.get(url).json()\n",
        "    pages = response[\"query\"][\"pages\"]\n",
        "    if \"-1\" in pages:\n",
        "        return None\n",
        "    page_id = next(iter(pages))\n",
        "    wikidata_id = pages[page_id][\"pageprops\"][\"wikibase_item\"]\n",
        "\n",
        "    # Then, get the predicates and their values of the Wikidata item\n",
        "    url = f\"https://www.wikidata.org/w/api.php?action=wbgetentities&ids={wikidata_id}&format=json&props=claims\"\n",
        "    response = requests.get(url).json()\n",
        "    entity = response[\"entities\"][wikidata_id]\n",
        "    predicates = {}\n",
        "    for claim_id, claim in entity[\"claims\"].items():\n",
        "        predicate_id = claim[\"mainsnak\"][\"property\"]\n",
        "        predicate_value = claim[\"mainsnak\"][\"datavalue\"][\"value\"]\n",
        "        predicates[predicate_id] = predicate_value\n",
        "    return predicates\n",
        "\n",
        "#TEST\n",
        "people_list=[]\n",
        "for l in dataPerson:\n",
        "  people_list.append(l[1])\n",
        "\n",
        "# takes a list of all the people and the list of category occurences where all the categories that were assigned to these wiki-pages\n",
        "# are listed and ranked after how often they occur;\n",
        "# output: a list where the person is the key and the entries show wich categories the entity shares with wich other entitities and how many they are\n",
        "def get_people_dict(people_list, occurences_list):\n",
        "  people_dict = {}\n",
        "  for person in people_list:\n",
        "    person_categories = []\n",
        "    for occurence in occurences_list:\n",
        "      if person in occurence[2]:\n",
        "        person_categories.append((occurence[0], occurence[1], occurence[2]))\n",
        "    people_dict[person] = person_categories\n",
        "  return people_dict\n",
        "\n",
        "#returns a dict where for each person in people_list we calculate how often they occur in the same category as the keys in person_dict\n",
        "# (where already for each person we looked at each of the categories he appears and counted how many other people from people_list appear in these )\n",
        "def count_people_occurrences(person_dict, people_list):\n",
        "  # Initialize an empty dictionary to hold the counts\n",
        "  count_dict = {}\n",
        "  # Loop through all pairs of drivers and count the number of occurrences\n",
        "  for key, value in person_dict.items():\n",
        "    counter = {}\n",
        "    for person in people_list:\n",
        "      counter[person] = 0\n",
        "      for cat in value:\n",
        "        if person in cat[2]:\n",
        "          counter[person] += 1\n",
        "    count_dict[key] = counter\n",
        "  return count_dict\n",
        "\n",
        "#sorting\n",
        "def sort_dict_by_value_desc(d):\n",
        "  # Sort the inner dictionary by value in descending order\n",
        "  sorted_dict = []\n",
        "  sorted_dict = OrderedDict(sorted(d.items(), key=lambda x: x[1], reverse=True))\n",
        "  return sorted_dict\n",
        "\n",
        "def get_categories_union(overlap_dict, people_list):\n",
        "  # Initialize an empty dictionary to hold the counts\n",
        "  count_dict = {}\n",
        "  for key, value in overlap_dict.items():\n",
        "    counter = {}\n",
        "    for item, number in value.items():\n",
        "      counter[item] = number_categories_per_person[key] + number_categories_per_person[item] - number\n",
        "    count_dict[key] = counter\n",
        "  return count_dict\n",
        "\n",
        "def get_avg_pairwise_sim(cat_div_union, cat_div_overlap, number_categories_per_person, people_list):\n",
        "  count_dict = {}\n",
        "  for key, value in cat_div_union.items():\n",
        "    inter = {}\n",
        "    for key1, value1 in cat_div_overlap.items():\n",
        "      if key == key1:\n",
        "        for name, number in value.items():\n",
        "          for name1, number1 in value1.items():\n",
        "            if name == name1:\n",
        "              if not number  or not number1:\n",
        "                inter[name] = 0\n",
        "              else:\n",
        "                inter[name] = (number1 / number)\n",
        "    count_dict[key] = inter\n",
        "  return count_dict\n",
        "\n",
        "def get_cat_diversity(shared_categories, copy_new_ordered_dict_person):\n",
        "  count_dict = {}\n",
        "  for key, value in copy_new_ordered_dict_person.items():\n",
        "    for item in value.items():\n",
        "      pv = 0\n",
        "      #print(item)\n",
        "      link = item[0]\n",
        "      trip = item[1]\n",
        "      if len(trip) == 3:\n",
        "        pv = trip[2]\n",
        "      if link not in count_dict:\n",
        "        count_dict[link] = pv\n",
        "  return count_dict\n"
      ],
      "metadata": {
        "id": "JKdvN7LDxfZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#---- SPLIT ----\n",
        "\n",
        "#shared_categories; number_categories_per_person; unsorted_category_diversity; cat_div_overlap; unsorted_cat_div_union; cat_div_union; unordered_avg_pairwise_sim; avg_pairwise_sim; unordered_cat_popularity;cat_popularity\n",
        "\n",
        "def get_categories_with_ranking():\n",
        "  person_questions_dict = dict(zip(person_df['Answer'], person_df['Question']))\n",
        "  cat_with_pv_person = {}\n",
        "\n",
        "  cat_ranking_person = get_categories(person_questions_dict)\n",
        "  cat_with_pv_person = get_pageviews_for_categories(cat_ranking_person)\n",
        "  categories_with_subs_and_pageviews_person = get_dict_for_every_location(cat_ranking_person, cat_with_pv_person)\n",
        "  new_ordered_dict_person = sorting_dict(categories_with_subs_and_pageviews_person)\n",
        "\n",
        "  return new_ordered_dict_person\n",
        "\n",
        "#all_people_cat_ranked = get_categories_with_ranking() #copy_new_ordered_dict_person\n",
        "\n",
        "import wikipediaapi\n",
        "import requests\n",
        "\n",
        "#given a name, retrieve the infomration in the short-description part of the wiki page\n",
        "def get_page_short_description(page_title):\n",
        "  # Prepare the API request URL\n",
        "  url = f\"https://en.wikipedia.org/api/rest_v1/page/summary/{page_title.replace(' ', '_')}\"\n",
        "  # Send the API request\n",
        "  response = requests.get(url)\n",
        "  data = response.json()\n",
        "  # Extract the short description from the API response\n",
        "  short_description = data.get('description', '')\n",
        "  return short_description\n",
        "\n",
        "def find_most_similar_category(persons, specific_category, num_similar_categories=3):\n",
        "  most_similar_categories = {}\n",
        "\n",
        "  for person, categories in persons.items():\n",
        "    category_texts = [category_link.split('/')[-1].replace('_', ' ') for category_link in categories.keys()]\n",
        "    category_texts.append(specific_category[person])  # Add the specific category for comparison\n",
        "\n",
        "    # Vectorize the category texts\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(category_texts)\n",
        "    #print(category_texts)\n",
        "    # Calculate cosine similarity between the specific category and other categories\n",
        "    similarities = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1])[0]\n",
        "      # Calculate cosine similarity between the specific category and other categories\n",
        "    similarities = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1])[0]\n",
        "    # Find the most similar categories textually\n",
        "    similar_indices = similarities.argsort()[-num_similar_categories:][::-1]\n",
        "    similar_categories = [category_texts[index] for index in similar_indices]\n",
        "    most_similar_categories[person] = similar_categories\n",
        "  return most_similar_categories\n",
        "\n",
        "\n",
        "def get_work_category(similar_categories):\n",
        "  person_with_work_categories = {}\n",
        "  person_with_work_category = {}\n",
        "  strings_to_check = [\"births\", \"deaths\"]\n",
        "  for person, categories in similar_categories.items():\n",
        "    for strs in strings_to_check:\n",
        "      for cats in categories:\n",
        "        if strs in cats:\n",
        "          categories.remove(cats)\n",
        "        else:\n",
        "          continue\n",
        "      person_with_work_categories[person] = categories\n",
        "  for person, categories in person_with_work_categories.items():\n",
        "    person_with_work_category[person] = categories[0]\n",
        "  return person_with_work_category\n",
        "\n",
        "\n",
        "def get_container_categories(work_cats):\n",
        "  for person, categories in work_cats.items():\n",
        "    wiki_category = categories\n",
        "    base_url = 'https://en.wikipedia.org/w/api.php'\n",
        "    params = {\n",
        "        'action': 'query',\n",
        "        'titles': wiki_category,\n",
        "        'prop': 'categories',\n",
        "        'format': 'json',\n",
        "        'cllimit': 'max'\n",
        "    }\n",
        "    response = requests.get(base_url, params=params)\n",
        "    data = response.json()\n",
        "    page_id = next(iter(data['query']['pages'].keys()))\n",
        "    categories = data['query']['pages'][page_id].get('categories', [])\n",
        "    container_categories = []\n",
        "    for category in categories:\n",
        "        if category['title'].startswith('Category:'):\n",
        "            container_categories.append(category['title'][9:])\n",
        "\n",
        "    strings_to_check = [\"CatAutoTOC \", \"Commons\", \"Template\"]\n",
        "    for strs in strings_to_check:\n",
        "      for cats in container_categories:\n",
        "        if strs in cats:\n",
        "          container_categories.remove(cats)\n",
        "        else:\n",
        "          continue\n",
        "    work_cats[person] = container_categories\n",
        "  return work_cats\n",
        "\n",
        "def replace_spaces_with_underscore(dictionary):\n",
        "  updated_dict = {}\n",
        "  for key, value in dictionary.items():\n",
        "    updated_dict[key] = value.replace(\" \", \"_\")\n",
        "  return updated_dict\n",
        "\n",
        "def format_category_dict(category_dict):\n",
        "  formatted_dict = {}\n",
        "  for key, value in category_dict.items():\n",
        "    if isinstance(value, list):\n",
        "      formatted_value = [f\"Category:{v.replace(' ', '_')}\" for v in value]\n",
        "    else:\n",
        "      formatted_value = f\"Category:{value.replace(' ', '_')}\"\n",
        "    formatted_dict[key] = formatted_value\n",
        "  return formatted_dict\n"
      ],
      "metadata": {
        "id": "Z93bZ3cpxkbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Functions to retrieve the occupation of a person\n",
        "\n",
        "The wikipedia-api library does not provide a direct method to retrieve the infobox of a Wikipedia page.\n",
        "However, we can use beautifulsoup4 to parse the HTML content of the Wikipedia page and extract the infobox.\n",
        "\"\"\"\n",
        "\n",
        "#function that retrieves the infobox using beautifulsoup4\n",
        "def get_infobox_from_wikipedia(page_title):\n",
        "  # Format the page title for the Wikipedia URL\n",
        "  formatted_title = page_title.replace(' ', '_')\n",
        "  # Construct the Wikipedia page URL\n",
        "  url = f\"https://en.wikipedia.org/wiki/{formatted_title}\"\n",
        "  # Send a GET request to the Wikipedia page\n",
        "  response = requests.get(url)\n",
        "  # Check if the page exists\n",
        "  if response.status_code != 200:\n",
        "    print(f\"Page '{page_title}' does not exist.\")\n",
        "    return None\n",
        "  # Create a BeautifulSoup object to parse the page content\n",
        "  soup = BeautifulSoup(response.content, 'html.parser')\n",
        "  # Find the infobox section of the page\n",
        "  infobox = soup.find(class_='infobox')\n",
        "  return infobox\n",
        "\n",
        "#function that retrieves the infobox from a Wikipedia page and specifically extracts the entries from the \"occupation\" or \"occupations\" field:\n",
        "def get_occupations_from_infobox(page_title):\n",
        "  # Retrieve the infobox from the Wikipedia page\n",
        "  infobox = get_infobox_from_wikipedia(page_title)\n",
        "  # Check if the infobox exists\n",
        "  if not infobox:\n",
        "    return []\n",
        "  # Search for the \"occupation\" or \"occupations\" field in the infobox\n",
        "  occupation_keywords = ['occupation', 'occupations', 'Occupation', 'Occupations']\n",
        "  occupations = []\n",
        "  for keyword in occupation_keywords:\n",
        "    field = infobox.find(string=keyword)\n",
        "    if field:\n",
        "      # Retrieve the occupation entries from the field\n",
        "      tmp_str = field.find_next(\"td\").find_next(\"div\").find_next(\"ul\")\n",
        "      from_table = extract_list_elements(tmp_str)\n",
        "      from_field = field.find_next(\"td\").text.strip()\n",
        "      from_field = from_field.split()\n",
        "      if len(from_table) > 0:\n",
        "        for i in from_table:\n",
        "          occupations.append(i)\n",
        "      elif len(from_field) > 0:\n",
        "        for i in from_field:\n",
        "          occupations.append(i)\n",
        "      else:\n",
        "        continue\n",
        "        #print(\"problem - get_occupations_from_infobox\")\n",
        "  return occupations\n",
        "\n",
        "#extract the elements between the <li> tags\n",
        "def extract_list_elements(html_string):\n",
        "  occupations_list = []\n",
        "  li_entries_list = []\n",
        "  html_str = str(html_string)\n",
        "  html_str = html_str.replace(\"</ul>\", \"\")\n",
        "  html_str = html_str.replace(\"<ul>\", \"\")\n",
        "  html_str = html_str.replace(\"</li>\", \"\")\n",
        "  li_entries_list = html_str.split(\"<li>\")\n",
        "  for i in li_entries_list:\n",
        "    if len(i) > 0: #check if empty\n",
        "      if len(i) < 16: #check if entry is a link\n",
        "        occupations_list.append(i)\n",
        "  return occupations_list\n",
        "\n",
        "# #searches the occupation for every entry in people list\n",
        "# def get_occupations(people_list):\n",
        "#   occupation_person_dict = {}\n",
        "#   for person in people_list:\n",
        "#     occupation_person_dict[person] = get_occupations_from_infobox(person)\n",
        "#   return occupation_person_dict\n",
        "\n",
        "#searches the occupation for every entry in people list\n",
        "def get_occupations(people_list):\n",
        "  occupation_person_dict = {}\n",
        "  identifiers = get_wikipedia_identifiers(people_list)\n",
        "  # pprint.pprint(identifiers)\n",
        "\n",
        "  properties_list = ['occupation']\n",
        "  for name, pid in identifiers.items():\n",
        "    inter = get_person_properties(pid, properties_list)\n",
        "    occupation_person_dict[name] = inter['occupation'][0]\n",
        "  #print(occupation_person_dict)\n",
        "  return occupation_person_dict"
      ],
      "metadata": {
        "id": "zlx3ad7-xoL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# person_answers_dict = dict(zip(person_df['Answer'], person_df['Question']))\n",
        "# pprint.pprint(person_answers_dict)\n",
        "\n",
        "# people_occupations = get_occupations(person_answers_dict)\n",
        "# pprint.pprint(people_occupations)\n"
      ],
      "metadata": {
        "id": "bPcVIafnMmOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#retrieves all of the relate links of a wiki page\n",
        "def get_related_links(wiki_link):\n",
        "  # Extract the page title from the Wikipedia link\n",
        "  title = wiki_link.split('/')[-1]\n",
        "  # Format the API URL to get the page content\n",
        "  api_url = f'https://en.wikipedia.org/w/api.php?action=query&titles={title}&prop=links&pllimit=max&format=json'\n",
        "  # Send a GET request to the API\n",
        "  response = requests.get(api_url)\n",
        "  if response.status_code == 200:\n",
        "    data = response.json()\n",
        "    # Extract the page ID from the API response\n",
        "    page_id = next(iter(data['query']['pages']))\n",
        "    # Check if the page exists and has links\n",
        "    if page_id != '-1' and 'links' in data['query']['pages'][page_id]:\n",
        "      # Retrieve the links from the API response\n",
        "      links = data['query']['pages'][page_id]['links']\n",
        "      # Extract the link titles and URLs\n",
        "      related_links = [{'url': f'https://en.wikipedia.org/wiki/{link[\"title\"]}', 'title': link['title']} for link in links]\n",
        "      return related_links\n",
        "  return []\n",
        "\n",
        "# function to check if the title consists of two words -> discrad a lot of entries for performance reasons\n",
        "def filter_two_word_titles(links):\n",
        "  filtered_links = []\n",
        "  url_concat = \"\"\n",
        "  for link in links:\n",
        "    url = link['url']\n",
        "    link['url'] = url.replace(' ', '_')\n",
        "    title = link['title']\n",
        "    words = title.split()\n",
        "    if len(words) == 2:\n",
        "      filtered_links.append(link)\n",
        "  return filtered_links\n",
        "\n",
        "#check if a given Wikipedia link corresponds to an entity with the \"instance of\" (P31) property set to \"human\" (Q5), indicating it represents a person\n",
        "def is_person_page(link):\n",
        "  # Extract the page title from the link\n",
        "  title = link['title']\n",
        "  # Query Wikidata API to check if the page corresponds to a person\n",
        "  wikidata_url = f\"https://www.wikidata.org/w/api.php?action=wbgetentities&format=json&sites=enwiki&titles={title}\"\n",
        "  response = requests.get(wikidata_url).json()\n",
        "  # Check if the response contains any entities\n",
        "  if 'entities' in response:\n",
        "    entities = response['entities']\n",
        "    # Check if there is an entity with P31 (instance of) set to Q5 (human)\n",
        "    for entity_id, entity in entities.items():\n",
        "      if 'claims' in entity and 'P31' in entity['claims']:\n",
        "        for claim in entity['claims']['P31']:\n",
        "          if claim['mainsnak']['datavalue']['value']['id'] == 'Q5':\n",
        "            return True\n",
        "  return False\n",
        "\n",
        "#function that calls the is_perso_page() function for every link in the pre discraded list\n",
        "def check_if_person(links):\n",
        "  person_links= []\n",
        "  for link in links:\n",
        "    if is_person_page(link):\n",
        "      person_links.append(link)\n",
        "  return person_links\n",
        "\n",
        "#function that selects 5 entries at random from a list of related people:\n",
        "def select_random_entries(related_people_list, num_entries=5):\n",
        "  random_entries = random.sample(related_people_list, num_entries)\n",
        "  return random_entries\n",
        "\n",
        "# takes a list of all the people and the list of category occurences where all the categories that were assigned to these wiki-pages\n",
        "# are listed and ranked after how often they occur;\n",
        "# output: a list where the person is the key and the entries show wich categories the entity shares with wich other entitities and how many they are\n",
        "def get_people_dict(people_list, occurences_list):\n",
        "  people_dict = {}\n",
        "  for person in people_list:\n",
        "    person_categories = []\n",
        "    for occurence in occurences_list:\n",
        "      if person in occurence[2]:\n",
        "        person_categories.append((occurence[0], occurence[1], occurence[2]))\n",
        "    people_dict[person] = person_categories\n",
        "  return people_dict\n",
        "\n",
        "#wiki_link; related_articles; filtered_links; related_people_list; select_random_people;\n",
        "\n",
        "\n",
        "def get_most_known_related_people(related_people_list):\n",
        "  pageviews_range_url = 'https://pageviews.wmcloud.org/?project=en.wikipedia.org&platform=all-access&agent=user&redirects=0&range=last-year&pages='\n",
        "  ord_dict = OrderedDict()\n",
        "  link_list = []\n",
        "  for item in related_people_list:\n",
        "    link_list.append(item['url'])\n",
        "  pruned_link_parts_list = extract_last_parts(link_list)\n",
        "  concat_str_for_links = concatenate_elements(pruned_link_parts_list) #combine up to 10 of these links to create a request to pageview\n",
        "  pageviews_url_list = combine_pv_urls(pageviews_range_url, concat_str_for_links) #now we have a list of pageview links with all the backlinks of the thumbcaption part of the wiki page (REUSED FROM YEARS PART)\n",
        "  categories_with_pageviews =combine_dicts_from_links(pageviews_url_list) #now we called the links and retreieved the pageviews; saved them as a dictionary\n",
        "  ordered_categories_with_pageviews = sort_dict_desc(categories_with_pageviews) #now the list is ordered in ascending order\n",
        "  ord_dict = ordered_categories_with_pageviews\n",
        "\n",
        "  return ord_dict"
      ],
      "metadata": {
        "id": "_JTvHYE5xuzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ordered_categories_of_most_related_people(most_related_peoples_list):\n",
        "  #tester = get_most_known_related_people()\n",
        "  ret = {}\n",
        "  for k,v in most_related_peoples_list.items():\n",
        "    related_people_ranked = dict(sorted(v.items(), key=lambda x: x[1], reverse=True))\n",
        "    top_most_popular_people = dict(itertools.islice(related_people_ranked.items(), 5))#take the top 5 most known people from the list\n",
        "\n",
        "    cat_ranking_related_person = get_categories(top_most_popular_people)\n",
        "    cat_with_pv_person = get_pageviews_for_categories(cat_ranking_related_person)\n",
        "    #categories_with_subs_and_pageviews_person = get_dict_for_every_location(cat_ranking_person, cat_with_pv_person)\n",
        "    categories_with_subs_and_pageviews_person = get_dict_for_every_location(cat_ranking_related_person, cat_with_pv_person)\n",
        "    new_ordered_dict_related_person = sorting_dict(categories_with_subs_and_pageviews_person)\n",
        "\n",
        "    copy_new_ordered_dict_person_test = prune_and_ordered_dict(new_ordered_dict_related_person, 20)\n",
        "    ordered_dict_related_person = sorting_dict(copy_new_ordered_dict_person_test)\n",
        "    ret[key] = ordered_dict_related_person\n",
        "  return ret\n",
        "\n",
        "#get the links of all answer-entities via link creation of the key\n",
        "def get_rel_pep():\n",
        "  person_questions_dict = dict(zip(person_df['Answer'], person_df['Question']))\n",
        "  wiki_link = 'https://en.wikipedia.org/wiki/'\n",
        "  ret = {}\n",
        "\n",
        "  for key,value in person_questions_dict.items():\n",
        "    modified_text = key.replace(' ', '_')\n",
        "    link = f\"{wiki_link}/{modified_text}\"\n",
        "    related_articles = get_related_links(link)\n",
        "    filtered_links = filter_two_word_titles(related_articles)\n",
        "    related_people_list = check_if_person(filtered_links)\n",
        "    ret[key] = related_people_list\n",
        "  return ret\n",
        "\n",
        "#get the related people with the categories\n",
        "def get_related_with_categories():\n",
        "  ret = {}\n",
        "  person_questions_dict = dict(zip(person_df['Answer'], person_df['Question']))\n",
        "\n",
        "  cat_ranking_person = get_categories(person_questions_dict)\n",
        "  cat_with_pv_person = get_pageviews_for_categories(cat_ranking_person)\n",
        "  categories_with_subs_and_pageviews_person = get_dict_for_every_location(cat_ranking_person, cat_with_pv_person)\n",
        "  new_ordered_dict_person = sorting_dict(categories_with_subs_and_pageviews_person)\n",
        "\n",
        "  for key,value in new_ordered_dict_person.items():\n",
        "    ret[key] = value\n",
        "\n",
        "  return ret"
      ],
      "metadata": {
        "id": "srvRakz78Jsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Converts a date string in the format '+YYYY-MM-DDT00:00:00Z' to 'DD.MM.YYYY' format,\n",
        "adds it to the list, and returns the modified list.\n",
        "Args: date_list (list): A list containing a date string in the format '+YYYY-MM-DDT00:00:00Z'.\n",
        "Returns: list: A modified list with the date string converted to 'DD.MM.YYYY' format.\n",
        "\"\"\"\n",
        "def convert_date(date_list):\n",
        "  formatted_list = []\n",
        "  for date_str in date_list:\n",
        "    # Extracting year, month, and day from the date string\n",
        "    year = date_str[1:5]\n",
        "    month = date_str[6:8]\n",
        "    day = date_str[9:11]\n",
        "    # Creating the new formatted date string\n",
        "    formatted_date = f\"{day}.{month}.{year}\"\n",
        "    # Adding the formatted date to the new list\n",
        "    formatted_list.append(formatted_date)\n",
        "\n",
        "  return formatted_list\n",
        "\n",
        "\"\"\"\n",
        "Retrieves properties of a person from Wikidata based on the person's ID.\n",
        "Args: person_id (str): The ID of the person in Wikidata.\n",
        "Returns: dict: A dictionary containing the person's properties.\n",
        "\"\"\"\n",
        "def get_person_properties(person_id, list_of_properties):\n",
        "  excluded_properties = [\"P18\", \"P109\", \"P989\", \"P948\", \"P214\"]  # Property IDs to exclude\n",
        "  url = f\"https://www.wikidata.org/w/api.php?action=wbgetclaims&entity={person_id}&format=json\"\n",
        "  response = requests.get(url)\n",
        "  data = response.json()\n",
        "  # list_of_properties = ['nickname', 'country of citizenship', 'name in native language', 'native language', 'height',\n",
        "  #                   'occupation', 'field of work', 'educated at', 'residence', 'work period', 'ethnic group',\n",
        "  #                   'notable work', 'member of', 'owner of', 'significant event', 'award received',\n",
        "  #                   'date of birth', 'place of birth', 'date of death', 'place of death', 'manner of death',\n",
        "  #                   'cause of death', 'social media followers', 'father', 'mother', 'sibling', 'spouse', 'child', 'unmarried partner', 'sport']\n",
        "\n",
        "  person_answers_dict = dict(zip(person_df['Answer'], person_df['Question']))\n",
        "  person_names = []\n",
        "  for answer, question in person_answers_dict.items():\n",
        "    person_names.append(answer)\n",
        "\n",
        "  ret = {}\n",
        "  inter = {}\n",
        "  properties = {}\n",
        "  claims = data.get('claims')\n",
        "  if claims:\n",
        "    for claim_property_id, claim_list in claims.items():\n",
        "      for claim in claim_list:\n",
        "        property_id = claim.get('mainsnak', {}).get('property')\n",
        "        if property_id in excluded_properties:\n",
        "          continue\n",
        "        property_value = claim.get('mainsnak', {}).get('datavalue', {}).get('value')\n",
        "        if property_id and property_value:\n",
        "          property_label = get_property_label(property_id)\n",
        "          if isinstance(property_value, dict) and 'id' in property_value:\n",
        "            property_value = get_entity_label(property_value['id'])\n",
        "          if property_label in properties:\n",
        "            # Append values for properties with multiple claims\n",
        "            properties[property_label].append(property_value)\n",
        "          else:\n",
        "            properties[property_label] = [property_value]\n",
        "\n",
        "  for property_label, property_values in properties.items():\n",
        "    if property_label in list_of_properties:\n",
        "      if 'date' in property_label:\n",
        "        a = convert_date([value['time'] for value in property_values])\n",
        "        inter[property_label] = a\n",
        "        #inter[property_label] = [value['time'] for value in property_values]\n",
        "      elif 'height' in property_label:\n",
        "        inter[property_label] = [value['amount'] for value in property_values]\n",
        "      elif 'social media followers' in property_label:\n",
        "        inter[property_label] = [value['amount'] for value in property_values]\n",
        "      elif 'name in native language' in property_label:\n",
        "        inter[property_label] = [value['text'] for value in property_values]\n",
        "      elif 'nickname' in property_label:\n",
        "        inter[property_label] = [value['text'] for value in property_values]\n",
        "      elif 'significant event' in property_label:\n",
        "        a = property_values\n",
        "        for event in property_values:\n",
        "          for name in person_names:\n",
        "            if name in event:\n",
        "              a.remove(event)\n",
        "        inter[property_label] = a\n",
        "      else:\n",
        "        inter[property_label] = property_values\n",
        "\n",
        "  for property_label, property_values in inter.items():\n",
        "    d = inter[property_label]\n",
        "    ret[property_label] = d[:3]\n",
        "\n",
        "  return ret\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Retrieves the human-readable label of an entity from Wikidata.\n",
        "Args: entity_id (str): The ID of the entity in Wikidata.\n",
        "Returns: str: The human-readable label of the entity.\n",
        "\"\"\"\n",
        "def get_entity_label(entity_id):\n",
        "  url = f\"https://www.wikidata.org/w/api.php?action=wbgetentities&ids={entity_id}&format=json\"\n",
        "  response = requests.get(url)\n",
        "  data = response.json()\n",
        "  label = data.get('entities', {}).get(entity_id, {}).get('labels', {}).get('en', {}).get('value')\n",
        "  return label\n",
        "\n",
        "\"\"\"\n",
        "Retrieves the human-readable label of a property from Wikidata.\n",
        "Args: property_id (str): The ID of the property in Wikidata.\n",
        "Returns: str: The human-readable label of the property.\n",
        "\"\"\"\n",
        "def get_property_label(property_id):\n",
        "  #print(property_id)\n",
        "  url = f\"https://www.wikidata.org/w/api.php?action=wbgetentities&ids={property_id}&format=json\"\n",
        "  response = requests.get(url)\n",
        "  data = response.json()\n",
        "  label = data.get('entities', {}).get(property_id, {}).get('labels', {}).get('en', {}).get('value')\n",
        "  return label\n",
        "\n",
        "\"\"\"\n",
        "Retrieves the Wikipedia identifiers for a list of person names.\n",
        "Args: person_names (list): A list of person names.\n",
        "Returns: dict: A dictionary mapping person names to their Wikipedia identifiers.\n",
        "\"\"\"\n",
        "def get_wikipedia_identifiers(person_names):\n",
        "  base_url = \"https://en.wikipedia.org/w/api.php\"\n",
        "  identifiers = {}\n",
        "\n",
        "  for person_name in person_names:\n",
        "    #print(person_name)\n",
        "    params = {\n",
        "        \"action\": \"query\",\n",
        "        \"format\": \"json\",\n",
        "        \"titles\": person_name,\n",
        "        \"prop\": \"pageprops\",\n",
        "        \"ppprop\": \"wikibase_item\"\n",
        "    }\n",
        "    response = requests.get(base_url, params=params)\n",
        "    data = response.json()\n",
        "    pages = data.get(\"query\", {}).get(\"pages\")\n",
        "    if pages:\n",
        "      for page in pages.values():\n",
        "        if \"pageprops\" in page:\n",
        "          wikipedia_id = page[\"pageprops\"].get(\"wikibase_item\")\n",
        "          if wikipedia_id:\n",
        "            identifiers[person_name] = wikipedia_id\n",
        "          else:\n",
        "            identifiers[person_name] = None\n",
        "\n",
        "  return identifiers\n"
      ],
      "metadata": {
        "id": "ZmuipQJduqF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Print and Test the functions!"
      ],
      "metadata": {
        "id": "-q_xTwdBHXTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #for locations\n",
        "\n",
        "# location_questions_dict = dict(zip(location_df['Answer'], location_df['Question']))\n",
        "# cat_with_pv_location = {}\n",
        "\n",
        "# #for locations\n",
        "# cat_ranking_location = get_categories(location_questions_dict)\n",
        "# #pprint.pprint(cat_ranking_location, indent=1)\n",
        "# cat_with_pv_location = get_pageviews_for_categories(cat_ranking_location)\n",
        "# #pprint.pprint(cat_with_pv_location, indent=1)\n",
        "# #for locations: sorting the dict after pages per category\n",
        "# categories_with_subs_and_pageviews_location = get_dict_for_every_location(cat_ranking_location, cat_with_pv_location)\n",
        "# #pprint.pprint(categories_with_subs_and_pageviews_location,indent=1)\n",
        "# new_ordered_dict_location = sorting_dict(categories_with_subs_and_pageviews_location)\n",
        "# #pprint.pprint(new_ordered_dict_location,indent=1)\n",
        "\n",
        "# #for 3 locations 1m33s"
      ],
      "metadata": {
        "id": "sJ7C8z2j_Qqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #for person\n",
        "# person_questions_dict = dict(zip(person_df['Answer'], person_df['Question']))\n",
        "# cat_with_pv_person = {}\n",
        "\n",
        "# #print(person_questions_dict)\n",
        "# #del str\n",
        "\n",
        "# #for person\n",
        "# cat_ranking_person = get_categories(person_questions_dict)\n",
        "# #pprint.pprint(cat_ranking_person, indent=1)\n",
        "# cat_with_pv_person = get_pageviews_for_categories(cat_ranking_person)\n",
        "# #pprint.pprint(cat_with_pv_person, indent=1)\n",
        "# #for person: sorting the dict after pages per category\n",
        "# categories_with_subs_and_pageviews_person = get_dict_for_every_location(cat_ranking_person, cat_with_pv_person)\n",
        "# #pprint.pprint(categories_with_subs_and_pageviews_person,indent=1)\n",
        "# new_ordered_dict_person = sorting_dict(categories_with_subs_and_pageviews_person)\n",
        "# #pprint.pprint(new_ordered_dict_person,indent=1)\n",
        "\n",
        "# #for 3 people 17m22s\n",
        "# #27s\n",
        "# # for 20 F1 drivers -> 20mins"
      ],
      "metadata": {
        "id": "k1_R-UOV1ZYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #just for easy printing such that you dont have to0 calculate everything everytime\n",
        "# copy_categories_with_subs_and_pageviews_location = categories_with_subs_and_pageviews_location\n",
        "# copy_new_ordered_dict_location = new_ordered_dict_location\n",
        "# #print(\"Ordered after the number of pages per category:\")\n",
        "# #pprint.pprint(copy_categories_with_subs_and_pageviews_location,indent=1)\n",
        "# #print(\"Ordered after the number of pageviews per category:\")\n",
        "# #pprint.pprint(copy_new_ordered_dict_location,indent=1, compact=True)\n",
        "\n",
        "# copy_categories_with_subs_and_pageviews_person = categories_with_subs_and_pageviews_person\n",
        "# copy_new_ordered_dict_person = new_ordered_dict_person\n",
        "# #print(\"Ordered after the number of pages per category:\")\n",
        "# #pprint.pprint(copy_categories_with_subs_and_pageviews_person,indent=1)\n",
        "# #print(\"Ordered after the number of pageviews per category:\")\n",
        "# #prune the dict down to 20 most visited categories per person\n",
        "# #copy_new_ordered_dict_person = prune_ordered_dict(copy_new_ordered_dict_person, 20)\n",
        "# #first delete categories like (livingpeople, deaths, births) and prune the dict down to 20 most visited categories per person\n",
        "# copy_new_ordered_dict_person_test = prune_and_ordered_dict(copy_new_ordered_dict_person, 20)\n",
        "# copy_new_ordered_dict_person = sorting_dict(copy_new_ordered_dict_person_test)\n",
        "# #pprint.pprint(copy_new_ordered_dict_person,indent=1, compact=True)\n",
        "\n",
        "# number_categories_per_person = {}\n",
        "# for key, value in copy_new_ordered_dict_person.items():\n",
        "#     number_categories_per_person[key] = len(value)\n",
        "\n",
        "# #8.30 for 10 drivers\n",
        "# category_occurences = find_most_common_links(copy_new_ordered_dict_person)\n",
        "# #print('These are the categories that occur most between the person-entities:')\n",
        "# #pprint.pprint(category_occurences, indent=1, compact=True)"
      ],
      "metadata": {
        "id": "mJj4vnEC6TKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions for the unexpected-categories approach:"
      ],
      "metadata": {
        "id": "Wl3CKiM1lVqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"\n",
        "1. retrieve all of the related links of a wiki-persons-page (first 500);\n",
        "2. pre prune the list, such that only entries with 2 words in the title are left; (for performance reasons we assume 2 words equaly is human)\n",
        "3. analyse the links that are left via wikipedia APi and the \"instance of\" (P31) property set to \"human\" (Q5)\n",
        "4. now we have a list of related people to compare to our answer-person-entity\n",
        "5.1 retrieve the categories of the answer_entities (top 10-20)\n",
        "5.2 retrieve the categories of those related people as done above with the answer_entities (top 10-20)\n",
        "\n",
        "6. compare the categories of the related-people of the corresponding answer-entity and list the ones that appear most often (between answer-entity=A AND related_people of A )\n",
        "7. look for the \"unexpected\", a popular category that is well known and where from a list of related_people our answer-entity is the only one that appears in\n",
        "\n",
        "8. create some metrics and threshholds -> testing\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "Function that retrieves the related/popular people of the answer-entities;\n",
        "Args:     the answer-entity name\n",
        "Returns:  a list where each entry of the list is a dictionary; the keys and values are the (title and url) of the related people\n",
        "\"\"\"\n",
        "def get_related_people_from_person_name(person_name):\n",
        "  wiki_link = 'https://en.wikipedia.org/wiki'\n",
        "  related_people_list = []\n",
        "\n",
        "  modified_text = person_name.replace(' ', '_') #replace spaces with underscores in the name to use it in link\n",
        "  link = f\"{wiki_link}/{modified_text}\"\n",
        "  #print(link)\n",
        "  related_articles = get_related_links(link)\n",
        "  filtered_links = filter_two_word_titles(related_articles)\n",
        "  related_people_list = check_if_person(filtered_links)\n",
        "\n",
        "  return related_people_list\n",
        "\n",
        "\"\"\"\n",
        "Function that gets the pageviews of up to 10 links at a time;\n",
        "Args:     dictionary where the keys are the answer-entities and the value is a link_list\n",
        "Returns:  dictionary where the keys are the answer-entities and the value is a list with the links and pageviews\n",
        "\"\"\"\n",
        "def get_pageviews_from_links(link_dict):\n",
        "  pageviews_range_url = 'https://pageviews.wmcloud.org/?project=en.wikipedia.org&platform=all-access&agent=user&redirects=0&range=last-year&pages='\n",
        "  return_dict = {}\n",
        "\n",
        "  for key,value in link_dict.items():\n",
        "    link_list = value\n",
        "\n",
        "    pruned_link_parts_list = extract_last_parts(link_list)\n",
        "    concat_str_for_links = concatenate_elements(pruned_link_parts_list)             #combine up to 10 of these links to create a request to pageview\n",
        "    pageviews_url_list = combine_pv_urls(pageviews_range_url, concat_str_for_links) #now we have a list of pageview links with all the backlinks of the thumbcaption part of the wiki page (REUSED FROM YEARS PART)\n",
        "    #pprint.pprint(pageviews_url_list)\n",
        "    categories_with_pageviews =combine_dicts_from_links(pageviews_url_list)         #now we called the links and retreieved the pageviews; saved them as a dictionary\n",
        "    #pprint.pprint(categories_with_pageviews)\n",
        "    ordered_categories_with_pageviews = sort_dict_desc(categories_with_pageviews)   #now the list is ordered in ascending order\n",
        "\n",
        "    return_dict[key] = ordered_categories_with_pageviews\n",
        "\n",
        "  return return_dict\n",
        "\n",
        "\"\"\"\n",
        "Function that takes the 5 most popular related-people from the related_people_pageviews_dict and\n",
        "retrieves the corresponding wiki-categories with the their pageviews;\n",
        "Args:     dictionary where the keys are the answer-entities and the value is a dict of related people with their pageviews\n",
        "Returns:  dictionary where the keys are the answer-entities and the value is\n",
        "  a dict where the keys are the related-people and the valus are tupples with the 10 most popular categories they appear and with the pageviews of that category\n",
        "\"\"\"\n",
        "def get_categories_of_people_list(people_list, limit=5):\n",
        "  return_dict = {}\n",
        "  #intermediate_relatedpeople_with_categories_dict = {}\n",
        "\n",
        "  for key,value in people_list.items():\n",
        "    related_people_orderd = dict(sorted(value.items(), key=lambda x: x[1], reverse=True))   #order the dict after the pageviews in descending order\n",
        "    top_most_popular_people = dict(itertools.islice(related_people_orderd.items(), 5))      #take the top 5 most known people from the list\n",
        "\n",
        "    if len(related_people_orderd) == 0 or  len(top_most_popular_people) == 0:\n",
        "      continue\n",
        "\n",
        "    categories_of_related_people = get_categories(top_most_popular_people)\n",
        "    categories_with_pageviews_person = get_pageviews_for_categories(categories_of_related_people)\n",
        "    categories_with_subs_and_pageviews_person = get_dict_for_every_location(categories_of_related_people, categories_with_pageviews_person)\n",
        "\n",
        "    #print(\"Test_3\")\n",
        "    new_ordered_dict_related_person = sorting_dict(categories_with_subs_and_pageviews_person)\n",
        "    copy_new_ordered_dict_person_test = prune_and_ordered_dict(new_ordered_dict_related_person, 10)\n",
        "    ordered_dict_related_person = sorting_dict(copy_new_ordered_dict_person_test)\n",
        "    return_dict[key] = ordered_dict_related_person\n",
        "  return return_dict\n",
        "\n",
        "\"\"\"\n",
        "Function that is created with help of reusable function from above\n",
        "Args:     person_question_dict with all people with their questions\n",
        "Returns:  dictionary where the keys are the answer-entities and the value is a OrderedDict of all categories with pageviews\n",
        "\"\"\"\n",
        "def get_categories_with_pv_answerEntities():\n",
        "  person_questions_dict = dict(zip(person_df['Answer'], person_df['Question']))\n",
        "  cat_ranking_person = get_categories(person_questions_dict)\n",
        "  #pprint.pprint(cat_ranking_person, indent=1)\n",
        "  cat_with_pv_person = get_pageviews_for_categories(cat_ranking_person)\n",
        "  #pprint.pprint(cat_with_pv_person, indent=1)\n",
        "  #for person: sorting the dict after pages per category\n",
        "  categories_with_subs_and_pageviews_person = get_dict_for_every_location(cat_ranking_person, cat_with_pv_person)\n",
        "  #pprint.pprint(categories_with_subs_and_pageviews_person,indent=1)\n",
        "  new_ordered_dict_person = sorting_dict(categories_with_subs_and_pageviews_person)\n",
        "\n",
        "  return new_ordered_dict_person\n",
        "\n",
        "\"\"\"\n",
        "Counts the occurrences of categories (links) in the input dictionary.\n",
        "Args: data (dict): A dictionary containing category links as keys and tuples as values.\n",
        "Returns: dict: A new dictionary where the keys are the category links and the values are\n",
        "  the corresponding tuples of the category with a middle value indicating how\n",
        "  often the link/category appeared in the other keys.\n",
        "\"\"\"\n",
        "def count_categories(related_people_with_categories, answer_entities_with_categories):\n",
        "  category_appereances = {}\n",
        "\n",
        "  for answerEntityKey, aeCategory in answer_entities_with_categories.items():\n",
        "    inner_dict = {}\n",
        "    for answerKey, relatedDict in related_people_with_categories.items():\n",
        "      if answerEntityKey == answerKey:\n",
        "        for catLink, tupl in aeCategory.items():\n",
        "          people_list = []\n",
        "          for relatedPersonKey, catWithPvs in relatedDict.items():\n",
        "            for relatedLink, relatedTupl in  catWithPvs.items():\n",
        "              if catLink == relatedLink:\n",
        "                rel_pers_str = str(relatedPersonKey)\n",
        "                if rel_pers_str not in people_list and catLink == relatedLink:\n",
        "                  people_list.append(rel_pers_str)\n",
        "                #inner_dict[relatedLink] = (len(people_list),relatedTupl, people_list)\n",
        "                inner_dict[relatedLink] = (len(people_list),relatedTupl, people_list)\n",
        "    category_appereances[answerEntityKey] = inner_dict\n",
        "\n",
        "  for k,v in category_appereances.items():\n",
        "    if len(v) == 0:\n",
        "      for answerEntityKey, aeCategory in answer_entities_with_categories.items():\n",
        "        if k == answerEntityKey:\n",
        "          inner_dict={}\n",
        "          for a,b in aeCategory.items():\n",
        "            inner_dict[a] = (0, b, [])\n",
        "          category_appereances[k] = inner_dict\n",
        "\n",
        "  return category_appereances\n",
        "\n",
        "\"\"\"\n",
        "Calculates the Intersection over Union between the answer-entity and each of the related-person entries seperately\n",
        "Args: data (dict): A dictionary containing the answer-entity with the corresponding related-people and their most popular categories that they share with ther answer-entity\n",
        "Returns: dict: A dict where the value is a list of tuple like this: ('Max Verstappen', 'Charles Leclerc', 5, 20, 0.25) (person_a, person_b, num_shared_categories, num_total_categories, num_shared_categories/num_total_categories)\n",
        "\"\"\"\n",
        "def calculate_IoU_from_countedCategoryDict(counted_category_apperances):\n",
        "  #first we recover the list of people that are related to answer-entity\n",
        "  p_dict = {}\n",
        "  for key,value in counted_category_apperances.items():\n",
        "    person_list = []\n",
        "    #if len(value) > 0:\n",
        "    for link, f_tup in value.items():\n",
        "      #print(f_tup)\n",
        "      num= f_tup[0]\n",
        "      tup= f_tup[1]\n",
        "      p_lst = f_tup[2]\n",
        "      for person in p_lst:\n",
        "        if person not in person_list:\n",
        "          person_list.append(person)\n",
        "    #else:\n",
        "      #person_list = [('', (1, 1, []))]\n",
        "    p_dict[key] = person_list\n",
        "\n",
        "  iou_between_person_list = {}\n",
        "\n",
        "  #now calculate the IoU\n",
        "  for key_cat,value_cat in counted_category_apperances.items():\n",
        "    for key,value in p_dict.items():\n",
        "      if key == key_cat:\n",
        "        inter_list = {}\n",
        "        for person in value:\n",
        "          p_count = 0\n",
        "          for link, f_tup in value_cat.items():\n",
        "            p_lst = f_tup[2]\n",
        "            if person in p_lst:\n",
        "              p_count += 1\n",
        "          inter_list[person] = p_count\n",
        "      iou_between_person_list[key_cat] = inter_list\n",
        "\n",
        "  IoU_dict = {}\n",
        "  for key, value in iou_between_person_list.items():\n",
        "    IoU_list = []\n",
        "    for person, number in value.items():\n",
        "      num_total_categs = 20\n",
        "      IoU_list.append((person,number,num_total_categs, number/num_total_categs))\n",
        "    IoU_dict[key] = IoU_list\n",
        "\n",
        "  #return iou_between_person_list\n",
        "  return IoU_dict\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Calculates the avg_diversity_from_IoU\n",
        "Args:\n",
        "Returns:\n",
        "\"\"\"\n",
        "def calculate_avg_diversity_from_IoU(intersection_between_people_with_ae):\n",
        "  avg_diversity_dict = {}\n",
        "\n",
        "  for key,value in intersection_between_people_with_ae.items():\n",
        "    diversity_sum = 0\n",
        "    pairwise_comparisons = 0\n",
        "    for item in value:\n",
        "      all_categs = item[2]\n",
        "      IoU = item[1]\n",
        "      diversity_sum += (all_categs - IoU)\n",
        "      pairwise_comparisons += 1\n",
        "    try:\n",
        "      avg_diversity_dict[key] = (diversity_sum / pairwise_comparisons)\n",
        "    except Exception as e:\n",
        "      #print(e)\n",
        "      avg_diversity_dict[key] = 0\n",
        "  return avg_diversity_dict\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Calculates the categories score from the category diversity (calculated by calculate_avg_diversity_from_IoU() ) and the cat_popularity (=pageviews)\n",
        "Args:\n",
        "Returns:\n",
        "\"\"\"\n",
        "def calculate_categories_score(counted_category_apperances, avg_diversity_from_IoU):\n",
        "  categories_scores_dict = {}\n",
        "  for key,value in counted_category_apperances.items():\n",
        "    inter_dict={}\n",
        "    for item in value.items():\n",
        "      link = item[0]\n",
        "      cat_popularity = item[1][1]\n",
        "      for name, cat_div in avg_diversity_from_IoU.items():\n",
        "        if name == key:\n",
        "          inter_dict[link] = cat_popularity * cat_div\n",
        "    categories_scores_dict[key] = inter_dict\n",
        "\n",
        "  ordered_dicter = {}\n",
        "  ordered_scores ={}\n",
        "  for k,v in categories_scores_dict.items():\n",
        "    ordered_scores[k] = OrderedDict(v)\n",
        "  for k,v in ordered_scores.items():\n",
        "    ordered_dicter[k] = OrderedDict(sorted(v.items(), key=lambda x: x[1], reverse=True))\n",
        "\n",
        "  #modify the scores to give some categories a lower one\n",
        "  for key,value in ordered_dicter.items():\n",
        "    for link, score in value.items():\n",
        "      if '20th' in link or '21st' in link:\n",
        "        value[link]  = score / 4\n",
        "\n",
        "  return ordered_dicter\n",
        "\n",
        "\n",
        "#template_sentence_person_list = ['The person you are looking for is/was occupied as 0 and a member of the category 1', 'The person you are looking for is/was active in the occupation 0 and appears in the category: 1']\n",
        "template_sentence_person_list = ['The person you are looking for is/was occupied as 0 and a member of the category 1']\n",
        "\n",
        "#takes the categories scores dict and chooses the category with the highest score\n",
        "def create_hint_sentences_unexCategs(categories_scores_dict, person_answers_dict):\n",
        "  people_occupations = get_occupations(person_answers_dict)\n",
        "  most_unexpected_categories_dict = {}\n",
        "  hint_sentence_unexCateg_dict = {}\n",
        "\n",
        "\n",
        "\n",
        "  try:\n",
        "\n",
        "    for key,value in categories_scores_dict.items():\n",
        "      categories_scores_dict[key] = OrderedDict(sorted(value.items(), key=lambda x: x[1], reverse=True))\n",
        "      most_unexpected_categories_dict[key] = (next(iter(value.items())), people_occupations[key])\n",
        "\n",
        "    occu_str = 'television_presenter'\n",
        "    for key,value in most_unexpected_categories_dict.items():\n",
        "      hint_sentence_unexCateg_dict[key] = []\n",
        "      try:\n",
        "        if people_occupations[key] == 'Racing':\n",
        "          occu_str = people_occupations[key]\n",
        "        else:\n",
        "          occu_str = people_occupations[key]\n",
        "      except Exception as e:\n",
        "        pass\n",
        "        #print(\"create_hint_sentences_unexCategs\" + str(e))\n",
        "      for sentence in template_sentence_person_list:\n",
        "        hint_sentence_unexCateg_dict[key].append( sentence.replace('0', occu_str).replace('1', get_category_title(most_unexpected_categories_dict[key][0][0]).split(':')[-1].replace('_', ' ') ))\n",
        "\n",
        "  except Exception as e: print(e)\n",
        "\n",
        "\n",
        "  return hint_sentence_unexCateg_dict"
      ],
      "metadata": {
        "id": "cnbHXuXYsvjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# person_answers_dict = dict(zip(person_df['Answer'], person_df['Question']))\n",
        "# pprint.pprint(person_answers_dict)\n",
        "\n",
        "# people_occupations = get_occupations(person_answers_dict)\n",
        "# pprint.pprint(people_occupations)\n"
      ],
      "metadata": {
        "id": "--vjFH7nL91j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CREATE A FUNCTION THAT PUTS EVERYTHING TOGETHER - FOR UNEXPECTED CATEGORIES\n",
        "\"\"\"\n",
        "1. retrieve all of the related links of a wiki-persons-page (first 500);\n",
        "2. pre prune the list, such that only entries with 2 words in the title are left; (for performance reasons)\n",
        "3. analyse the links that are left via wikipedia APi and the \"instance of\" (P31) property set to \"human\" (Q5)\n",
        "4. now we have a list of related people to compare to our answer-person-entity\n",
        "5. search for the unexpected category: a category that is popular/well-known, but the answer-entity is one of the only entries from his occupation that appears in the category\n",
        "6. rank them via the calculated scores\n",
        "7. create hint-sentences with it\n",
        "\"\"\"\n",
        "def get_person_hints_unexpected_categories():\n",
        "  person_answers_dict = dict(zip(person_df['Answer'], person_df['Question']))\n",
        "  related_people_dict = {}\n",
        "  related_people_link_dict= {}\n",
        "  related_people_pageviews_dict = {}\n",
        "  most_popular_related_people_with_categories = {}\n",
        "\n",
        "  #time saving for first part (related people recovery) 2m\n",
        "  for key,value in person_answers_dict.items():\n",
        "    related_people_dict[key] = get_related_people_from_person_name(key)\n",
        "\n",
        "\n",
        "  #time saving for second part (related peoples with pageviews and ordering) - 16m (6-7m)\n",
        "  for key,value in related_people_dict.items():\n",
        "    inter_link_list = []\n",
        "    for item in value:\n",
        "      inter_link_list.append(item['url'])\n",
        "    related_people_link_dict[key] = inter_link_list\n",
        "    related_people_pageviews_dict = get_pageviews_from_links(related_people_link_dict)\n",
        "\n",
        "  #time saving for third part (related peoples categories recovery and ordering) - 43m+ (14m-24m)\n",
        "  most_popular_related_people_with_categories = get_categories_of_people_list(related_people_pageviews_dict)\n",
        "  #pprint.pprint(\"most_popular_related_people_with_categories\" )\n",
        "  #pprint.pprint(most_popular_related_people_with_categories)\n",
        "\n",
        "  #time saving for third part retrieves the categories of the answer-entities - 9m+ (6m)\n",
        "  answer_entities_with_categories = get_categories_with_pv_answerEntities()\n",
        "\n",
        "  #time saving for fourth part counts the categories of the answer-entities - 3s\n",
        "  counted_category_apperances = count_categories(most_popular_related_people_with_categories, answer_entities_with_categories)\n",
        "\n",
        "  # pprint.pprint(counted_category_apperances)\n",
        "\n",
        "\n",
        "  #just for the ordering of the inner list\n",
        "  ordered_data = {}\n",
        "  for key,value in counted_category_apperances.items():\n",
        "    tmp = OrderedDict(value)\n",
        "    ordered_data[key] = OrderedDict(sorted(tmp.items(), key=lambda x: x[1][0], reverse=True) )\n",
        "  counted_category_apperances = ordered_data\n",
        "\n",
        "  # pprint.pprint(counted_category_apperances)\n",
        "\n",
        "  #1. calculate the IoU between max and every other person - (M,C) = 5/20; (M,L) = 2/20; (M,D) = 2/20; (M,A) = 2/20; (M,F) = 2/20;\n",
        "  intersection_between_people_with_ae = calculate_IoU_from_countedCategoryDict(counted_category_apperances)\n",
        "  # pprint.pprint(intersection_between_people_with_ae)\n",
        "\n",
        "  #2. calculate the average diversity between the 6 drivers - (20-5) + (20-2) + (20-2) + (20-2) + (20-2) = 87; (#avg_diversity/#pairwise_comparison) = 87/5 = 17,4\n",
        "  avg_diversity_from_IoU = calculate_avg_diversity_from_IoU(intersection_between_people_with_ae)\n",
        "  # pprint.pprint(avg_diversity_from_IoU)\n",
        "\n",
        "  #3. calculate a type of categoreis_score - categories_score = cat_diversity * cat_popularity(pvs)\n",
        "  categories_scores_dict = calculate_categories_score(counted_category_apperances, avg_diversity_from_IoU)\n",
        "  for key,value in categories_scores_dict.items():\n",
        "    categories_scores_dict[key] = OrderedDict(sorted(value.items(), key=lambda x: x[1], reverse=True))\n",
        "  # pprint.pprint(categories_scores_dict)\n",
        "\n",
        "  #create some sentences with the occupation and a unexpected category as hints\n",
        "  #pprint.pprint(categories_scores_dict, indent=2, sort_dicts=False)\n",
        "  # pprint.pprint(categories_scores_dict)\n",
        "  # pprint.pprint(person_answers_dict)\n",
        "\n",
        "\n",
        "  mucd = create_hint_sentences_unexCategs(categories_scores_dict, person_answers_dict)\n",
        "  # pprint.pprint(mucd)\n",
        "\n",
        "  inter = {}\n",
        "\n",
        "  for key, value in mucd.items():\n",
        "    for answer,question in person_answers_dict.items():\n",
        "      if key == answer:\n",
        "        #print(question,value[0])\n",
        "        sim_score = get_similarity_score(question,value[0])\n",
        "        #print(key, value, sim_score, inter, mucd)\n",
        "        inter[key]  = {value[0] : sim_score}\n",
        "        #mucd[key] = {value : sim_score}\n",
        "\n",
        "  return inter"
      ],
      "metadata": {
        "id": "vfBtFByDNlOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #ca. 27m of execution for 2 people (19min)\n",
        "# unexpected_categories_hints = get_person_hints_unexpected_categories()\n",
        "\n",
        "# pprint.pprint(unexpected_categories_hints, indent=2)"
      ],
      "metadata": {
        "id": "aYF6V3xBSAsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions for the unexpected-predicate approach:"
      ],
      "metadata": {
        "id": "FHqRIHd3ao5Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, the above was about category membership, but how about predicates.\n",
        "\n",
        "Again compare the predicates of our answer-entity with the predicates of similar people (calculated above in related_people) and find a special/unexpected/surprising predicate about our answer-entity.\n",
        "\n",
        "By similar people here we could take ones that we have already calculated in the unexpected-categories approach.\n",
        "\n",
        "---\n",
        "\n",
        "Say our John Pack has 5 predicates such as:\n",
        "*   p1 - he has 2 kids,\n",
        "*   p2 - he was married 5 times,\n",
        "*   p3 - he is 170cm tall,\n",
        "*   p4 - he was born in White Stream, Alabama,\n",
        "*   p5 - he was in prison when in his 20s.\n",
        "\n",
        "Which one is good to use as a hint?\n",
        "I believe the one that is exceptional, hence, based on that thinking p1, p3 are not good, p2, p4, p5 are good.\n",
        "\n",
        "What scoring strategy would select p2, p4, p5?\n",
        "I believe one based on how often similar people like our John have the values of same predicates. That is, how many US politicians have more than 2 wives in their life? how many have kids, 2 kids, etc."
      ],
      "metadata": {
        "id": "-Y3fItfIcEj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#CREATE A FUNCTION THAT PUTS EVERYTHING TOGETHER - FOR UNEXPECTED PREDICATES\n",
        "\"\"\"\n",
        "1. retrieve all of the properties of a wikidata-persons-page;\n",
        "2. pre prune the list, such that image entries etc are discarded; (for performance reasons)\n",
        "3. format the entries to get a nice dict\n",
        "4. now we have a list of properties/predicates to compare and rank\n",
        "\n",
        "5. now create some kind of decision function to  find a unexpected or surprising property\n",
        "\"\"\"\n",
        "\n",
        "#https://www.wikidata.org/wiki/Wikidata:SPARQL_query_service/queries/examples#Subproperties_of_location_(P276)\n",
        "# Subproperties of location (P276)\n",
        "# All properties with descriptions and aliases and types\n",
        "# MV link:https://www.wikidata.org/wiki/Q2239218\n",
        "\n",
        "#list of interesting properties of people\n",
        "\n",
        "\n",
        "list_of_properties = ['nickname', 'country of citizenship', 'name in native language', 'native language', 'height',\n",
        "                  'occupation', 'field of work', 'educated at', 'residence', 'work period', 'ethnic group',\n",
        "                  'notable work', 'member of', 'owner of', 'significant event', 'award received',\n",
        "                  'date of birth', 'place of birth', 'date of death', 'place of death', 'manner of death',\n",
        "                  'cause of death', 'social media followers', 'father', 'mother', 'sibling', 'spouse', 'child', 'unmarried partner', 'sport']\n",
        "\n",
        "properties_blank_sentences = {\n",
        "  'child': 'The person you are looking for, has / children.',\n",
        "  'sibling': 'The person you are looking for, has / siblings.',\n",
        "  'native language': 'The person you are looking for, speaks 0.',\n",
        "  'occupation': 'The person you are looking for, is occupied as 0.',\n",
        "  'award received': 'The person you are looking for has won multiple awards in his life, some of them are 0.',\n",
        "  'ethnic group': 'The person you are looking for was/is a member of the follwoing ethnic group: 0.',\n",
        "  'nickname': 'The person you are looking for was/is also known under the follwoing nickname: 0.',\n",
        "  'significant event': 'Some of the most significant events of the searched person were: 0',\n",
        "  'notable work': 'The person you are looking for was involved in some very notable works, like: 0.',\n",
        "  'child + sibling': 'The person you are looking for, has / children and * siblings.',\n",
        "  'date of birth + place of birth': 'The person you are looking for was born on / in *.',\n",
        "  'date of birth + place of birth + date of death + place of death': 'The person you are looking for was born on / in * and died on - in +.' ,\n",
        "  'work period' :  'The person you are looking for was most active during / and *.'\n",
        "  }\n",
        "\n",
        "\"\"\"\n",
        "Retrieves the Wikipedia identifiers for a list of person names.\n",
        "Args: None\n",
        "Returns: dict: A dictionary mapping person names to their Wikipedia identifiers.\n",
        "\"\"\"\n",
        "def get_properties_predicates_hints():\n",
        "  person_answers_dict = dict(zip(person_df['Answer'], person_df['Question']))\n",
        "  person_names = []\n",
        "  for answer, question in person_answers_dict.items():\n",
        "    person_names.append(answer)\n",
        "  identifiers = get_wikipedia_identifiers(person_names)\n",
        "  properties_person_name_dict = {}\n",
        "\n",
        "  for name, pid in identifiers.items():\n",
        "    properties_person_name_dict[name] = get_person_properties(pid, list_of_properties)\n",
        "\n",
        "  return properties_person_name_dict\n",
        "\n",
        "\"\"\"\n",
        "Creates hint sentences based on properties of a person and blank sentences.\n",
        "Args: properties_person_name_dict (dict): A dictionary containing properties of a person mapped to their corresponding values.\n",
        "      properties_blank_sentences (dict): A dictionary containing blank sentences with placeholders.\n",
        "Returns: dict: A dictionary containing hint sentences for each person's properties.\n",
        "\"\"\"\n",
        "def create_hint_sentences_predicates(properties_person_name_dict, properties_blank_sentences):\n",
        "  #person_answers_dict = dict(zip(person_df['Answer'], person_df['Question']))\n",
        "  person_answers_dict = dict(zip(person_df['Answer'], person_df['Question']))\n",
        "  person_names = []\n",
        "  #person_names = [\"Michael Jackson\", \"Albert Einstein\", \"Max Verstappen\"]\n",
        "  for answer, question in person_answers_dict.items():\n",
        "    person_names.append(answer)\n",
        "\n",
        "  hint_sentence_dict = {}\n",
        "\n",
        "  for pers_name, value in properties_person_name_dict.items():\n",
        "    properties_sentences_dict = {}\n",
        "    for proper, entries in value.items():\n",
        "      if proper in properties_blank_sentences:\n",
        "        if type(entries) != dict:\n",
        "          if len(entries) >= 3: #when we want to list 3 items\n",
        "            first_two = entries[:2]\n",
        "            intermediate_str = ', '.join(first_two)\n",
        "            intermediate_str = intermediate_str + ' and '+ str(entries[2])\n",
        "          elif len(entries) == 2: #when we want to list 2 items\n",
        "            intermediate_str = str(entries[0]) + ' and '+ str(entries[1])\n",
        "          elif len(entries) == 1: #when we want to list 1 item\n",
        "            intermediate_str = entries[0]\n",
        "          properties_sentences_dict[proper] = properties_blank_sentences[proper].replace('0', str(intermediate_str))\n",
        "\n",
        "    if 'child' in value:\n",
        "      properties_sentences_dict['child'] = properties_blank_sentences['child'].replace('/', str(len(value['child'])))\n",
        "    if 'sibling' in value:\n",
        "      properties_sentences_dict['sibling'] = properties_blank_sentences['sibling'].replace('/', str(len(value['sibling'])))\n",
        "    if 'child' in value and 'sibling' in value:\n",
        "      properties_sentences_dict['child + sibling'] = properties_blank_sentences['child + sibling'].replace('/', str(len(value['child']))).replace('*', str(len(value['sibling'])))\n",
        "\n",
        "    if 'date of birth' in value and 'place of birth' in value:\n",
        "      properties_sentences_dict['date of birth + place of birth'] = properties_blank_sentences['date of birth + place of birth'].replace('/', str(value['date of birth'][0])).replace('*', str(value['place of birth'][0]))\n",
        "    if 'date of birth' in value and 'place of birth' in value and 'date of death' in value and 'place of death' in value:\n",
        "      properties_sentences_dict['date of birth + place of birth + date of death + place of death'] = properties_blank_sentences['date of birth + place of birth + date of death + place of death'].replace('/', str(value['date of birth'][0])).replace('*', str(value['place of birth'][0])).replace('-', str(value['date of death'][0])).replace('+', str(value['place of death'][0]))\n",
        "\n",
        "    hint_sentence_dict[pers_name] = properties_sentences_dict\n",
        "\n",
        "\n",
        "\n",
        "  return hint_sentence_dict\n",
        "\n",
        "\n",
        "def get_person_hints_unexpected_predicates():\n",
        "  properties_person_name_dict = get_properties_predicates_hints()\n",
        "  hint_sentences_predicates = create_hint_sentences_predicates(properties_person_name_dict, properties_blank_sentences)\n",
        "  #pprint.pprint(hint_sentences_predicates)\n",
        "  person_answers_dict = dict(zip(person_df['Answer'], person_df['Question']))\n",
        "  for key, value in hint_sentences_predicates.items():\n",
        "    for predicate, sentence in value.items():\n",
        "      for answer,question in person_answers_dict.items():\n",
        "        if key == answer:\n",
        "          #print(question,sentence)\n",
        "          sim_score = get_similarity_score(question,sentence)\n",
        "          hint_sentences_predicates[key][predicate] = {sentence : sim_score}\n",
        "  return hint_sentences_predicates\n"
      ],
      "metadata": {
        "id": "7W9z4KGxf0Vi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #properties_person_name_dict = get_properties_predicates_hints()\n",
        "# #test_prop = properties_person_name_dict\n",
        "# #pprint.pprint(properties_person_name_dict)\n",
        "# ok =  get_person_hints_unexpected_predicates()\n",
        "# pprint.pprint(ok)\n",
        "\n",
        "\n",
        "# #TESTING\n",
        "\n",
        "# # hint_sentences_predicates = create_hint_sentences_predicates(test_prop, properties_blank_sentences)\n",
        "# # pprint.pprint(hint_sentences_predicates)"
      ],
      "metadata": {
        "id": "7Y_yokzxca_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **New prediction Methods for locations:**"
      ],
      "metadata": {
        "id": "6-A126sxQfu7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We tried different ways of finding pieces of information that are most popular about a certain location. The first approach was to recover some fixed properties of each location to combine into hint-sentences. The second was to reuse and modify the unexpected-categories approach from the people-prediciton-methods.\n",
        "\n",
        "---\n",
        "\n",
        "Short explenation of the functions:\n",
        "*   Unexpected **categories** part: similar to the unexpected_categories part from the people-prediciton. We compare the searched-location to similar ones (compare the categories), rank them after the most popular ones and choose the most unusual/surprising one.\n",
        "*   Fixed **properties**: we look up some predetermined pieces of information and compare the question together with the hint sentences to get the most similar one as hint-sentence."
      ],
      "metadata": {
        "id": "hXIhijsy-Fem"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### location_hints_unexpected_categories:"
      ],
      "metadata": {
        "id": "6F0-qieloW-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Function that retrieves the related/popular locations of the answer-entities;\n",
        "Args:     the answer-entity name\n",
        "Returns:  a list where each entry of the list is a dictionary; the keys and values are the (title and url) of the related locations\n",
        "\"\"\"\n",
        "def get_related_location_from_location_name(location_name):\n",
        "  wiki_link = 'https://en.wikipedia.org/wiki'\n",
        "  related_people_list = []\n",
        "\n",
        "  modified_text = location_name.replace(' ', '_') #replace spaces with underscores in the name to use it in link\n",
        "  link = f\"{wiki_link}/{modified_text}\"\n",
        "  #print(link)\n",
        "  related_articles = get_related_links(link)\n",
        "  #filtered_links = filter_two_word_titles(related_articles)\n",
        "  #related_people_list = check_if_person(filtered_links)\n",
        "\n",
        "  return related_articles\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Function that is created with help of reusable function from above\n",
        "Args:     location_question_dict with all locations with their questions\n",
        "Returns:  dictionary where the keys are the answer-entities and the value is a OrderedDict of all categories with pageviews\n",
        "\"\"\"\n",
        "def get_categories_with_pv_answerEntities_location():\n",
        "  location_questions_dict = dict(zip(location_df['Answer'], location_df['Question']))\n",
        "  cat_ranking_location = get_categories(location_questions_dict)\n",
        "  #pprint.pprint(cat_ranking_person, indent=1)\n",
        "  cat_with_pv_location = get_pageviews_for_categories(cat_ranking_location)\n",
        "  #pprint.pprint(cat_with_pv_person, indent=1)\n",
        "  #for person: sorting the dict after pages per category\n",
        "  categories_with_subs_and_pageviews_location = get_dict_for_every_location(cat_ranking_location, cat_with_pv_location)\n",
        "  #pprint.pprint(categories_with_subs_and_pageviews_person,indent=1)\n",
        "  new_ordered_dict_location = sorting_dict(categories_with_subs_and_pageviews_location)\n",
        "\n",
        "  return new_ordered_dict_location\n",
        "\n",
        "#template_sentence_location_list = ['The location you are looking for is/was a member of the category 1', 'The location you are looking for appears in the category: 1']\n",
        "template_sentence_location_list = ['The location you are looking for is/was a member of the category 1']\n",
        "\n",
        "#takes the categories scores dict and chooses the category with the highest score\n",
        "def create_hint_sentences_unexCategs_location(categories_scores_dict, location_answers_dict):\n",
        "  #+people_occupations = get_occupations(location_answers_dict)\n",
        "  most_unexpected_categories_dict = {}\n",
        "  hint_sentence_unexCateg_dict = {}\n",
        "\n",
        "  print(\"categories_scores_dict\")\n",
        "  pprint.pprint(categories_scores_dict,indent=1)\n",
        "\n",
        "  try:\n",
        "    for key,value in categories_scores_dict.items():\n",
        "      categories_scores_dict[key] = OrderedDict(sorted(value.items(), key=lambda x: x[1], reverse=True))\n",
        "      most_unexpected_categories_dict[key] = (next(iter(value.items())))\n",
        "\n",
        "    #occu_str = 'mechanic'\n",
        "\n",
        "    #print(\"most_unexpected_categories_dict\")\n",
        "    #pprint.pprint(most_unexpected_categories_dict['Austria'][0],indent=1)\n",
        "\n",
        "    for key,value in most_unexpected_categories_dict.items():\n",
        "      hint_sentence_unexCateg_dict[key] = []\n",
        "      for sentence in template_sentence_location_list:\n",
        "        hint_sentence_unexCateg_dict[key].append( sentence.replace('1', get_category_title(most_unexpected_categories_dict[key][0]).split(':')[-1].replace('_', ' ') ))\n",
        "  except Exception as e: print(e)\n",
        "\n",
        "\n",
        "  return hint_sentence_unexCateg_dict\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Counts the occurrences of categories (links) in the input dictionary.\n",
        "Args: data (dict): A dictionary containing category links as keys and tuples as values.\n",
        "Returns: dict: A new dictionary where the keys are the category links and the values are\n",
        "  the corresponding tuples of the category with a middle value indicating how\n",
        "  often the link/category appeared in the other keys.\n",
        "\"\"\"\n",
        "def count_categories_location(related_people_with_categories, answer_entities_with_categories):\n",
        "  category_appereances = {}\n",
        "\n",
        "\n",
        "\n",
        "  for answerEntityKey, aeCategory in answer_entities_with_categories.items():\n",
        "    for answerKey, relatedDict in related_people_with_categories.items():\n",
        "      if answerEntityKey == answerKey:\n",
        "       # print(answerEntityKey, answerKey)\n",
        "        inner_dict = {}\n",
        "\n",
        "        for categ, pvs in aeCategory.items():\n",
        "          people_list = []\n",
        "          for relatedPersonKey, catWithPvs in relatedDict.items():\n",
        "            for relCateg, relPvs in  catWithPvs.items():\n",
        "              if categ == relCateg:\n",
        "                #print(categ, relCateg)\n",
        "                rel_location_str = str(relatedPersonKey)\n",
        "                if rel_location_str not in people_list and categ == relCateg:\n",
        "                  people_list.append(rel_location_str)\n",
        "                #inner_dict[relatedLink] = (len(people_list),relatedTupl, people_list)\n",
        "                inner_dict[categ] = (len(people_list),relPvs, people_list)\n",
        "    category_appereances[answerEntityKey] = inner_dict\n",
        "\n",
        "  #pprint.pprint(category_appereances)\n",
        "\n",
        "  for k,v in category_appereances.items():\n",
        "    if len(v) == 0:\n",
        "      for answerEntityKey, aeCategory in answer_entities_with_categories.items():\n",
        "        if k == answerEntityKey:\n",
        "          inner_dict={}\n",
        "          for a,b in aeCategory.items():\n",
        "            inner_dict[a] = (0, b, [])\n",
        "          category_appereances[k] = inner_dict\n",
        "\n",
        "  return category_appereances"
      ],
      "metadata": {
        "id": "TniyBI6VYF_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CREATE A FUNCTION THAT PUTS EVERYTHING TOGETHER - FOR UNEXPECTED CATEGORIES FOR LOCATIONS\n",
        "\"\"\"\n",
        "1. retrieve all of the related links of a wiki-location-page (limit at first 500);\n",
        "2. pre prune the list, such that unecessary categories are discarded (for performance reasons)\n",
        "3. now we have a list of related locations to compare to our answer-location-entity\n",
        "4. search for the unexpected category: a category that is popular/well-known, but the answer-entity is one of the only entries from the list of related locations that appears in the category\n",
        "5. rank them via the calculated scores\n",
        "6. create hint-sentences with them\n",
        "7. improve upon all of the sentence generation functions\n",
        "\"\"\"\n",
        "def get_location_hints_unexpected_categories():\n",
        "  location_answers_dict = dict(zip(location_df['Answer'], location_df['Question']))\n",
        "  related_location_dict = {}\n",
        "  related_location_link_dict= {}\n",
        "  related_location_pageviews_dict = {}\n",
        "  most_popular_related_location_with_categories = {}\n",
        "\n",
        "  #time saving for first part (related locations recovery) 2m\n",
        "  for key,value in location_answers_dict.items():\n",
        "    related_location_dict[key] = get_related_location_from_location_name(key)\n",
        "\n",
        "  #time saving for second part (related locations with pageviews and ordering) - 16m (6-7m)\n",
        "  for key,value in related_location_dict.items():\n",
        "    inter_link_list = []\n",
        "    for item in value:\n",
        "      inter_link_list.append(item['url'])\n",
        "    related_location_link_dict[key] = inter_link_list\n",
        "    related_location_pageviews_dict = get_pageviews_from_links(related_location_link_dict)\n",
        "\n",
        "  # pprint.pprint(related_location_link_dict)\n",
        "  # pprint.pprint(related_location_pageviews_dict)\n",
        "\n",
        "  #time saving for third part (related locations categories recovery and ordering) - 43m+ (14m-24m)\n",
        "  most_popular_related_location_with_categories = get_categories_of_people_list(related_location_pageviews_dict)\n",
        "\n",
        "  #pprint.pprint(\"most_popular_related_people_with_categories\" )\n",
        "  #pprint.pprint(most_popular_related_location_with_categories)\n",
        "\n",
        "  #time saving for third part retrieves the categories of the answer-entities - 9m+ (6m)\n",
        "  answer_entities_with_categories_location = get_categories_with_pv_answerEntities_location()\n",
        "\n",
        "  pprint.pprint(answer_entities_with_categories_location)\n",
        "\n",
        "  #time saving for fourth part counts the categories of the answer-entities - 3s\n",
        "  counted_category_apperances_location = count_categories_location(most_popular_related_location_with_categories, answer_entities_with_categories_location)\n",
        "\n",
        "\n",
        "  #pprint.pprint(\"counted_category_apperances_location\" )\n",
        "  #pprint.pprint(counted_category_apperances_location)\n",
        "  #just for the ordering of the inner list\n",
        "  # HERE IS THE PROBLEM\n",
        "  ordered_data = {}\n",
        "  for key,value in counted_category_apperances_location.items():\n",
        "    tmp = OrderedDict(value)\n",
        "    ordered_data[key] = OrderedDict(sorted(tmp.items(), key=lambda x: x[1][0], reverse=True) )\n",
        "  counted_category_apperances_location = ordered_data\n",
        "\n",
        "\n",
        "  pprint.pprint(counted_category_apperances_location)\n",
        "\n",
        "  #1. calculate the IoU between max and every other person - (M,C) = 5/20; (M,L) = 2/20; (M,D) = 2/20; (M,A) = 2/20; (M,F) = 2/20;\n",
        "  intersection_between_locations_with_ae = calculate_IoU_from_countedCategoryDict(counted_category_apperances_location)\n",
        "  #2. calculate the average diversity between the 6 drivers - (20-5) + (20-2) + (20-2) + (20-2) + (20-2) = 87; (#avg_diversity/#pairwise_comparison) = 87/5 = 17,4\n",
        "  avg_diversity_from_IoU_location = calculate_avg_diversity_from_IoU(intersection_between_locations_with_ae)\n",
        "  #3. calculate a type of categoreis_score - categories_score = cat_diversity * cat_popularity(pvs)\n",
        "  categories_scores_dict_location = calculate_categories_score(counted_category_apperances_location, avg_diversity_from_IoU_location)\n",
        "  for key,value in categories_scores_dict_location.items():\n",
        "    categories_scores_dict_location[key] = OrderedDict(sorted(value.items(), key=lambda x: x[1], reverse=True))\n",
        "  #create some sentences with the occupation and a unexpected category as hints\n",
        "  mucd = create_hint_sentences_unexCategs_location(categories_scores_dict_location, location_answers_dict)\n",
        "\n",
        "  pprint.pprint(mucd,  sort_dicts=False)\n",
        "\n",
        "  inter = {}\n",
        "\n",
        "  for key, value in mucd.items():\n",
        "    for answer,question in location_answers_dict.items():\n",
        "      if key == answer:\n",
        "        sim_score = get_similarity_score(question,value[0])\n",
        "        inter[key]  = {value[0] : sim_score}\n",
        "  return inter"
      ],
      "metadata": {
        "id": "ccK8lWg-7Si2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #ca. 22m of execution for 1 location (21 min)\n",
        "# unexpected_categories_hints_location = get_location_hints_unexpected_categories()\n",
        "# pprint.pprint(unexpected_categories_hints_location, indent=2,  sort_dicts=False)"
      ],
      "metadata": {
        "id": "j3UyqJni79fB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### location_hints_fixed_properties:"
      ],
      "metadata": {
        "id": "AJdmA-alojmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Retrieve all data from the Wikidata page of a given location.\n",
        "Args: location (str): The name of the location.\n",
        "Returns: dict: A dictionary containing all data from the Wikidata page.\n",
        "\"\"\"\n",
        "def retrieve_location_data(location):\n",
        "  # Create the Wikidata API URL\n",
        "  url = f\"https://www.wikidata.org/w/api.php?action=wbgetentities&sites=enwiki&titles={location}&format=json\"\n",
        "  try:\n",
        "    # Send a GET request to the Wikidata API\n",
        "    response = requests.get(url)\n",
        "    data = response.json()\n",
        "    # Get the QID of the location\n",
        "    entities = data.get(\"entities\")\n",
        "    qid = next(iter(entities))\n",
        "    # Retrieve all data for the location\n",
        "    location_data = entities[qid]\n",
        "    return location_data\n",
        "\n",
        "  except requests.exceptions.RequestException as e:\n",
        "    pass\n",
        "    #print(f\"An error occurred: {e}\")\n",
        "  return None\n",
        "\n",
        "\"\"\"\n",
        "Get the data for a specific property from the retrieved location data.\n",
        "Args: location_data (dict): The data retrieved from the Wikidata page. property_code (str): The property code (e.g., P35) for the desired property.\n",
        "Returns: list: A list of property values for the given property code.\n",
        "\"\"\"\n",
        "def get_property_data(location_data, property_code):\n",
        "  # Check if the property exists in the location data\n",
        "  if property_code in location_data.get(\"claims\"):\n",
        "    # Retrieve the property values\n",
        "    property_values = location_data[\"claims\"][property_code]\n",
        "    # Extract the values from the property data\n",
        "    values = [value[\"mainsnak\"][\"datavalue\"][\"value\"] for value in property_values]\n",
        "    return values\n",
        "  return []\n",
        "\n",
        "\n",
        "\n",
        "def get_entity_name(entity_id):\n",
        "  url = f\"https://www.wikidata.org/wiki/Special:EntityData/{entity_id}.json\"\n",
        "  response = requests.get(url)\n",
        "  data = response.json()\n",
        "\n",
        "  try:\n",
        "    name = data['entities'][entity_id]['labels']['en']['value']\n",
        "    return name\n",
        "  except KeyError:\n",
        "    return None\n",
        "\n",
        "\n",
        "def create_hint_sentences(location_answers_dict, locations_identifiers_dict, properties_blank_sentences_locations):\n",
        "  ret ={}\n",
        "  location_infos_wikidata = {}\n",
        "\n",
        "  for key,value in location_answers_dict.items():\n",
        "    location_infos_wikidata[key] = retrieve_location_data(key)\n",
        "    for propert, sentence in properties_blank_sentences_locations.items():\n",
        "      locations_identifiers_dict[propert] =  get_property_data(location_infos_wikidata[key], propert)\n",
        "      #prune the list with multipole entries down to the desired one\n",
        "      if len(locations_identifiers_dict[propert]) > 1:\n",
        "        if propert != 'P47' and propert != 'P37' and propert != 'P206' and propert != 'P421' and  propert != 'P463' and propert != 'P793' and propert != 'P38':\n",
        "          locations_identifiers_dict[propert] = locations_identifiers_dict[propert][-1]\n",
        "      #if entry not null, search for the id of the entry\n",
        "      if len(locations_identifiers_dict[propert]) > 0:\n",
        "        propety = locations_identifiers_dict[propert]\n",
        "        try:\n",
        "          e_list = []\n",
        "          for entry in propety:\n",
        "            id = entry['id']\n",
        "            e_list.append(get_entity_name(id))\n",
        "          locations_identifiers_dict[propert] = e_list\n",
        "        except Exception as e:\n",
        "          try:\n",
        "              e_list = []\n",
        "              for entry in propety:\n",
        "                id = entry[0]['id']\n",
        "                e_list.append(get_entity_name(id))\n",
        "              locations_identifiers_dict[propert] = e_list\n",
        "          except Exception as e:\n",
        "            try:\n",
        "              e_list = []\n",
        "              id = propety['id']\n",
        "              e_list.append(get_entity_name(id))\n",
        "              locations_identifiers_dict[propert] = e_list\n",
        "            except Exception as e:\n",
        "              pass\n",
        "              #print(f\"An error occurred: {e}\" + propert)\n",
        "      if propert == 'P1082':\n",
        "        locations_identifiers_dict[propert] = locations_identifiers_dict[propert]['amount']\n",
        "    #now lets choose wich one we use for the sentences (sometimes random for others specific ones)\n",
        "    for prop, sentence in properties_blank_sentences_locations.items():\n",
        "      for p, val in locations_identifiers_dict.items():\n",
        "        if p == prop:\n",
        "          if len(val) == 0:\n",
        "            continue\n",
        "          elif len(val) == 1:\n",
        "            word = str(val[0])\n",
        "            properties_blank_sentences_locations[p] = sentence.replace('*', word)\n",
        "          else:\n",
        "            if p != 'P1082':\n",
        "              try:\n",
        "                test =  locations_identifiers_dict[p]\n",
        "                random_Words = random.sample(test,3)\n",
        "                random_Words = ', '.join(random_Words)\n",
        "              except:\n",
        "                try:\n",
        "                  test =  locations_identifiers_dict[p]\n",
        "                  random_Words = random.sample(test,2)\n",
        "                  random_Words = ', '.join(random_Words)\n",
        "                except:\n",
        "                  random_Words = random.choice(val)\n",
        "              properties_blank_sentences_locations[p] = sentence.replace('*', random_Words)\n",
        "            else:\n",
        "              properties_blank_sentences_locations[p] = sentence.replace('*', locations_identifiers_dict[p] )\n",
        "    ret[key] = properties_blank_sentences_locations\n",
        "  return ret"
      ],
      "metadata": {
        "id": "ofl8X7r6oT16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ranking_forfixed_properties():\n",
        "  #1. calculate the IoU between max and every other person - (M,C) = 5/20; (M,L) = 2/20; (M,D) = 2/20; (M,A) = 2/20; (M,F) = 2/20;\n",
        "  intersection_between_people_with_ae = calculate_IoU_from_countedCategoryDict(counted_category_apperances)\n",
        "  #2. calculate the average diversity between the 6 drivers - (20-5) + (20-2) + (20-2) + (20-2) + (20-2) = 87; (#avg_diversity/#pairwise_comparison) = 87/5 = 17,4\n",
        "  avg_diversity_from_IoU = calculate_avg_diversity_from_IoU(intersection_between_people_with_ae)\n",
        "  #3. calculate a type of categoreis_score - categories_score = cat_diversity * cat_popularity(pvs)\n",
        "  categories_scores_dict = calculate_categories_score(counted_category_apperances, avg_diversity_from_IoU)\n",
        "  for key,value in categories_scores_dict.items():\n",
        "    categories_scores_dict[key] = OrderedDict(sorted(value.items(), key=lambda x: x[1], reverse=True))\n",
        "  #create some sentences with the occupation and a unexpected category as hints\n",
        "  mucd = create_hint_sentences_unexCategs_location(categories_scores_dict, person_answers_dict)\n",
        "\n",
        "  return mucd\n",
        "\n",
        "def get_location_hints_fixed_properties():\n",
        "  # generate hints by using some fixed properties of the location:\n",
        "  locations_identifiers_dict = {}\n",
        "\n",
        "  locations_identifiers_dict['P610'] = {'highest_point': []}\n",
        "  locations_identifiers_dict['P6'] = {'head_of_government': []}\n",
        "  locations_identifiers_dict['P17'] = {'country': []}\n",
        "  locations_identifiers_dict['P30'] = {'continent': []}\n",
        "  locations_identifiers_dict['P35'] = {'head_of_state': []}\n",
        "  locations_identifiers_dict['P36'] = {'capital': []}\n",
        "  locations_identifiers_dict['P38'] = {'currency': []}\n",
        "  locations_identifiers_dict['P131'] = {'located_on': []}\n",
        "  locations_identifiers_dict['P1082'] = {'population': []}\n",
        "  locations_identifiers_dict['P1376'] = {'city_is_capital_of': []}\n",
        "\n",
        "  locations_identifiers_dict['P47'] = {'share_border': []}\n",
        "  locations_identifiers_dict['P37'] = {'languages': []}\n",
        "  locations_identifiers_dict['P206'] = {'body_of_water': []}\n",
        "  locations_identifiers_dict['P421'] = {'time_zone': []}\n",
        "  locations_identifiers_dict['P463'] = {'member_of': []}\n",
        "  locations_identifiers_dict['P793'] = {'significant_event': []}\n",
        "  locations_identifiers_dict['P1830'] = {'owner_of': []}\n",
        "\n",
        "  properties_blank_sentences_locations = {\n",
        "    'P610': 'The highest point in searched location is *.',\n",
        "    'P6': 'Head of government of searched location is *.',\n",
        "    'P17': 'The searched location is in country *.',\n",
        "    'P30': 'The searched location is on continent *.',\n",
        "    'P35': 'Head of state of searched location is *.',\n",
        "    'P36': 'The capital of searched location is  *.',\n",
        "    'P38': 'The currencies used in searched location is/are  *.',\n",
        "    'P37': 'Spoken language(s) in searched location is/are *.',\n",
        "    'P47': 'The searched location shares border with *.',\n",
        "    'P131': 'The searched city is located on  *.',\n",
        "    'P206': 'Some of the next bodies of water of searched location are *.',\n",
        "    'P421': 'The searched location is in time-zone *.',\n",
        "    'P463': 'Searched location is member of *.',\n",
        "    'P793': 'A significant event happened in this location was *.',\n",
        "    'P1082': 'The searched location has a population of *.',\n",
        "    'P1376': 'The searched city is capital of  *.',\n",
        "    'P1830': 'The searched location is owner of *.',\n",
        "    }\n",
        "\n",
        "  location_answers_dict = dict(zip(location_df['Answer'], location_df['Question']))\n",
        "  location_infos_wikidata = {}\n",
        "\n",
        "  sol = create_hint_sentences(location_answers_dict, locations_identifiers_dict, properties_blank_sentences_locations)\n",
        "\n",
        "  inter = {}\n",
        "  ret={}\n",
        "\n",
        "  for key, value in sol.items():\n",
        "    for answer,question in location_answers_dict.items():\n",
        "      if key == answer:\n",
        "        for code, sentence in value.items():\n",
        "          sim_score = get_similarity_score(question,sentence)\n",
        "          inter[code]  = {sentence : sim_score}\n",
        "        ret[key] = inter\n",
        "\n",
        "  return ret\n"
      ],
      "metadata": {
        "id": "K-0FB_Zlzbcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # location_hints_fixed_properties = get_location_hints_fixed_properties()\n",
        "  # pprint.pprint(location_hints_fixed_properties, indent=1, sort_dicts=False)"
      ],
      "metadata": {
        "id": "3O33NbMOVBh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Generate hints - (putting everything together)**"
      ],
      "metadata": {
        "id": "7CedDGQxiBpG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Function that puts everything together\n",
        "# Call hint-generation-functions for years, people and locations\n",
        "# Save the results in a txt file\n",
        "\n",
        "def generate_hints_from_xlsx():\n",
        "  generated_hint_sentences = {}\n",
        "  combined_hint_sentences = {}\n",
        "\n",
        "  #generates the hints for the YEARS question\n",
        "  years_hints = generate_hints_years()\n",
        "  generated_hint_sentences['years'] = years_hints\n",
        "  #pprint.pprint(years_hints, indent=1, sort_dicts=False)\n",
        "\n",
        "  #generates the hints for the PEOPLE question\n",
        "  people_hints_unexpected_categories = get_person_hints_unexpected_categories()\n",
        "  people_hints_unexpected_predicates = get_person_hints_unexpected_predicates()\n",
        "  people_hints = {}\n",
        "  people_hints['categories'] = people_hints_unexpected_categories\n",
        "  people_hints['predicates'] = people_hints_unexpected_predicates\n",
        "  generated_hint_sentences['people'] = people_hints\n",
        "  pprint.pprint(people_hints, indent=1, sort_dicts=False)\n",
        "\n",
        "  #generates the hints for the LOCATION question\n",
        "  location_hints_unexpected_categories = get_location_hints_unexpected_categories()\n",
        "  location_hints_fixed_properties = get_location_hints_fixed_properties()\n",
        "  location_hints = {}\n",
        "  location_hints['categories'] = location_hints_unexpected_categories\n",
        "  location_hints['properties'] = location_hints_fixed_properties\n",
        "  generated_hint_sentences['locations'] = location_hints\n",
        "  pprint.pprint(location_hints, indent=1, sort_dicts=False)\n",
        "\n",
        "  return generated_hint_sentences"
      ],
      "metadata": {
        "id": "vMMsZn0NiQXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generated_hint_sentences = {}\n",
        "# combined_hint_sentences = {}\n",
        "\n",
        "# #generates the hints for the YEARS question\n",
        "# years_hints = generate_hints_years()\n",
        "# generated_hint_sentences['years'] = years_hints\n",
        "# pprint.pprint(years_hints, indent=1, sort_dicts=False)\n"
      ],
      "metadata": {
        "id": "HGI5Jl-mVlsY",
        "outputId": "91431492-9e20-4312-a82a-41fab8460f59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1994: {'sports': {'p_f1': {'In the previous year, Alain Prost has won the F1 Drivers World Championship.': 0.7816421985626221},\n",
            "                   'worlds': {'In the same year, Brazil has won the FIFA World Cup.': 0.6666070222854614},\n",
            "                   'cl': {'In the same year, AC Milan has won the UEFA Champions League.': 0.6647369861602783},\n",
            "                   'f_cl': {'In the following year, Ajax has won the UEFA Champions League.': 0.6391717791557312},\n",
            "                   'p_cl': {'In the previous year, Marseille has won the UEFA Champions League.': 0.6317457556724548}},\n",
            "        'thumbcaption': {'In the same year, the Kaiser Permanente building after the  Northridge earthquake': 0.5455725193023682},\n",
            "        'vizgr': {'1994/05/01': {'In the same year, three-time Formula One world champion Ayrton Senna is killed in an accident during the San Marino Grand Prix in Imola, Italy.': 0.7630124092102051},\n",
            "                  '1994/04/30': {'In the same year, formula One driver Roland Ratzenberger is killed while qualifying for the  San Marino Grand Prix.': 0.7324753999710083},\n",
            "                  '1994/04/06': {'In the same year, kurt Cobain, songwriter and frontman for the band Nirvana, is found dead at his Lake Washington home, apparently of a single self-inflicted gunshot wound.': 0.6706864833831787},\n",
            "                  '1994/09/05': {\"In the same year, new South Wales State MP for Cabramatta John Newman is shot outside his home, in Australia's first political assassination since 1977.\": 0.6559743881225586},\n",
            "                  '1994/07/19': {'In the same year, four 26-pound ceiling tiles fall from the roof of the Kingdome in Seattle, Washington, just hours before a scheduled Seattle Mariners game.': 0.6421981453895569}},\n",
            "        'question': 'When did Michael Schumacher win his first F1 World '\n",
            "                    'Drivers Title?'},\n",
            " 2004: {'sports': {'f_f1': {'In the following year, Fernando Alonso has won the F1 Drivers World Championship.': 0.7957704067230225},\n",
            "                   'f_cl': {'In the following year, Liverpool has won the UEFA Champions League.': 0.6432414054870605},\n",
            "                   'p_cl': {'In the previous year, AC Milan has won the UEFA Champions League.': 0.6365283131599426},\n",
            "                   'euros': {'In the same year, Greece has won the UEFA Euro Football Championship.': 0.6051347255706787},\n",
            "                   'cl': {'In the same year, Porto has won the UEFA Champions League.': 0.6031892895698547}},\n",
            "        'thumbcaption': {'In the same year, the  transit of Venus, the first such occurrence since 1882': 0.5443825721740723},\n",
            "        'vizgr': {'2004/03/12': {'In the same year, russian presidential election, : Vladimir Putin easily wins a second term.': 0.633924663066864},\n",
            "                  '2004/05/15': {'In the same year, south Africa is awarded the 2010 FIFA World Cup.': 0.6325787305831909},\n",
            "                  '2004/12/14': {\"In the same year, the world's tallest bridge, the Millau bridge over the River Tarn in the Massif Central mountains, France, is opened by President Jacques Chirac.\": 0.6242968440055847},\n",
            "                  '2004/09/29': {\"In the same year, in Mojave, California, the first Ansari X-Prize flight takes place of SpaceShipOne, which is competing with a number of spacecraft (including Canada's Da Vinci Project, claimed to be its closest rival) and goes on to win the prize on October 4.\": 0.6223667860031128},\n",
            "                  '2004/05/04': {'In the same year, the Toronto Maple Leafs played their last NHL playoff game.': 0.6157877445220947}},\n",
            "        'question': 'When did Michael Schumacher win his last F1 World Drivers '\n",
            "                    'Title?'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #generates the hints for the PEOPLE question\n",
        "# people_hints_unexpected_categories = get_person_hints_unexpected_categories()\n",
        "# people_hints_unexpected_predicates = get_person_hints_unexpected_predicates()\n",
        "# people_hints = {}\n",
        "# people_hints['categories'] = people_hints_unexpected_categories\n",
        "# people_hints['predicates'] = people_hints_unexpected_predicates\n",
        "# generated_hint_sentences['people'] = people_hints\n",
        "# pprint.pprint(people_hints, indent=1, sort_dicts=False)"
      ],
      "metadata": {
        "id": "2T2rWFo5Vsrr",
        "outputId": "42625e4a-e07a-4356-bf3d-96fd41056f62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'categories': {'Richard Hammond': {'The person you are looking for is/was occupied as television presenter and a member of the category English television presenters': 0.5718711018562317},\n",
            "                'Max Verstappen': {'The person you are looking for is/was occupied as racing automobile driver and a member of the category Twitch (service) streamers': 0.6192768812179565},\n",
            "                'Jeremy Clarkson': {'The person you are looking for is/was occupied as journalist and a member of the category 1960 births': 0.5250772833824158}},\n",
            " 'predicates': {'Richard Hammond': {'occupation': {'The person you are looking for, is occupied as television presenter, writer and journalist.': 0.5875199437141418},\n",
            "                                    'notable work': {'The person you are looking for was involved in some very notable works, like: Top Gear.': 0.5896652936935425},\n",
            "                                    'date of birth + place of birth': {'The person you are looking for was born on 19.12.1969 in Solihull.': 0.49674224853515625}},\n",
            "                'Max Verstappen': {'occupation': {'The person you are looking for, is occupied as racing automobile driver, Formula One driver and racing driver.': 0.6964481472969055},\n",
            "                                   'award received': {'The person you are looking for has won multiple awards in his life, some of them are Talent of the year, Dutch Sportsman of the year and Lorenzo Bandini Trophy.': 0.6143848896026611},\n",
            "                                   'native language': {'The person you are looking for, speaks Dutch.': 0.5818939208984375},\n",
            "                                   'date of birth + place of birth': {'The person you are looking for was born on 30.09.1997 in Hasselt.': 0.5591246485710144}},\n",
            "                'Jeremy Clarkson': {'occupation': {'The person you are looking for, is occupied as journalist, television presenter and writer.': 0.5885734558105469},\n",
            "                                    'notable work': {'The person you are looking for was involved in some very notable works, like: Top Gear and The Grand Tour.': 0.6211810111999512},\n",
            "                                    'native language': {'The person you are looking for, speaks English.': 0.5987207293510437},\n",
            "                                    'date of birth + place of birth': {'The person you are looking for was born on 11.04.1960 in Doncaster.': 0.5190430283546448}}}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #generates the hints for the LOCATION question\n",
        "# location_hints_unexpected_categories = get_location_hints_unexpected_categories()\n",
        "# location_hints_fixed_properties = get_location_hints_fixed_properties()\n",
        "# location_hints = {}\n",
        "# location_hints['categories'] = location_hints_unexpected_categories\n",
        "# location_hints['properties'] = location_hints_fixed_properties\n",
        "# generated_hint_sentences['locations'] = location_hints\n",
        "# pprint.pprint(location_hints, indent=1, sort_dicts=False)"
      ],
      "metadata": {
        "id": "718W-bsPVtgL",
        "outputId": "fddeaa85-2168-41b3-b662-12226db668e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Austria': OrderedDict([('Category:Countries in Europe', 76851),\n",
            "                         ('Category:Member states of the United Nations',\n",
            "                          14261),\n",
            "                         ('Category:Landlocked countries', 2931),\n",
            "                         ('Category:Member states of the European Union', 2292),\n",
            "                         ('Category:Austria', 1264),\n",
            "                         ('Category:Member states of the Union for the '\n",
            "                          'Mediterranean',\n",
            "                          1068),\n",
            "                         ('Category:Federal constitutional republics', 637),\n",
            "                         ('Category:Member states of the Three Seas Initiative',\n",
            "                          392),\n",
            "                         ('Category:Nuclear-free zones', 111),\n",
            "                         ('Category:States and territories established in 1955',\n",
            "                          355),\n",
            "                         ('Category:OECD members', 285)]),\n",
            " 'Innsbruck': OrderedDict([('Category:Austrian state capitals', 387),\n",
            "                           ('Category:Cities and towns in Tyrol (state)', 369),\n",
            "                           ('Category:Innsbruck', 112),\n",
            "                           ('Category:Districts of Tyrol (state)', 78),\n",
            "                           ('Category:Populated places on the Inn (river)',\n",
            "                            73)])}\n",
            "{'Austria': OrderedDict([('Category:Member states of the United Nations',\n",
            "                          (3, 14261, ['Japan', 'Canada', 'Australia'])),\n",
            "                         ('Category:OECD members',\n",
            "                          (2, 285, ['Japan', 'Australia'])),\n",
            "                         ('Category:Countries in Europe',\n",
            "                          (1, 76851, ['Germany'])),\n",
            "                         ('Category:Member states of the European Union',\n",
            "                          (1, 2292, ['Germany']))]),\n",
            " 'Innsbruck': OrderedDict([('Category:Austrian state capitals', (0, 387, [])),\n",
            "                           ('Category:Cities and towns in Tyrol (state)',\n",
            "                            (0, 369, [])),\n",
            "                           ('Category:Innsbruck', (0, 112, [])),\n",
            "                           ('Category:Districts of Tyrol (state)', (0, 78, [])),\n",
            "                           ('Category:Populated places on the Inn (river)',\n",
            "                            (0, 73, []))])}\n",
            "categories_scores_dict\n",
            "{'Austria': OrderedDict([('Category:Countries in Europe', 1402530.75),\n",
            "                         ('Category:Member states of the United Nations',\n",
            "                          260263.25),\n",
            "                         ('Category:Member states of the European Union',\n",
            "                          41829.0),\n",
            "                         ('Category:OECD members', 5201.25)]),\n",
            " 'Innsbruck': OrderedDict([('Category:Austrian state capitals', 0),\n",
            "                           ('Category:Cities and towns in Tyrol (state)', 0),\n",
            "                           ('Category:Innsbruck', 0),\n",
            "                           ('Category:Districts of Tyrol (state)', 0),\n",
            "                           ('Category:Populated places on the Inn (river)',\n",
            "                            0)])}\n",
            "{'Austria': ['The location you are looking for is/was a member of the category '\n",
            "             'Countries in Europe'],\n",
            " 'Innsbruck': ['The location you are looking for is/was a member of the '\n",
            "               'category Austrian state capitals']}\n",
            "{'categories': {'Austria': {'The location you are looking for is/was a member of the category Countries in Europe': 0.701798677444458},\n",
            "                'Innsbruck': {'The location you are looking for is/was a member of the category Austrian state capitals': 0.6820323467254639}},\n",
            " 'properties': {'Austria': {'P610': {'The highest point in searched location is Grossglockner.': 0.6264255046844482},\n",
            "                            'P6': {'Head of government of searched location is Karl Nehammer.': 0.5802534818649292},\n",
            "                            'P17': {'The searched location is in country Austria.': 0.6002004742622375},\n",
            "                            'P30': {'The searched location is on continent Europe.': 0.5868927836418152},\n",
            "                            'P35': {'Head of state of searched location is Alexander Van der Bellen.': 0.5581653118133545},\n",
            "                            'P36': {'The capital of searched location is  Vienna.': 0.6519556641578674},\n",
            "                            'P38': {'The currencies used in searched location is/are  Reichsmark, Austrian schilling, Austrian schilling.': 0.5665154457092285},\n",
            "                            'P37': {'Spoken language(s) in searched location is/are Austrian Sign Language, German.': 0.5837541222572327},\n",
            "                            'P47': {'The searched location shares border with West Germany, Liechtenstein, Germany.': 0.5691820979118347},\n",
            "                            'P131': {'The searched city is located on  Tyrol.': 0.682868480682373},\n",
            "                            'P206': {'Some of the next bodies of water of searched location are Inn, Rhine, Lake Constance.': 0.5883907675743103},\n",
            "                            'P421': {'The searched location is in time-zone UTC+01:00, Europe/Vienna, Central European Time.': 0.5101152658462524},\n",
            "                            'P463': {'Searched location is member of Universal Postal Union, Nuclear Suppliers Group, International Centre for Migration Policy Development.': 0.5278304219245911},\n",
            "                            'P793': {'A significant event happened in this location was 1964 Winter Olympics, 1976 Winter Olympics.': 0.45144885778427124},\n",
            "                            'P1082': {'The searched location has a population of +8979894.': 0.5292630195617676},\n",
            "                            'P1376': {'The searched city is capital of  Innsbruck-Land District.': 0.6342917680740356},\n",
            "                            'P1830': {'The searched location is owner of Möslalm.': 0.5257034301757812}},\n",
            "                'Innsbruck': {'P610': {'The highest point in searched location is Grossglockner.': 0.6264255046844482},\n",
            "                              'P6': {'Head of government of searched location is Karl Nehammer.': 0.5802534818649292},\n",
            "                              'P17': {'The searched location is in country Austria.': 0.6002004742622375},\n",
            "                              'P30': {'The searched location is on continent Europe.': 0.5868927836418152},\n",
            "                              'P35': {'Head of state of searched location is Alexander Van der Bellen.': 0.5581653118133545},\n",
            "                              'P36': {'The capital of searched location is  Vienna.': 0.6519556641578674},\n",
            "                              'P38': {'The currencies used in searched location is/are  Reichsmark, Austrian schilling, Austrian schilling.': 0.5665154457092285},\n",
            "                              'P37': {'Spoken language(s) in searched location is/are Austrian Sign Language, German.': 0.5837541222572327},\n",
            "                              'P47': {'The searched location shares border with West Germany, Liechtenstein, Germany.': 0.5691820979118347},\n",
            "                              'P131': {'The searched city is located on  Tyrol.': 0.682868480682373},\n",
            "                              'P206': {'Some of the next bodies of water of searched location are Inn, Rhine, Lake Constance.': 0.5883907675743103},\n",
            "                              'P421': {'The searched location is in time-zone UTC+01:00, Europe/Vienna, Central European Time.': 0.5101152658462524},\n",
            "                              'P463': {'Searched location is member of Universal Postal Union, Nuclear Suppliers Group, International Centre for Migration Policy Development.': 0.5278304219245911},\n",
            "                              'P793': {'A significant event happened in this location was 1964 Winter Olympics, 1976 Winter Olympics.': 0.45144885778427124},\n",
            "                              'P1082': {'The searched location has a population of +8979894.': 0.5292630195617676},\n",
            "                              'P1376': {'The searched city is capital of  Innsbruck-Land District.': 0.6342917680740356},\n",
            "                              'P1830': {'The searched location is owner of Möslalm.': 0.5257034301757812}}}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#takes ca 1:20h for 3 people, 2 years, 2 locations\n",
        "generated_hint_sentences = generate_hints_from_xlsx()\n",
        "pprint.pprint(generated_hint_sentences, indent=1, sort_dicts=False)"
      ],
      "metadata": {
        "id": "6gwFueIomIdd",
        "outputId": "d8332466-da71-4984-8105-347c4ac940b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'categories': {'Richard Hammond': {'The person you are looking for is/was occupied as television presenter and a member of the category English television presenters': 0.5718711018562317},\n",
            "                'Max Verstappen': {'The person you are looking for is/was occupied as racing automobile driver and a member of the category Twitch (service) streamers': 0.6192768812179565},\n",
            "                'Jeremy Clarkson': {'The person you are looking for is/was occupied as journalist and a member of the category 1960 births': 0.5250772833824158}},\n",
            " 'predicates': {'Richard Hammond': {'occupation': {'The person you are looking for, is occupied as television presenter, writer and journalist.': 0.5875199437141418},\n",
            "                                    'notable work': {'The person you are looking for was involved in some very notable works, like: Top Gear.': 0.5896652936935425},\n",
            "                                    'date of birth + place of birth': {'The person you are looking for was born on 19.12.1969 in Solihull.': 0.49674224853515625}},\n",
            "                'Max Verstappen': {'occupation': {'The person you are looking for, is occupied as racing automobile driver, Formula One driver and racing driver.': 0.6964481472969055},\n",
            "                                   'award received': {'The person you are looking for has won multiple awards in his life, some of them are Talent of the year, Dutch Sportsman of the year and Lorenzo Bandini Trophy.': 0.6143848896026611},\n",
            "                                   'native language': {'The person you are looking for, speaks Dutch.': 0.5818939208984375},\n",
            "                                   'date of birth + place of birth': {'The person you are looking for was born on 30.09.1997 in Hasselt.': 0.5591246485710144}},\n",
            "                'Jeremy Clarkson': {'occupation': {'The person you are looking for, is occupied as journalist, television presenter and writer.': 0.5885734558105469},\n",
            "                                    'notable work': {'The person you are looking for was involved in some very notable works, like: Top Gear and The Grand Tour.': 0.6211810111999512},\n",
            "                                    'native language': {'The person you are looking for, speaks English.': 0.5987207293510437},\n",
            "                                    'date of birth + place of birth': {'The person you are looking for was born on 11.04.1960 in Doncaster.': 0.5190430283546448}}}}\n",
            "{'Austria': OrderedDict([('Category:Countries in Europe', 76851),\n",
            "                         ('Category:Member states of the United Nations',\n",
            "                          14261),\n",
            "                         ('Category:Landlocked countries', 2931),\n",
            "                         ('Category:Member states of the European Union', 2292),\n",
            "                         ('Category:Austria', 1264),\n",
            "                         ('Category:Member states of the Union for the '\n",
            "                          'Mediterranean',\n",
            "                          1068),\n",
            "                         ('Category:Federal constitutional republics', 637),\n",
            "                         ('Category:Member states of the Three Seas Initiative',\n",
            "                          392),\n",
            "                         ('Category:Nuclear-free zones', 111),\n",
            "                         ('Category:States and territories established in 1955',\n",
            "                          355),\n",
            "                         ('Category:OECD members', 285)]),\n",
            " 'Innsbruck': OrderedDict([('Category:Austrian state capitals', 387),\n",
            "                           ('Category:Cities and towns in Tyrol (state)', 369),\n",
            "                           ('Category:Innsbruck', 112),\n",
            "                           ('Category:Districts of Tyrol (state)', 78),\n",
            "                           ('Category:Populated places on the Inn (river)',\n",
            "                            73)])}\n",
            "{'Austria': OrderedDict([('Category:Member states of the United Nations',\n",
            "                          (3, 14261, ['Japan', 'Canada', 'Australia'])),\n",
            "                         ('Category:OECD members',\n",
            "                          (2, 285, ['Japan', 'Australia'])),\n",
            "                         ('Category:Countries in Europe',\n",
            "                          (1, 76851, ['Germany'])),\n",
            "                         ('Category:Member states of the European Union',\n",
            "                          (1, 2292, ['Germany']))]),\n",
            " 'Innsbruck': OrderedDict([('Category:Austrian state capitals', (0, 387, [])),\n",
            "                           ('Category:Cities and towns in Tyrol (state)',\n",
            "                            (0, 369, [])),\n",
            "                           ('Category:Innsbruck', (0, 112, [])),\n",
            "                           ('Category:Districts of Tyrol (state)', (0, 78, [])),\n",
            "                           ('Category:Populated places on the Inn (river)',\n",
            "                            (0, 73, []))])}\n",
            "categories_scores_dict\n",
            "{'Austria': OrderedDict([('Category:Countries in Europe', 1402530.75),\n",
            "                         ('Category:Member states of the United Nations',\n",
            "                          260263.25),\n",
            "                         ('Category:Member states of the European Union',\n",
            "                          41829.0),\n",
            "                         ('Category:OECD members', 5201.25)]),\n",
            " 'Innsbruck': OrderedDict([('Category:Austrian state capitals', 0),\n",
            "                           ('Category:Cities and towns in Tyrol (state)', 0),\n",
            "                           ('Category:Innsbruck', 0),\n",
            "                           ('Category:Districts of Tyrol (state)', 0),\n",
            "                           ('Category:Populated places on the Inn (river)',\n",
            "                            0)])}\n",
            "{'Austria': ['The location you are looking for is/was a member of the category '\n",
            "             'Countries in Europe'],\n",
            " 'Innsbruck': ['The location you are looking for is/was a member of the '\n",
            "               'category Austrian state capitals']}\n",
            "{'categories': {'Austria': {'The location you are looking for is/was a member of the category Countries in Europe': 0.701798677444458},\n",
            "                'Innsbruck': {'The location you are looking for is/was a member of the category Austrian state capitals': 0.6820323467254639}},\n",
            " 'properties': {'Austria': {'P610': {'The highest point in searched location is Grossglockner.': 0.6264255046844482},\n",
            "                            'P6': {'Head of government of searched location is Karl Nehammer.': 0.5802534818649292},\n",
            "                            'P17': {'The searched location is in country Austria.': 0.6002004742622375},\n",
            "                            'P30': {'The searched location is on continent Europe.': 0.5868927836418152},\n",
            "                            'P35': {'Head of state of searched location is Alexander Van der Bellen.': 0.5581653118133545},\n",
            "                            'P36': {'The capital of searched location is  Vienna.': 0.6519556641578674},\n",
            "                            'P38': {'The currencies used in searched location is/are  euro, Austrian schilling, Austrian schilling.': 0.5762724876403809},\n",
            "                            'P37': {'Spoken language(s) in searched location is/are Austrian Sign Language, German.': 0.5837541222572327},\n",
            "                            'P47': {'The searched location shares border with Slovakia, Italy, West Germany.': 0.5669173002243042},\n",
            "                            'P131': {'The searched city is located on  Tyrol.': 0.682868480682373},\n",
            "                            'P206': {'Some of the next bodies of water of searched location are Thaya, Inn, Neusiedl Lake.': 0.589394211769104},\n",
            "                            'P421': {'The searched location is in time-zone Europe/Vienna, UTC+02:00, UTC+01:00.': 0.4585426151752472},\n",
            "                            'P463': {'Searched location is member of Asian Development Bank, European Union, Visa Waiver Program.': 0.4811599552631378},\n",
            "                            'P793': {'A significant event happened in this location was 1976 Winter Olympics, 1964 Winter Olympics.': 0.4614744782447815},\n",
            "                            'P1082': {'The searched location has a population of +8979894.': 0.5292630195617676},\n",
            "                            'P1376': {'The searched city is capital of  Innsbruck-Land District.': 0.6342917680740356},\n",
            "                            'P1830': {'The searched location is owner of Möslalm.': 0.5257034301757812}},\n",
            "                'Innsbruck': {'P610': {'The highest point in searched location is Grossglockner.': 0.6264255046844482},\n",
            "                              'P6': {'Head of government of searched location is Karl Nehammer.': 0.5802534818649292},\n",
            "                              'P17': {'The searched location is in country Austria.': 0.6002004742622375},\n",
            "                              'P30': {'The searched location is on continent Europe.': 0.5868927836418152},\n",
            "                              'P35': {'Head of state of searched location is Alexander Van der Bellen.': 0.5581653118133545},\n",
            "                              'P36': {'The capital of searched location is  Vienna.': 0.6519556641578674},\n",
            "                              'P38': {'The currencies used in searched location is/are  euro, Austrian schilling, Austrian schilling.': 0.5762724876403809},\n",
            "                              'P37': {'Spoken language(s) in searched location is/are Austrian Sign Language, German.': 0.5837541222572327},\n",
            "                              'P47': {'The searched location shares border with Slovakia, Italy, West Germany.': 0.5669173002243042},\n",
            "                              'P131': {'The searched city is located on  Tyrol.': 0.682868480682373},\n",
            "                              'P206': {'Some of the next bodies of water of searched location are Thaya, Inn, Neusiedl Lake.': 0.589394211769104},\n",
            "                              'P421': {'The searched location is in time-zone Europe/Vienna, UTC+02:00, UTC+01:00.': 0.4585426151752472},\n",
            "                              'P463': {'Searched location is member of Asian Development Bank, European Union, Visa Waiver Program.': 0.4811599552631378},\n",
            "                              'P793': {'A significant event happened in this location was 1976 Winter Olympics, 1964 Winter Olympics.': 0.4614744782447815},\n",
            "                              'P1082': {'The searched location has a population of +8979894.': 0.5292630195617676},\n",
            "                              'P1376': {'The searched city is capital of  Innsbruck-Land District.': 0.6342917680740356},\n",
            "                              'P1830': {'The searched location is owner of Möslalm.': 0.5257034301757812}}}}\n",
            "{'years': {1994: {'sports': {'p_f1': {'In the previous year, Alain Prost has won the F1 Drivers World Championship.': 0.7816421985626221},\n",
            "                             'worlds': {'In the same year, Brazil has won the FIFA World Cup.': 0.6666070222854614},\n",
            "                             'cl': {'In the same year, AC Milan has won the UEFA Champions League.': 0.6647369861602783},\n",
            "                             'f_cl': {'In the following year, Ajax has won the UEFA Champions League.': 0.6391717791557312},\n",
            "                             'p_cl': {'In the previous year, Marseille has won the UEFA Champions League.': 0.6317457556724548}},\n",
            "                  'thumbcaption': {'In the same year, the Kaiser Permanente building after the  Northridge earthquake': 0.5455725193023682},\n",
            "                  'vizgr': {'1994/05/01': {'In the same year, three-time Formula One world champion Ayrton Senna is killed in an accident during the San Marino Grand Prix in Imola, Italy.': 0.7630124092102051},\n",
            "                            '1994/04/30': {'In the same year, formula One driver Roland Ratzenberger is killed while qualifying for the  San Marino Grand Prix.': 0.7324753999710083},\n",
            "                            '1994/04/06': {'In the same year, kurt Cobain, songwriter and frontman for the band Nirvana, is found dead at his Lake Washington home, apparently of a single self-inflicted gunshot wound.': 0.6706864833831787},\n",
            "                            '1994/09/05': {\"In the same year, new South Wales State MP for Cabramatta John Newman is shot outside his home, in Australia's first political assassination since 1977.\": 0.6559743881225586},\n",
            "                            '1994/07/19': {'In the same year, four 26-pound ceiling tiles fall from the roof of the Kingdome in Seattle, Washington, just hours before a scheduled Seattle Mariners game.': 0.6421981453895569}},\n",
            "                  'question': 'When did Michael Schumacher win his first F1 '\n",
            "                              'World Drivers Title?'},\n",
            "           2004: {'sports': {'f_f1': {'In the following year, Fernando Alonso has won the F1 Drivers World Championship.': 0.7957704067230225},\n",
            "                             'f_cl': {'In the following year, Liverpool has won the UEFA Champions League.': 0.6432414054870605},\n",
            "                             'p_cl': {'In the previous year, AC Milan has won the UEFA Champions League.': 0.6365283131599426},\n",
            "                             'euros': {'In the same year, Greece has won the UEFA Euro Football Championship.': 0.6051347255706787},\n",
            "                             'cl': {'In the same year, Porto has won the UEFA Champions League.': 0.6031892895698547}},\n",
            "                  'thumbcaption': {'In the same year, the  transit of Venus, the first such occurrence since 1882': 0.5443825721740723},\n",
            "                  'vizgr': {'2004/03/12': {'In the same year, russian presidential election, : Vladimir Putin easily wins a second term.': 0.633924663066864},\n",
            "                            '2004/05/15': {'In the same year, south Africa is awarded the 2010 FIFA World Cup.': 0.6325787305831909},\n",
            "                            '2004/12/14': {\"In the same year, the world's tallest bridge, the Millau bridge over the River Tarn in the Massif Central mountains, France, is opened by President Jacques Chirac.\": 0.6242968440055847},\n",
            "                            '2004/09/29': {\"In the same year, in Mojave, California, the first Ansari X-Prize flight takes place of SpaceShipOne, which is competing with a number of spacecraft (including Canada's Da Vinci Project, claimed to be its closest rival) and goes on to win the prize on October 4.\": 0.6223667860031128},\n",
            "                            '2004/05/04': {'In the same year, the Toronto Maple Leafs played their last NHL playoff game.': 0.6157877445220947}},\n",
            "                  'question': 'When did Michael Schumacher win his last F1 '\n",
            "                              'World Drivers Title?'}},\n",
            " 'people': {'categories': {'Richard Hammond': {'The person you are looking for is/was occupied as television presenter and a member of the category English television presenters': 0.5718711018562317},\n",
            "                           'Max Verstappen': {'The person you are looking for is/was occupied as racing automobile driver and a member of the category Twitch (service) streamers': 0.6192768812179565},\n",
            "                           'Jeremy Clarkson': {'The person you are looking for is/was occupied as journalist and a member of the category 1960 births': 0.5250772833824158}},\n",
            "            'predicates': {'Richard Hammond': {'occupation': {'The person you are looking for, is occupied as television presenter, writer and journalist.': 0.5875199437141418},\n",
            "                                               'notable work': {'The person you are looking for was involved in some very notable works, like: Top Gear.': 0.5896652936935425},\n",
            "                                               'date of birth + place of birth': {'The person you are looking for was born on 19.12.1969 in Solihull.': 0.49674224853515625}},\n",
            "                           'Max Verstappen': {'occupation': {'The person you are looking for, is occupied as racing automobile driver, Formula One driver and racing driver.': 0.6964481472969055},\n",
            "                                              'award received': {'The person you are looking for has won multiple awards in his life, some of them are Talent of the year, Dutch Sportsman of the year and Lorenzo Bandini Trophy.': 0.6143848896026611},\n",
            "                                              'native language': {'The person you are looking for, speaks Dutch.': 0.5818939208984375},\n",
            "                                              'date of birth + place of birth': {'The person you are looking for was born on 30.09.1997 in Hasselt.': 0.5591246485710144}},\n",
            "                           'Jeremy Clarkson': {'occupation': {'The person you are looking for, is occupied as journalist, television presenter and writer.': 0.5885734558105469},\n",
            "                                               'notable work': {'The person you are looking for was involved in some very notable works, like: Top Gear and The Grand Tour.': 0.6211810111999512},\n",
            "                                               'native language': {'The person you are looking for, speaks English.': 0.5987207293510437},\n",
            "                                               'date of birth + place of birth': {'The person you are looking for was born on 11.04.1960 in Doncaster.': 0.5190430283546448}}}},\n",
            " 'locations': {'categories': {'Austria': {'The location you are looking for is/was a member of the category Countries in Europe': 0.701798677444458},\n",
            "                              'Innsbruck': {'The location you are looking for is/was a member of the category Austrian state capitals': 0.6820323467254639}},\n",
            "               'properties': {'Austria': {'P610': {'The highest point in searched location is Grossglockner.': 0.6264255046844482},\n",
            "                                          'P6': {'Head of government of searched location is Karl Nehammer.': 0.5802534818649292},\n",
            "                                          'P17': {'The searched location is in country Austria.': 0.6002004742622375},\n",
            "                                          'P30': {'The searched location is on continent Europe.': 0.5868927836418152},\n",
            "                                          'P35': {'Head of state of searched location is Alexander Van der Bellen.': 0.5581653118133545},\n",
            "                                          'P36': {'The capital of searched location is  Vienna.': 0.6519556641578674},\n",
            "                                          'P38': {'The currencies used in searched location is/are  euro, Austrian schilling, Austrian schilling.': 0.5762724876403809},\n",
            "                                          'P37': {'Spoken language(s) in searched location is/are Austrian Sign Language, German.': 0.5837541222572327},\n",
            "                                          'P47': {'The searched location shares border with Slovakia, Italy, West Germany.': 0.5669173002243042},\n",
            "                                          'P131': {'The searched city is located on  Tyrol.': 0.682868480682373},\n",
            "                                          'P206': {'Some of the next bodies of water of searched location are Thaya, Inn, Neusiedl Lake.': 0.589394211769104},\n",
            "                                          'P421': {'The searched location is in time-zone Europe/Vienna, UTC+02:00, UTC+01:00.': 0.4585426151752472},\n",
            "                                          'P463': {'Searched location is member of Asian Development Bank, European Union, Visa Waiver Program.': 0.4811599552631378},\n",
            "                                          'P793': {'A significant event happened in this location was 1976 Winter Olympics, 1964 Winter Olympics.': 0.4614744782447815},\n",
            "                                          'P1082': {'The searched location has a population of +8979894.': 0.5292630195617676},\n",
            "                                          'P1376': {'The searched city is capital of  Innsbruck-Land District.': 0.6342917680740356},\n",
            "                                          'P1830': {'The searched location is owner of Möslalm.': 0.5257034301757812}},\n",
            "                              'Innsbruck': {'P610': {'The highest point in searched location is Grossglockner.': 0.6264255046844482},\n",
            "                                            'P6': {'Head of government of searched location is Karl Nehammer.': 0.5802534818649292},\n",
            "                                            'P17': {'The searched location is in country Austria.': 0.6002004742622375},\n",
            "                                            'P30': {'The searched location is on continent Europe.': 0.5868927836418152},\n",
            "                                            'P35': {'Head of state of searched location is Alexander Van der Bellen.': 0.5581653118133545},\n",
            "                                            'P36': {'The capital of searched location is  Vienna.': 0.6519556641578674},\n",
            "                                            'P38': {'The currencies used in searched location is/are  euro, Austrian schilling, Austrian schilling.': 0.5762724876403809},\n",
            "                                            'P37': {'Spoken language(s) in searched location is/are Austrian Sign Language, German.': 0.5837541222572327},\n",
            "                                            'P47': {'The searched location shares border with Slovakia, Italy, West Germany.': 0.5669173002243042},\n",
            "                                            'P131': {'The searched city is located on  Tyrol.': 0.682868480682373},\n",
            "                                            'P206': {'Some of the next bodies of water of searched location are Thaya, Inn, Neusiedl Lake.': 0.589394211769104},\n",
            "                                            'P421': {'The searched location is in time-zone Europe/Vienna, UTC+02:00, UTC+01:00.': 0.4585426151752472},\n",
            "                                            'P463': {'Searched location is member of Asian Development Bank, European Union, Visa Waiver Program.': 0.4811599552631378},\n",
            "                                            'P793': {'A significant event happened in this location was 1976 Winter Olympics, 1964 Winter Olympics.': 0.4614744782447815},\n",
            "                                            'P1082': {'The searched location has a population of +8979894.': 0.5292630195617676},\n",
            "                                            'P1376': {'The searched city is capital of  Innsbruck-Land District.': 0.6342917680740356},\n",
            "                                            'P1830': {'The searched location is owner of Möslalm.': 0.5257034301757812}}}}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #write a function that chooses the correct hints from the generated_hint_sentences()\n",
        "# #write them into a txt file afterwards\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# def combine_hints_to_dict(generated_hint_sentences):\n",
        "#   #generated_hint_sentences = generate_hints_from_xlsx()\n",
        "#   #just use generated_hint_sentences for testinbg purposes\n",
        "#   ret = {}\n",
        "#   question = ''\n",
        "\n",
        "#   years_dict = generated_hint_sentences['years']\n",
        "#   people_dict = generated_hint_sentences['people']\n",
        "#   location_dict = generated_hint_sentences['locations']\n",
        "\n",
        "#   per_year = {}\n",
        "#   for k,v in years_dict.items():\n",
        "#     inter_years = []\n",
        "#     if (type(v) is dict) and len(v) > 1:\n",
        "#       for a,b in v.items():\n",
        "#         inter_years.append(b)\n",
        "#     else:\n",
        "#       if k == 'question':\n",
        "#         question = v\n",
        "#       else:\n",
        "#         inter_years.append(v)\n",
        "#     per_year[k] =  inter_years\n",
        "#   ret['years'] = per_year\n",
        "\n",
        "\n",
        "#   per_people = {}\n",
        "#   for k,v in people_dict.items():\n",
        "#     inter_people = []\n",
        "#     for key, value in v.items():\n",
        "#       if (type(value) is dict) and len(value) > 1:\n",
        "#         for a,b in value.items():\n",
        "#           inter_people.append(b)\n",
        "#       else:\n",
        "#         inter_people.append(value)\n",
        "#       per_people[key] =  inter_people\n",
        "#   ret['people'] = per_people\n",
        "\n",
        "#   per_location = {}\n",
        "#   for k,v in location_dict.items():\n",
        "#     inter_location = []\n",
        "#     for key, value in v.items():\n",
        "#       if (type(value) is dict) and len(value) > 1:\n",
        "#         for a,b in value.items():\n",
        "#           inter_location.append(b)\n",
        "#       else:\n",
        "#         inter_location.append(value)\n",
        "#       per_location[key] =  inter_location\n",
        "#   ret['location'] = per_location\n",
        "#   ret['question'] = question\n",
        "\n",
        "#   return ret\n",
        "\n",
        "\n",
        "# gen = generated_hint_sentences\n",
        "# combined_hint_sentences = combine_hints_to_dict(gen)\n",
        "# pprint.pprint(combined_hint_sentences)\n",
        "\n",
        "\n",
        "\n",
        "# # def final_hints_toTxt(combined_hint_sentences):\n",
        "\n"
      ],
      "metadata": {
        "id": "4MHqqulLB85f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Webapp creation with Streamlit"
      ],
      "metadata": {
        "id": "E6jWlamtxQkh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Streamlit installation\n"
      ],
      "metadata": {
        "id": "pAjqxxWMuFbn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install -q streamlit\n",
        "\n",
        "!npm install localtunnel"
      ],
      "metadata": {
        "id": "j3gpy12HtSwk"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WebApp creation"
      ],
      "metadata": {
        "id": "AQKEFyH9xosQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#here the actual webapp is written\n",
        "%%writefile automaticHintGenerationWebapp.py\n",
        "\n",
        "import streamlit as st\n",
        "\n",
        "x = st.slider('Select a value')\n",
        "st.write(x, 'squared is', x * x)"
      ],
      "metadata": {
        "id": "CFXNRCxKuJxz",
        "outputId": "7f18c11c-3861-428b-ffdc-3d20c4daec4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing automaticHintGenerationWebapp.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WebApp deployment"
      ],
      "metadata": {
        "id": "3lmJgnxhxty3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run ./automaticHintGenerationWebapp.py &>/content/logs.txt &"
      ],
      "metadata": {
        "id": "WYmQHx8AuQAA"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!npx localtunnel --port 8501 & curl ipv4.icanhazip.com"
      ],
      "metadata": {
        "id": "50pYS-gVuRtY",
        "outputId": "233f5213-6cda-4d89-916f-f767476adb1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.125.155.173\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 5.115s\n",
            "your url is: https://fluffy-ears-throw.loca.lt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jgJD4mpVxWcv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TESTING**"
      ],
      "metadata": {
        "id": "hkTRQOwXgWnM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Various testing function to try out new approaches for speed and functionality\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "qP3xADNFjA2i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Flow Chart creation for better visual"
      ],
      "metadata": {
        "id": "Dz0yt9CJJCCt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create flow charts via graphviz for python; For a visual demonstration of how the unexpected categories approach works\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "sSUWeNJ3JHEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# This cell will not display any output\n",
        "!pip install graphviz\n",
        "import graphviz\n",
        "from graphviz import Source\n",
        "from IPython.display import display"
      ],
      "metadata": {
        "id": "lOWzbs1uJLRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Here we create a UML flow-chart, to show the sequence of function calls in the get_person_hints_unexpected_categories() function.\n",
        "\"\"\"\n",
        "\n",
        "g = graphviz.Digraph('G', filename='flowchart_unexpected_categories.gv')\n",
        "g.attr(compound='true')\n",
        "g.node('Start', shape='Mdiamond')\n",
        "g.node('End', shape='Msquare')\n",
        "\n",
        "with g.subgraph(name='cluster5') as f:\n",
        "  f.node('related_people_dict[key] = get_related_people_from_person_name(key)')\n",
        "  f.node('related_people_pageviews_dict = get_pageviews_from_links(related_people_link_dict)')\n",
        "  f.node('most_popular_related_people_with_categories = get_categories_of_people_list(related_people_pageviews_dict)')\n",
        "  f.node('answer_entities_with_categories = get_categories_with_pv_answerEntities()')\n",
        "  f.node('counted_category_apperances = count_categories(most_popular_related_people_with_categories, answer_entities_with_categories)')\n",
        "  f.node('intersection_between_people_with_ae = calculate_IoU_from_countedCategoryDict(counted_category_apperances)')\n",
        "  f.node('avg_diversity_from_IoU = calculate_avg_diversity_from_IoU(intersection_between_people_with_ae)')\n",
        "  f.node('categories_scores_dict = calculate_categories_score(counted_category_apperances, avg_diversity_from_IoU)')\n",
        "  f.node('hint_sentences = create_hint_sentences_unexpected_Categs(categories_scores_dict, person_answers_dict)')\n",
        "\n",
        "  f.edge('related_people_dict[key] = get_related_people_from_person_name(key)', 'related_people_pageviews_dict = get_pageviews_from_links(related_people_link_dict)')\n",
        "  f.edge('related_people_pageviews_dict = get_pageviews_from_links(related_people_link_dict)', 'most_popular_related_people_with_categories = get_categories_of_people_list(related_people_pageviews_dict)')\n",
        "  f.edge('most_popular_related_people_with_categories = get_categories_of_people_list(related_people_pageviews_dict)', 'answer_entities_with_categories = get_categories_with_pv_answerEntities()')\n",
        "  f.edge('answer_entities_with_categories = get_categories_with_pv_answerEntities()', 'counted_category_apperances = count_categories(most_popular_related_people_with_categories, answer_entities_with_categories)')\n",
        "  f.edge('counted_category_apperances = count_categories(most_popular_related_people_with_categories, answer_entities_with_categories)', 'intersection_between_people_with_ae = calculate_IoU_from_countedCategoryDict(counted_category_apperances)')\n",
        "  f.edge('intersection_between_people_with_ae = calculate_IoU_from_countedCategoryDict(counted_category_apperances)', 'avg_diversity_from_IoU = calculate_avg_diversity_from_IoU(intersection_between_people_with_ae)')\n",
        "  f.edge('avg_diversity_from_IoU = calculate_avg_diversity_from_IoU(intersection_between_people_with_ae)', 'categories_scores_dict = calculate_categories_score(counted_category_apperances, avg_diversity_from_IoU)')\n",
        "  f.edge('categories_scores_dict = calculate_categories_score(counted_category_apperances, avg_diversity_from_IoU)', 'hint_sentences = create_hint_sentences_unexpected_Categs(categories_scores_dict, person_answers_dict)')\n",
        "\n",
        "g.edge('Start', 'related_people_dict[key] = get_related_people_from_person_name(key)', lhead='cluster5')\n",
        "g.edge('hint_sentences = create_hint_sentences_unexpected_Categs(categories_scores_dict, person_answers_dict)', 'End', ltail='cluster5')\n",
        "\n",
        "g.view()\n",
        "\n",
        "# Specify the path to your .gv file\n",
        "gv_path = \"/content/flowchart_unexpected_categories.gv\"\n",
        "# Create a Graphviz Source object\n",
        "graph = Source.from_file(gv_path, format=\"svg\")\n",
        "# Display the graph\n",
        "display(graph)"
      ],
      "metadata": {
        "id": "WhaxlVpEJRFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Here we create a shorter UML flow-chart, to show the sequence of function calls in the get_person_hints_unexpected_categories() function.\n",
        "\"\"\"\n",
        "\n",
        "g = graphviz.Digraph('G', filename='short_flowchart_unexpected_categories.gv')\n",
        "g.attr(compound='true')\n",
        "g.node('Start', shape='Mdiamond')\n",
        "g.node('End', shape='Msquare')\n",
        "\n",
        "with g.subgraph(name='cluster5') as f:\n",
        "  f.node('get_related_people_from_person_name(key)')\n",
        "  f.node('get_pageviews_from_links(related_people_link_dict)')\n",
        "  f.node('get_categories_of_people_list(related_people_pageviews_dict)')\n",
        "  f.node('get_categories_with_pv_answerEntities()')\n",
        "  f.node('count_categories(most_popular_related_people_with_categories, answer_entities_with_categories)')\n",
        "  f.node('calculate_IoU_from_countedCategoryDict(counted_category_apperances)')\n",
        "  f.node('calculate_avg_diversity_from_IoU(intersection_between_people_with_ae)')\n",
        "  f.node('calculate_categories_score(counted_category_apperances, avg_diversity_from_IoU)')\n",
        "  f.node('create_hint_sentences_unexpected_Categs(categories_scores_dict, person_answers_dict)')\n",
        "\n",
        "  f.edge('get_related_people_from_person_name(key)', 'get_pageviews_from_links(related_people_link_dict)')\n",
        "  f.edge('get_pageviews_from_links(related_people_link_dict)', 'get_categories_of_people_list(related_people_pageviews_dict)')\n",
        "  f.edge('get_categories_of_people_list(related_people_pageviews_dict)', 'get_categories_with_pv_answerEntities()')\n",
        "  f.edge('get_categories_with_pv_answerEntities()', 'count_categories(most_popular_related_people_with_categories, answer_entities_with_categories)')\n",
        "  f.edge('count_categories(most_popular_related_people_with_categories, answer_entities_with_categories)', 'calculate_IoU_from_countedCategoryDict(counted_category_apperances)')\n",
        "  f.edge('calculate_IoU_from_countedCategoryDict(counted_category_apperances)', 'calculate_avg_diversity_from_IoU(intersection_between_people_with_ae)')\n",
        "  f.edge('calculate_avg_diversity_from_IoU(intersection_between_people_with_ae)', 'calculate_categories_score(counted_category_apperances, avg_diversity_from_IoU)')\n",
        "  f.edge('calculate_categories_score(counted_category_apperances, avg_diversity_from_IoU)', 'create_hint_sentences_unexpected_Categs(categories_scores_dict, person_answers_dict)')\n",
        "\n",
        "g.edge('Start', 'get_related_people_from_person_name(key)', lhead='cluster5')\n",
        "g.edge('create_hint_sentences_unexpected_Categs(categories_scores_dict, person_answers_dict)', 'End', ltail='cluster5')\n",
        "\n",
        "g.view()\n",
        "\n",
        "# Specify the path to your .gv file\n",
        "gv_path = \"/content/short_flowchart_unexpected_categories.gv\"\n",
        "# Create a Graphviz Source object\n",
        "graph = Source.from_file(gv_path, format=\"svg\")\n",
        "# Display the graph\n",
        "display(graph)"
      ],
      "metadata": {
        "id": "pjMwGGc8Lntq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Here we create a UML flow-chart, to show the sequence of function calls in the get_person_hints_unexpected_categories() function.\n",
        "And we go a level deeper and list every function call that is made inside of the get_person_hints_unexpected_categories() function.\n",
        "\"\"\"\n",
        "\n",
        "g = graphviz.Digraph('G', filename='flowchart_unexpected_categories_cluster.gv')\n",
        "g.attr(compound='true')\n",
        "g.node('Start', shape='Mdiamond')\n",
        "g.node('End', shape='Msquare')\n",
        "\n",
        "with g.subgraph(name='cluster0') as c:\n",
        "  with c.subgraph(name='cluster1') as a:\n",
        "    a.node('related_articles = get_related_links(link)')\n",
        "    a.node('filtered_links = filter_two_word_titles(related_articles)')\n",
        "    a.node('related_people_list = check_if_person(filtered_links)')\n",
        "\n",
        "    a.edge('related_articles = get_related_links(link)', 'filtered_links = filter_two_word_titles(related_articles)')\n",
        "    a.edge('filtered_links = filter_two_word_titles(related_articles)', 'related_people_list = check_if_person(filtered_links)')\n",
        "  with c.subgraph(name='cluster2') as b:\n",
        "    b.node('pruned_link_parts_list = extract_last_parts(link_list)')\n",
        "    b.node('concat_str_for_links = concatenate_elements(pruned_link_parts_list)')\n",
        "    b.node('pageviews_url_list = combine_pv_urls(pageviews_range_url, concat_str_for_links)')\n",
        "    b.node('categories_with_pageviews =combine_dicts_from_links(pageviews_url_list)')\n",
        "    b.node('ordered_categories_with_pageviews = sort_dict_desc(categories_with_pageviews)')\n",
        "\n",
        "    b.edge('pruned_link_parts_list = extract_last_parts(link_list)', 'concat_str_for_links = concatenate_elements(pruned_link_parts_list)')\n",
        "    b.edge('concat_str_for_links = concatenate_elements(pruned_link_parts_list)', 'pageviews_url_list = combine_pv_urls(pageviews_range_url, concat_str_for_links)')\n",
        "    b.edge('pageviews_url_list = combine_pv_urls(pageviews_range_url, concat_str_for_links)', 'categories_with_pageviews =combine_dicts_from_links(pageviews_url_list)')\n",
        "    b.edge('categories_with_pageviews =combine_dicts_from_links(pageviews_url_list)', 'ordered_categories_with_pageviews = sort_dict_desc(categories_with_pageviews)')\n",
        "  with c.subgraph(name='cluster3') as d:\n",
        "    d.node('top_most_popular_people = take top 5 most pageviews people')\n",
        "    d.node('categories_of_related_people = get_categories(top_most_popular_people)')\n",
        "    d.node('categories_with_pageviews_person = get_pageviews_for_categories(categories_of_related_people)')\n",
        "    d.node('categories_with_subs_and_pageviews_person = get_dict_for_every_location(categories_of_related_people, categories_with_pageviews_person)')\n",
        "    d.node('ordered_dict_related_person= prune_and_ordered_dict(new_ordered_dict_related_person, 10)')\n",
        "\n",
        "    d.edge('top_most_popular_people = take top 5 most pageviews people', 'categories_of_related_people = get_categories(top_most_popular_people)')\n",
        "    d.edge('categories_of_related_people = get_categories(top_most_popular_people)', 'categories_with_pageviews_person = get_pageviews_for_categories(categories_of_related_people)')\n",
        "    d.edge('categories_with_pageviews_person = get_pageviews_for_categories(categories_of_related_people)', 'categories_with_subs_and_pageviews_person = get_dict_for_every_location(categories_of_related_people, categories_with_pageviews_person)')\n",
        "    d.edge('categories_with_subs_and_pageviews_person = get_dict_for_every_location(categories_of_related_people, categories_with_pageviews_person)', 'ordered_dict_related_person= prune_and_ordered_dict(new_ordered_dict_related_person, 10)')\n",
        "  with c.subgraph(name='cluster4') as e:\n",
        "    e.node('cat_ranking_person = get_categories(person_questions_dict)')\n",
        "    e.node('cat_with_pv_person = get_pageviews_for_categories(cat_ranking_person)')\n",
        "    e.node('categories_with_subs_and_pageviews_person = get_dict_for_every_location(cat_ranking_person, cat_with_pv_person)')\n",
        "\n",
        "    e.edge('cat_ranking_person = get_categories(person_questions_dict)', 'cat_with_pv_person = get_pageviews_for_categories(cat_ranking_person)')\n",
        "    e.edge('cat_with_pv_person = get_pageviews_for_categories(cat_ranking_person)', 'categories_with_subs_and_pageviews_person = get_dict_for_every_location(cat_ranking_person, cat_with_pv_person)')\n",
        "  with c.subgraph(name='cluster6') as h:\n",
        "    h.node('intersection_between_people_with_ae = calculate_IoU_from_countedCategoryDict(counted_category_apperances)')\n",
        "    h.node('avg_diversity_from_IoU = calculate_avg_diversity_from_IoU(intersection_between_people_with_ae)')\n",
        "    h.node('categories_scores_dict = calculate_categories_score(counted_category_apperances, avg_diversity_from_IoU)')\n",
        "    h.node('hint_sentences = create_hint_sentences_unexpected_Categs(categories_scores_dict, person_answers_dict)')\n",
        "\n",
        "    h.edge('intersection_between_people_with_ae = calculate_IoU_from_countedCategoryDict(counted_category_apperances)', 'avg_diversity_from_IoU = calculate_avg_diversity_from_IoU(intersection_between_people_with_ae)')\n",
        "    h.edge('avg_diversity_from_IoU = calculate_avg_diversity_from_IoU(intersection_between_people_with_ae)', 'categories_scores_dict = calculate_categories_score(counted_category_apperances, avg_diversity_from_IoU)')\n",
        "    h.edge('categories_scores_dict = calculate_categories_score(counted_category_apperances, avg_diversity_from_IoU)', 'hint_sentences = create_hint_sentences_unexpected_Categs(categories_scores_dict, person_answers_dict)')\n",
        "\n",
        "g.edge('Start', 'related_articles = get_related_links(link)', lhead='cluster1')\n",
        "g.edge('related_people_list = check_if_person(filtered_links)', 'pruned_link_parts_list = extract_last_parts(link_list)', ltail='cluster1', lhead='cluster2')\n",
        "g.edge('ordered_categories_with_pageviews = sort_dict_desc(categories_with_pageviews)', 'top_most_popular_people = take top 5 most pageviews people', ltail='cluster2', lhead='cluster3')\n",
        "g.edge('ordered_dict_related_person= prune_and_ordered_dict(new_ordered_dict_related_person, 10)', 'cat_ranking_person = get_categories(person_questions_dict)', ltail='cluster3', lhead='cluster4')\n",
        "g.edge('categories_with_subs_and_pageviews_person = get_dict_for_every_location(cat_ranking_person, cat_with_pv_person)', 'intersection_between_people_with_ae = calculate_IoU_from_countedCategoryDict(counted_category_apperances)',ltail='cluster4', lhead='cluster6')\n",
        "g.edge('hint_sentences = create_hint_sentences_unexpected_Categs(categories_scores_dict, person_answers_dict)', 'End', ltail='cluster6')\n",
        "\n",
        "g.view()\n",
        "\n",
        "# Specify the path to your .gv file\n",
        "gv_path = \"/content/flowchart_unexpected_categories_cluster.gv\"\n",
        "# Create a Graphviz Source object\n",
        "graph = Source.from_file(gv_path, format=\"svg\")\n",
        "# Display the graph\n",
        "display(graph)"
      ],
      "metadata": {
        "id": "Z4O7fnTUJTRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Here we create a UML flow-chart, to show the sequence of function calls in the get_person_hints_unexpected_categories() function.\n",
        "And we go a level deeper and list every function call that is made inside of the get_person_hints_unexpected_categories() function.\n",
        "\"\"\"\n",
        "\n",
        "g = graphviz.Digraph('G', filename='short_flowchart_unexpected_categories_cluster.gv')\n",
        "g.attr(compound='true')\n",
        "g.node('Start', shape='Mdiamond')\n",
        "g.node('End', shape='Msquare')\n",
        "\n",
        "with g.subgraph(name='cluster0') as c:\n",
        "  with c.subgraph(name='cluster1') as a:\n",
        "    a.node('get_related_links(link)')\n",
        "    a.node('filter_two_word_titles(related_articles)')\n",
        "    a.node('check_if_person(filtered_links)')\n",
        "\n",
        "    a.edge('get_related_links(link)', 'filter_two_word_titles(related_articles)')\n",
        "    a.edge('filter_two_word_titles(related_articles)', 'check_if_person(filtered_links)')\n",
        "\n",
        "  with c.subgraph(name='cluster2') as b:\n",
        "    b.node('extract_last_parts(link_list)')\n",
        "    b.node('concatenate_elements(pruned_link_parts_list)')\n",
        "    b.node('combine_pv_urls(pageviews_range_url, concat_str_for_links)')\n",
        "    b.node('combine_dicts_from_links(pageviews_url_list)')\n",
        "    b.node('sort_dict_desc(categories_with_pageviews)')\n",
        "\n",
        "    b.edge('extract_last_parts(link_list)', 'concatenate_elements(pruned_link_parts_list)')\n",
        "    b.edge('concatenate_elements(pruned_link_parts_list)', 'combine_pv_urls(pageviews_range_url, concat_str_for_links)')\n",
        "    b.edge('combine_pv_urls(pageviews_range_url, concat_str_for_links)', 'combine_dicts_from_links(pageviews_url_list)')\n",
        "    b.edge('combine_dicts_from_links(pageviews_url_list)', 'sort_dict_desc(categories_with_pageviews)')\n",
        "  with c.subgraph(name='cluster3') as d:\n",
        "    d.node('top_5most_popular_people')\n",
        "    d.node('get_categories(top_most_popular_people)')\n",
        "    d.node('get_pageviews_for_categories(categories_of_related_people)')\n",
        "    d.node('get_dict_for_every_location(categories_of_related_people, categories_with_pageviews_person)_person)')\n",
        "    d.node('prune_and_ordered_dict(new_ordered_dict_related_person, 10)')\n",
        "\n",
        "    d.edge('top_5most_popular_people', 'get_categories(top_most_popular_people)')\n",
        "    d.edge('get_categories(top_most_popular_people)', 'get_pageviews_for_categories(categories_of_related_people)')\n",
        "    d.edge('get_pageviews_for_categories(categories_of_related_people)', 'get_dict_for_every_location(categories_of_related_people, categories_with_pageviews_person)_person)')\n",
        "    d.edge('get_dict_for_every_location(categories_of_related_people, categories_with_pageviews_person)_person)', 'prune_and_ordered_dict(new_ordered_dict_related_person, 10)')\n",
        "  with c.subgraph(name='cluster4') as e:\n",
        "    e.node('get_categories(person_questions_dict)')\n",
        "    e.node('get_pageviews_for_categories(cat_ranking_person)')\n",
        "    e.node('get_dict_for_every_location(cat_ranking_person, cat_with_pv_person)')\n",
        "\n",
        "    e.edge('get_categories(person_questions_dict)', 'get_pageviews_for_categories(cat_ranking_person)')\n",
        "    e.edge('get_pageviews_for_categories(cat_ranking_person)', 'get_dict_for_every_location(cat_ranking_person, cat_with_pv_person)')\n",
        "  with c.subgraph(name='cluster6') as h:\n",
        "    h.node('calculate_IoU_from_countedCategoryDict(counted_category_apperances)')\n",
        "    h.node('calculate_avg_diversity_from_IoU(intersection_between_people_with_ae)')\n",
        "    h.node('calculate_categories_score(counted_category_apperances, avg_diversity_from_IoU)')\n",
        "    h.node('create_hint_sentences_unexpected_Categs(categories_scores_dict, person_answers_dict')\n",
        "\n",
        "    h.edge('calculate_IoU_from_countedCategoryDict(counted_category_apperances)', 'calculate_avg_diversity_from_IoU(intersection_between_people_with_ae)')\n",
        "    h.edge('calculate_avg_diversity_from_IoU(intersection_between_people_with_ae)', 'calculate_categories_score(counted_category_apperances, avg_diversity_from_IoU)')\n",
        "    h.edge('calculate_categories_score(counted_category_apperances, avg_diversity_from_IoU)', 'create_hint_sentences_unexpected_Categs(categories_scores_dict, person_answers_dict')\n",
        "\n",
        "g.edge('Start', 'get_related_links(link)', lhead='cluster1')\n",
        "g.edge('check_if_person(filtered_links)', 'extract_last_parts(link_list)', ltail='cluster1', lhead='cluster2')\n",
        "g.edge('sort_dict_desc(categories_with_pageviews)', 'top_5most_popular_people', ltail='cluster2', lhead='cluster3')\n",
        "g.edge('prune_and_ordered_dict(new_ordered_dict_related_person, 10)', 'get_categories(person_questions_dict)', ltail='cluster3', lhead='cluster4')\n",
        "g.edge('get_dict_for_every_location(cat_ranking_person, cat_with_pv_person)', 'calculate_IoU_from_countedCategoryDict(counted_category_apperances)',ltail='cluster4', lhead='cluster6')\n",
        "g.edge('create_hint_sentences_unexpected_Categs(categories_scores_dict, person_answers_dict', 'End', ltail='cluster6')\n",
        "\n",
        "g.view()\n",
        "\n",
        "# Specify the path to your .gv file\n",
        "gv_path = \"/content/short_flowchart_unexpected_categories_cluster.gv\"\n",
        "# Create a Graphviz Source object\n",
        "graph = Source.from_file(gv_path, format=\"svg\")\n",
        "# Display the graph\n",
        "display(graph)"
      ],
      "metadata": {
        "id": "mpcj5XTjMTwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "chUoW7MMOqbv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R87Y1LPF55PL"
      },
      "source": [
        "## Clean GitHub directory\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "%rm -r optimizationOfHintGeneration\n",
        "%cd /content/\n",
        "%rm hintsLocation.txt\n",
        "%rm hintsPerson.txt\n",
        "%rm resultsEvents.txt"
      ],
      "metadata": {
        "id": "2cREJk4T5v7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TESTING for function creation"
      ],
      "metadata": {
        "id": "srTixJygOrmW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The functions and function-calls below were mainly used to create the get_person_hints_unexpected_categories function"
      ],
      "metadata": {
        "id": "wnBVditQf6Bo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "1. retrieve all of the related links of a wiki-persons-page (first 500);\n",
        "2. pre prune the list, such that only entries with 2 words in the title are left; (for performance reasons)\n",
        "3. analyse the links that are left via wikipedia APi and the \"instance of\" (P31) property set to \"human\" (Q5)\n",
        "4. now we have a list of related people to compare to our answer-person-entity\n",
        "\"\"\"\n",
        "#this is just for reducing time during testing - 7m+ (30s-4m)\n",
        "\n",
        "test_person_answers_dict = dict(zip(person_df['Answer'], person_df['Question']))\n",
        "test_related_people_dict = {}\n",
        "test_related_people_link_dict= {}\n",
        "test_related_people_pageviews_dict = {}\n",
        "test_most_popular_related_people_with_categories = {}\n",
        "\n",
        "#time saving for first part (related people recovery) 2m\n",
        "for key,value in test_person_answers_dict.items():\n",
        "  test_related_people_dict[key] = get_related_people_from_person_name(key)\n",
        "#pprint.pprint(test_related_people_dict)"
      ],
      "metadata": {
        "id": "zWj3izpw2Sc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "1. retrieve all of the related links of a wiki-persons-page (first 500);\n",
        "2. pre prune the list, such that only entries with 2 words in the title are left; (for performance reasons)\n",
        "3. analyse the links that are left via wikipedia APi and the \"instance of\" (P31) property set to \"human\" (Q5)\n",
        "4. now we have a list of related people to compare to our answer-person-entity\n",
        "\"\"\"\n",
        "#this is just for reducing time during testing - 7m+ (30s-4m)\n",
        "\n",
        "test_location_answers_dict = dict(zip(location_df['Answer'], location_df['Question']))\n",
        "test_related_location_dict = {}\n",
        "test_related_location_link_dict= {}\n",
        "test_related_location_pageviews_dict = {}\n",
        "test_most_popular_related_location_with_categories = {}\n",
        "\n",
        "#time saving for first part (related people recovery) 2m\n",
        "for key,value in test_location_answers_dict.items():\n",
        "  test_related_location_dict[key] = get_related_location_from_location_name(key)\n",
        "#pprint.pprint(test_related_people_dict)"
      ],
      "metadata": {
        "id": "espv_NyokVGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#time saving for second part (related peoples with pageviews and ordering) - 16m (6-7m)\n",
        "for key,value in test_related_people_dict.items():\n",
        "  inter_link_list = []\n",
        "  for item in value:\n",
        "    inter_link_list.append(item['url'])\n",
        "  test_related_people_link_dict[key] = inter_link_list\n",
        "  test_related_people_pageviews_dict = get_pageviews_from_links(test_related_people_link_dict)\n",
        "#pprint.pprint(test_related_people_pageviews_dict)"
      ],
      "metadata": {
        "id": "r9eHU-2jk9PJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#time saving for second part (related peoples with pageviews and ordering) - 16m (6-7m)\n",
        "for key,value in test_related_location_dict.items():\n",
        "  inter_link_list = []\n",
        "  for item in value:\n",
        "    inter_link_list.append(item['url'])\n",
        "  test_related_location_link_dict[key] = inter_link_list\n",
        "  test_related_location_pageviews_dict = get_pageviews_from_links(test_related_location_link_dict)\n",
        "#pprint.pprint(test_related_people_pageviews_dict)"
      ],
      "metadata": {
        "id": "XTcSOQm2kpqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key,value in test_related_people_pageviews_dict.items():\n",
        "  related_people_orderd = dict(sorted(value.items(), key=lambda x: x[1], reverse=True))   #order the dict after the pageviews in descending order\n",
        "  top_most_popular_people = dict(itertools.islice(related_people_orderd.items(), 5))\n",
        "\n",
        "  pprint.pprint(related_people_orderd)\n",
        "  pprint.pprint(top_most_popular_people)\n",
        "\n",
        "  categories_of_related_people = get_categories(top_most_popular_people)\n",
        "  pprint.pprint(categories_of_related_people)\n",
        "  print(\"\\n\") #PAAAAASST\n",
        "\n",
        "  categories_with_pageviews_person = get_pageviews_for_categories(categories_of_related_people)\n",
        "  pprint.pprint(categories_with_pageviews_person)\n",
        "  print(\"\\n\")\n",
        "\n",
        "  categories_with_subs_and_pageviews_person = get_dict_for_every_location(categories_of_related_people, categories_with_pageviews_person)\n",
        "  pprint.pprint(categories_with_subs_and_pageviews_person)\n",
        "  print(\"\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "6uJ-9fHpe4oU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key,value in test_related_location_pageviews_dict.items():\n",
        "  related_location_orderd = dict(sorted(value.items(), key=lambda x: x[1], reverse=True))   #order the dict after the pageviews in descending order\n",
        "  top_most_popular_location = dict(itertools.islice(related_location_orderd.items(), 5))\n",
        "\n",
        "  pprint.pprint(related_location_orderd)\n",
        "  pprint.pprint(top_most_popular_location)\n",
        "\n",
        "  categories_of_related_location = get_categories(top_most_popular_location)\n",
        "  pprint.pprint(categories_of_related_location)\n",
        "  print(\"\\n\") #PAAAAASST\n",
        "\n",
        "  categories_with_pageviews_location = get_pageviews_for_categories(categories_of_related_location)\n",
        "  pprint.pprint(categories_with_pageviews_location)\n",
        "  print(\"\\n\")\n",
        "\n",
        "  categories_with_subs_and_pageviews_location = get_dict_for_every_location(categories_of_related_location, categories_with_pageviews_location)\n",
        "  pprint.pprint(categories_with_subs_and_pageviews_location)\n",
        "  print(\"\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "2oSvqHTPlEQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# categories_of_related_people = get_categories(top_most_popular_people)\n",
        "# pprint.pprint(categories_of_related_people)\n",
        "# print(\"\\n\") #PAAAAASST\n",
        "#categories_with_pageviews_person = get_pageviews_for_categories(categories_of_related_people)\n",
        "pprint.pprint(categories_with_pageviews_person)\n",
        "print(\"\\n\")\n",
        "#categories_with_subs_and_pageviews_person = get_dict_for_every_location(categories_of_related_people, categories_with_pageviews_person)\n",
        "pprint.pprint(categories_with_subs_and_pageviews_person)\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "id": "wp7aIUzp2UkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# categories_of_related_people = get_categories(top_most_popular_people)\n",
        "# pprint.pprint(categories_of_related_people)\n",
        "# print(\"\\n\") #PAAAAASST\n",
        "#categories_with_pageviews_person = get_pageviews_for_categories(categories_of_related_people)\n",
        "pprint.pprint(categories_with_pageviews_location)\n",
        "print(\"\\n\")\n",
        "#categories_with_subs_and_pageviews_person = get_dict_for_every_location(categories_of_related_people, categories_with_pageviews_person)\n",
        "pprint.pprint(categories_with_subs_and_pageviews_location)\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "id": "ceXpkGRzl_sQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#time saving for third part (related peoples categories recovery and ordering) - 43m+ (14m-24m)\n",
        "test_most_popular_related_people_with_categories = get_categories_of_people_list(test_related_people_pageviews_dict)\n",
        "pprint.pprint(test_most_popular_related_people_with_categories)"
      ],
      "metadata": {
        "id": "KcTYo7mBmMbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#time saving for third part (related peoples categories recovery and ordering) - 43m+ (14m-24m)\n",
        "test_most_popular_related_location_with_categories = get_categories_of_people_list(test_related_location_pageviews_dict)\n",
        "pprint.pprint(test_most_popular_related_location_with_categories)"
      ],
      "metadata": {
        "id": "gvw9PvhrlCK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#time saving for third part retrieves the categories of the answer-entities - 9m+ (6m)\n",
        "test_answer_entities_with_categories = get_categories_with_pv_answerEntities()\n",
        "pprint.pprint(test_answer_entities_with_categories, indent=1, compact=True)"
      ],
      "metadata": {
        "id": "OEduFlSGlDlZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#time saving for third part retrieves the categories of the answer-entities - 9m+ (6m)\n",
        "test_answer_entities_with_categories_location = get_categories_with_pv_answerEntities_location()\n",
        "pprint.pprint(test_answer_entities_with_categories_location, indent=1, compact=True)"
      ],
      "metadata": {
        "id": "UKB6vHTRmjuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#time saving for fourth part counts the categories of the answer-entities - 3s\n",
        "test_counted_category_apperances = count_categories(test_most_popular_related_people_with_categories, test_answer_entities_with_categories)\n",
        "#just for the ordering of the inner list\n",
        "ordered_data = {}\n",
        "for key,value in test_counted_category_apperances.items():\n",
        "  tmp = OrderedDict(value)\n",
        "  ordered_data[key] = OrderedDict(sorted(tmp.items(), key=lambda x: x[1][0], reverse=True) )\n",
        "test_counted_category_apperances = ordered_data\n",
        "pprint.pprint(test_counted_category_apperances)"
      ],
      "metadata": {
        "id": "A3t42JlHlE1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#time saving for fourth part counts the categories of the answer-entities - 3s\n",
        "test_counted_category_apperances_location = count_categories(test_most_popular_related_location_with_categories, test_answer_entities_with_categories_location)\n",
        "#just for the ordering of the inner list\n",
        "ordered_data = {}\n",
        "for key,value in test_counted_category_apperances_location.items():\n",
        "  tmp = OrderedDict(value)\n",
        "  ordered_data[key] = OrderedDict(sorted(tmp.items(), key=lambda x: x[1][0], reverse=True) )\n",
        "test_counted_category_apperances_location = ordered_data\n",
        "pprint.pprint(test_counted_category_apperances_location)"
      ],
      "metadata": {
        "id": "_R4lD8_Wmr4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1. calculate the IoU between max and every other person:\n",
        "# (M,C) = 5/20; (M,L) = 2/20; (M,D) = 2/20; (M,A) = 2/20; (M,F) = 2/20;\n",
        "intersection_between_people_with_ae = calculate_IoU_from_countedCategoryDict(test_counted_category_apperances)\n",
        "print(\"IoU between the people and the answer-entity:\")\n",
        "pprint.pprint(intersection_between_people_with_ae,indent=4)\n",
        "\n",
        "#2. calculate the average diversity between the 6 drivers:\n",
        "# (20-5) + (20-2) + (20-2) + (20-2) + (20-2) = 87; (#avg_diversity/#pairwise_comparison) = 87/5 = 17,4\n",
        "avg_diversity_from_IoU = calculate_avg_diversity_from_IoU(intersection_between_people_with_ae)\n",
        "print(\"Avergage diversity from IoU:\")\n",
        "pprint.pprint(avg_diversity_from_IoU, indent=4)\n",
        "\n",
        "#3. calculate a type of categoreis_score\n",
        "# categories_score = cat_diversity * cat_popularity(pvs)\n",
        "categories_scores_dict = calculate_categories_score(test_counted_category_apperances, avg_diversity_from_IoU)\n",
        "# print(\"Category score:\")\n",
        "for key,value in categories_scores_dict.items():\n",
        "  categories_scores_dict[key] = OrderedDict(sorted(value.items(), key=lambda x: x[1], reverse=True))\n",
        "  #ordered_dicter[k] = OrderedDict(sorted(v.items(), key=lambda x: x[1], reverse=True))\n",
        "pprint.pprint(categories_scores_dict,indent=4)\n",
        "\n",
        "# retrieve the occupation of our answer-entities for better hint sentence creation\n",
        "#people_occupations = get_occupations(test_person_answers_dict)\n",
        "#pprint.pprint(people_occupations,indent=4)\n",
        "\n",
        "#create some sentences with the occupation and a unexpected category as hints\n",
        "person_answers_dict = dict(zip(person_df['Answer'], person_df['Question']))\n",
        "mucd = create_hint_sentences_unexCategs(categories_scores_dict, person_answers_dict)\n",
        "pprint.pprint(mucd)\n",
        "\n",
        "#now we could try and compare the person-question and the hints via BERNT and choose the most similar one"
      ],
      "metadata": {
        "id": "RQR9zKqVbIG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1. calculate the IoU between max and every other person:\n",
        "# (M,C) = 5/20; (M,L) = 2/20; (M,D) = 2/20; (M,A) = 2/20; (M,F) = 2/20;\n",
        "intersection_between_location_with_ae = calculate_IoU_from_countedCategoryDict(test_counted_category_apperances_location)\n",
        "print(\"IoU between the people and the answer-entity:\")\n",
        "pprint.pprint(intersection_between_location_with_ae,indent=4)\n",
        "\n",
        "#2. calculate the average diversity between the 6 drivers:\n",
        "# (20-5) + (20-2) + (20-2) + (20-2) + (20-2) = 87; (#avg_diversity/#pairwise_comparison) = 87/5 = 17,4\n",
        "avg_diversity_from_IoU_location = calculate_avg_diversity_from_IoU(intersection_between_location_with_ae)\n",
        "print(\"Avergage diversity from IoU:\")\n",
        "pprint.pprint(avg_diversity_from_IoU_location, indent=4)\n",
        "\n",
        "#3. calculate a type of categoreis_score\n",
        "# categories_score = cat_diversity * cat_popularity(pvs)\n",
        "categories_scores_dict_location = calculate_categories_score(test_counted_category_apperances_location, avg_diversity_from_IoU_location)\n",
        "# print(\"Category score:\")\n",
        "for key,value in categories_scores_dict_location.items():\n",
        "  categories_scores_dict_location[key] = OrderedDict(sorted(value.items(), key=lambda x: x[1], reverse=True))\n",
        "  #ordered_dicter[k] = OrderedDict(sorted(v.items(), key=lambda x: x[1], reverse=True))\n",
        "pprint.pprint(categories_scores_dict_location,indent=4)\n",
        "\n",
        "# retrieve the occupation of our answer-entities for better hint sentence creation\n",
        "#people_occupations = get_occupations(test_person_answers_dict)\n",
        "#pprint.pprint(people_occupations,indent=4)\n",
        "\n",
        "#create some sentences with the occupation and a unexpected category as hints\n",
        "location_answers_dict = dict(zip(location_df['Answer'], location_df['Question']))\n",
        "mucd = create_hint_sentences_unexCategs_location(categories_scores_dict_location, location_answers_dict)\n",
        "pprint.pprint(mucd)\n",
        "\n",
        "#now we could try and compare the person-question and the hints via BERNT and choose the most similar one"
      ],
      "metadata": {
        "id": "pBuR4xemmzMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " For Testing and research: Execute this part once (ca. 60m for 3people), then ruese the test variables from this section (faster)"
      ],
      "metadata": {
        "id": "aoazxgQByklM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Continue testing"
      ],
      "metadata": {
        "id": "dHIEhJe7Rs8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing new things and approaches\n",
        "wiki_link = 'https://en.wikipedia.org/wiki/'\n",
        "\n",
        "def check_inner_tuple_length(pv_and_pages_dict):\n",
        "  # Iterate over the dictionary\n",
        "  for key, value in pv_and_pages_dict.items():\n",
        "    # Iterate over the ordered dict values\n",
        "    for inner_key, inner_value in value.items():\n",
        "      # Check if the inner tuple has two elements\n",
        "      try:\n",
        "        if len(inner_value) == 2:\n",
        "          # Update the value by adding a zero at the end\n",
        "          value[inner_key] = inner_value + (0,)\n",
        "      except Exception as e: print(e)\n",
        "\n",
        "  return pv_and_pages_dict\n",
        "\n",
        "def unexpected_categories(person_answer_entity_list):\n",
        "  wiki_link_list={}\n",
        "  for val in person_answer_entity_list:\n",
        "    names_underscores = val.replace(' ', '_')\n",
        "    answer_entity_wiki_link = wiki_link + names_underscores\n",
        "    wiki_link_list[val] =  answer_entity_wiki_link\n",
        "\n",
        "  categories = get_categories(wiki_link_list)\n",
        "  categories_with_pv = get_pageviews_for_categories(categories)\n",
        "  categories_with_subs_pvs = get_dict_for_every_location(categories, categories_with_pv)\n",
        "\n",
        "  test1 = check_inner_tuple_length(categories_with_subs_pvs )\n",
        "  test2 = prune_and_ordered_dict(test1, 20)\n",
        "  ordered_categories_dict_AnswerPeopleEntities = sorting_dict(test2)\n",
        "  #up to this point, the function retrieves,pre-prunes and orderes the most popular categories from the person-answer-entities\n",
        "  #now wee need to do the same for each of the selected_related_people_list, to then compare the categories and rank them from most to least occuring\n",
        "  return(ordered_categories_dict_AnswerPeopleEntities)\n",
        "\n",
        "def related_people_with_categories(categs_list):\n",
        "  selected_people = {}\n",
        "  selected_people_with_categories = {}\n",
        "  for key, value in categs_list.items():\n",
        "    names_underscores = key.replace(' ', '_')\n",
        "    related_articles = get_related_links(wiki_link + names_underscores)\n",
        "    filtered_links = filter_two_word_titles(related_articles)\n",
        "    related_people_list = check_if_person(filtered_links)\n",
        "    select_random_people = select_random_entries(related_people_list)\n",
        "\n",
        "    selected_people[key] = select_random_people\n",
        "\n",
        "  return selected_people\n"
      ],
      "metadata": {
        "id": "Yd4AeTR_KPir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "TEST OF FUNCTION FROM THE CELLS ABOVE\n",
        "continue here\n",
        "Loops and function-calls to get the most popular related people of each answer-entity,\n",
        "with the categories of each related-person ranked from most-popular to least,\n",
        "to then beeing able of comparing those categories and finding an unusual one, for this specific answer-entity;\n",
        "\"\"\"\n",
        "\n",
        "#Testing out the new functions:\n",
        "person_answers_dict = dict(zip(person_df['Answer'], person_df['Question']))\n",
        "related_people_dict = {}\n",
        "related_people_link_dict= {}\n",
        "related_people_pageviews_dict = {}\n",
        "\n",
        "#PART 1:\n",
        "#for key,value in person_answers_dict.items():\n",
        "#  related_people_dict[key] = get_related_people_from_person_name(key)\n",
        "\n",
        "#this is just for reducing time during testing\n",
        "related_people_dict = test_related_people_dict\n",
        "#pprint.pprint(related_people_dict)\n",
        "\n",
        "#PART 2:\n",
        "# for key,value in related_people_dict.items():\n",
        "#   inter_link_list = []\n",
        "#   for item in value:\n",
        "#     inter_link_list.append(item['url'])\n",
        "#   related_people_link_dict[key] = inter_link_list\n",
        "#   related_people_pageviews_dict = get_pageviews_from_links(related_people_link_dict)\n",
        "\n",
        "#this is just for reducing time during testing\n",
        "related_people_pageviews_dict = test_related_people_pageviews_dict\n",
        "#pprint.pprint(related_people_pageviews_dict)\n",
        "\n",
        "#PART 3:\n",
        "#most_popular_related_people_with_categories = get_categories_of_people_list(related_people_pageviews_dict)\n",
        "most_popular_related_people_with_categories = test_most_popular_related_people_with_categories\n",
        "#pprint.pprint(most_popular_related_people_with_categories)\n",
        "\n",
        "#PART 4:\n",
        "#creates a list with all the answer-entity names\n",
        "#answer_entities_with_categories = get_categories_with_pv_answerEntities()\n",
        "answer_entities_with_categories = test_answer_entities_with_categories\n",
        "#pprint.pprint(answer_entities_with_categories, indent=1, compact=True)\n",
        "\n",
        "#PART 4:\n",
        "#compare anc count\n",
        "#counted_category_apperances = count_categories(most_popular_related_people_with_categories, answer_entities_with_categories)\n",
        "counted_category_apperances = test_counted_category_apperances\n",
        "# pprint.pprint(counted_category_apperances, indent=1, compact=True)\n",
        "\n",
        "#shared_categories_between_people = count_categories(most_popular_related_people_with_categories)\n",
        "#pprint.pprint(shared_categories_between_people, indent=1, compact=True)\n",
        "\n",
        "#Now compare the categories:\n",
        "#Most appearing category between person-answer-entity and the corresponding related-people\n",
        "#Most popular category, that corresponds to just the person-answer-entity and as few of the related-people as possible (=unexpected category)\n",
        "\n",
        "\n",
        "\n",
        "# now calculate the most popukar categories of the selected_people and compare them\n",
        "\n",
        "person_answer_entity_list = list(person_df['Answer'])\n",
        "#print(person_answer_entity_list)\n",
        "#unexpected_categories(person_answer_entity_list)\n",
        "#print(unexpected_categories(person_answer_entity_list))\n",
        "a = unexpected_categories(person_answer_entity_list)\n",
        "# pprint.pprint(a )\n",
        "#pprint.pprint(unexpected_categories(person_answer_entity_list))\n",
        "#b = related_people_with_categories(a)\n",
        "#pprint.pprint(b)\n",
        "#print(person_questions_dict)\n",
        "#10 mins"
      ],
      "metadata": {
        "id": "XU0XbS0QfnWv",
        "outputId": "621c4e5c-2eda-4584-ad53-fdd7d7053295",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "object of type 'int' has no len()\n",
            "Sorting failed for person Lionel Messi: 'int' object is not subscriptable in sorting_dict\n",
            "Sorting failed for person Max Verstappen: 'int' object is not subscriptable in sorting_dict\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pageview Test Functions\n"
      ],
      "metadata": {
        "id": "GXcHCM1dcbai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import requests\n",
        "\n",
        "from selenium import webdriver\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def get_table_info(url):\n",
        "    \"\"\"\n",
        "    Given a URL, this function opens the url and retrieves the information stored in a table.\n",
        "    \"\"\"\n",
        "    options = webdriver.FirefoxOptions()\n",
        "    #options.headless = True\n",
        "    options.add_argument('--headless')\n",
        "\n",
        "    driver = webdriver.Firefox(options=options)\n",
        "    driver.get(url)\n",
        "    time.sleep(3) # Wait for the page to load completely\n",
        "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "    driver.quit()\n",
        "    table = soup.find('table')\n",
        "    rows = table.find_all('tr')\n",
        "    headers = [header.text.strip() for header in rows[0].find_all('th')]\n",
        "    data = []\n",
        "    for row in rows[1:]:\n",
        "        data.append([cell.text.strip() for cell in row.find_all('td')])\n",
        "    return (headers, data)\n",
        "\n",
        "def get_table_info_requests(url):\n",
        "    \"\"\"\n",
        "    Given a URL, this function opens the url and retrieves the information stored in a table.\n",
        "    \"\"\"\n",
        "    headers = []\n",
        "    data = []\n",
        "\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    table = soup.find('table')\n",
        "    rows = table.find_all('tr')\n",
        "    headers = [header.text.strip() for header in rows[0].find_all('th')]\n",
        "    data = []\n",
        "    for row in rows[1:]:\n",
        "        data.append([cell.text.strip() for cell in row.find_all('td')])\n",
        "\n",
        "    return (headers, data)\n",
        "\n",
        "def get_links_in_section(wikipedia_url, section_heading):\n",
        "    \"\"\"\n",
        "    Given a Wikipedia URL, and a section of this article, returns an array of dictionaries containing the href, title, and description of each link on that section of the page.\n",
        "    \"\"\"\n",
        "    response = requests.get(wikipedia_url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    section = soup.find('span', {'id': section_heading})\n",
        "    if section is None:\n",
        "        return None\n",
        "    section_content = section.parent.find_next_sibling('ul')\n",
        "    if section_content is None:\n",
        "        return None\n",
        "    links = []\n",
        "    for item in section_content.find_all('li'):\n",
        "        link = item.find('a')\n",
        "        if link is not None and link.has_attr('href') and link['href'].startswith('/wiki/'):\n",
        "            #title = link.get('title', '')\n",
        "            title = link.get('title')\n",
        "            description = item.text.strip()\n",
        "            url = f\"https://en.wikipedia.org{link['href']}\"\n",
        "            links.append({'title': title, 'description': description, 'url': url})\n",
        "    #print(links)\n",
        "    return links\n",
        "\n",
        "def get_links_in_section_with_sublinks(wikipedia_url, section_title):\n",
        "    \"\"\"\n",
        "    Given a Wikipedia URL, and a section of this article, returns an array of dictionaries containing the href, title, and description of each link  with its sublinks as well.\n",
        "    \"\"\"\n",
        "    response = requests.get(wikipedia_url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        section_heading = soup.find('span', {'id': section_title})\n",
        "        if section_heading is None:\n",
        "            return None\n",
        "        section = section_heading.parent\n",
        "        section_content = section.find_next_sibling('ul')\n",
        "        if section_content is None:\n",
        "            return None\n",
        "        links = []\n",
        "        for item in section_content.find_all('li'):\n",
        "            link = item.find('a')\n",
        "            if link is not None:\n",
        "                link_title = link.get('title')\n",
        "                link_url = 'https://en.wikipedia.org' + link.get('href')\n",
        "                link_description = item.text.replace(link_title, '').strip()\n",
        "                links.append({'title': link_title, 'url': link_url, 'description': link_description})\n",
        "                for sublink in item.find_all('a', href=True, recursive=False):\n",
        "                    sublink_title = sublink.get('title')\n",
        "                    sublink_url = 'https://en.wikipedia.org' + sublink.get('href')\n",
        "                    sublink_description = sublink.parent.text.replace(sublink_title, '').replace(link_title, '').strip()\n",
        "                    links.append({'title': sublink_title, 'url': sublink_url, 'description': sublink_description})\n",
        "        return links\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def get_events_links(title):\n",
        "    \"\"\"\n",
        "    Gets all the links in the \"Events\" section of a Wikipedia page.\n",
        "    Returns a list of strings containing the URLs of each event.\n",
        "    \"\"\"\n",
        "    url = f\"https://en.wikipedia.org/w/api.php?action=parse&format=json&page={title}&prop=text\"\n",
        "    response = requests.get(url)\n",
        "    data = response.json()\n",
        "\n",
        "    soup = BeautifulSoup(data['parse']['text']['*'], 'html.parser')\n",
        "    events_section = soup.find('span', {'class': 'mw-headline', 'id': 'Events'})\n",
        "    if events_section:\n",
        "        events_list = events_section.find_next('ul')\n",
        "        if events_list:\n",
        "            events = events_list.find_all('a')\n",
        "            event_links = [f\"https://en.wikipedia.org{event['href']}\" for event in events]\n",
        "            return event_links\n",
        "    return None\n",
        "\n",
        "def create_url_from_title(title):\n",
        "    \"\"\"\n",
        "    Given a list of dictionaries containing href, title, and description for each entry, extracts the title from each link and constructs a new URL using the title.\n",
        "    \"\"\"\n",
        "    urls = []\n",
        "    if title != None:\n",
        "      # Replace spaces with underscores in the title\n",
        "      title = title.replace(' ', '_')\n",
        "\n",
        "      # Construct a new URL using the title\n",
        "      url = 'https://pageviews.wmcloud.org/redirectviews/?project=en.wikipedia.org&platform=all-access&agent=user&range=all-time&sort=views&direction=1&view=list&page=' + title\n",
        "    return url\n",
        "\n",
        "def extract_visitorNumber_from_link(link):\n",
        "    \"\"\"\n",
        "    extract the first entry of the table\n",
        "    \"\"\"\n",
        "    headers, data = get_table_info(link)\n",
        "\n",
        "    try:\n",
        "      if data[1]:\n",
        "        return data[1][2]\n",
        "      else:\n",
        "        return 0\n",
        "    except IndexError:\n",
        "      print(\"Index out of range\")\n",
        "\n",
        "def get_visitor_numbers(pageviews_url):\n",
        "    \"\"\"\n",
        "    returns visitor numbers of the page\n",
        "    \"\"\"\n",
        "    response = requests.get(pageviews_url)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()['items']\n",
        "        visitors = {}\n",
        "        for item in data:\n",
        "            visitors[item['timestamp']] = item['views']\n",
        "        return visitors\n",
        "    else:\n",
        "        return None"
      ],
      "metadata": {
        "id": "5VwT46tA0CZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TEST\n",
        "\n",
        "#Seitenaufrufe\n",
        "#url = 'https://pageviews.toolforge.org/?project=en.wikipedia.org&pages=Formula1|Michael_Schumacher'\n",
        "#headers, data = get_table_info(url)\n",
        "#print(headers)\n",
        "#print(data)\n",
        "\n",
        "#get all wiki links on a page -> discard the one we dont need -> analyse the ones that could be usefull -> sort them after the backlinks\n",
        "#print(get_events_links(2004))\n",
        "\n",
        "# Search the most popular births and deaths in that year and then order them\n",
        "url1 = 'https://en.wikipedia.org/wiki/Category:1994_births'\n",
        "url2 = 'https://en.wikipedia.org/wiki/Category:1994_deaths'\n",
        "url3 = 'https://en.wikipedia.org/wiki/1994#Events'\n",
        "\n",
        "#links_events_section = get_links_in_section_with_sublinks(url3, 'Events')\n",
        "#print(len(links_events_section))\n",
        "#print(links_events_section)\n",
        "#print(get_births(1994))\n",
        "\n",
        "#visitor number test\n",
        "link = 'https://pageviews.wmcloud.org/redirectviews/?project=en.wikipedia.org&platform=all-access&agent=user&range=all-time&sort=views&direction=1&view=list&page=North_American_Free_Trade_Agreement'\n",
        "#print(extract_visitorNumber_from_link(link))"
      ],
      "metadata": {
        "id": "dYFPOhzAVotw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#When did Michael Schumacher win his first F1 World Drivers Title? -> 1994\n",
        "#First 1994 is searched on wikipedia, then all the links from the event sections of that page are filtered and written into a dict together with their pageviews.\n",
        "#Now order these lists such that the entry with most pageviews is on top\n",
        "#THIS SHOULD WORK\n",
        "\n",
        "year=2004\n",
        "url = 'https://en.wikipedia.org/wiki/2004'\n",
        "\n",
        "links_events_section = get_links_in_section_with_sublinks(url, 'Events')\n",
        "print(len(links_events_section))\n",
        "\n",
        "for link in links_events_section:\n",
        "    t_title =  link.get('title')\n",
        "    t_url = create_url_from_title(t_title)\n",
        "    link[\"pageview_url\"] = t_url\n",
        "    link[\"pageviews\"] = extract_visitorNumber_from_link(t_url)\n",
        "\n",
        "#sort the list links_event_section after the pageviews such that the entity with the highest pageviews is first\n",
        "sorted_list = sorted(links_events_section, key=lambda x: int(x['pageviews'].replace(',', '')), reverse=True)"
      ],
      "metadata": {
        "id": "8hZTwIILMaNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint\n",
        "pageview_test_generated_hints_for_years = sorted_list\n",
        "\n",
        "pprint.pprint(pageview_test_generated_hints_for_years, indent=4)"
      ],
      "metadata": {
        "id": "IGiGzaMzbF-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Old predictions methods**"
      ],
      "metadata": {
        "id": "bp5WmYAKw6Mn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "du5Un8VufnlG"
      },
      "source": [
        "### Functions for reuse"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8o8t6UzdAaKj"
      },
      "source": [
        "#get Wikidata ID\n",
        "def getQitemOfName(entity):\n",
        "\n",
        "  searched = \"'\"+entity+\"'\"\n",
        "  sparql.setQuery('''\n",
        "  SELECT ?item ?itemLabel WHERE {\n",
        "    ?item rdfs:label '''+searched+'''@en.\n",
        "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
        "\n",
        "  }\n",
        "  ''')\n",
        "\n",
        "  sparql.setReturnFormat(JSON)\n",
        "  entities = sparql.query().convert()\n",
        "  entities_df = pd.json_normalize(entities['results']['bindings'])\n",
        "  words = entities_df[[\"item.value\"]].iloc[0].str.split(\"/\")[0]\n",
        "  return words[-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4tNPxs6Cp8f"
      },
      "source": [
        "#get name of entity from Wikidata ID\n",
        "\n",
        "def getLabelOfQitems(entities):\n",
        "\n",
        "  finalLabels = []\n",
        "  for item in entities:\n",
        "    #searched = \"'\"+item+\"'\"\n",
        "    sparql.setQuery('''\n",
        "    SELECT * WHERE {\n",
        "        wd:'''+item+ ''' rdfs:label ?label .\n",
        "        FILTER (langMatches( lang(?label), \"EN\" ) )\n",
        "    } LIMIT 1\n",
        "    ''')\n",
        "    sparql.setReturnFormat(JSON)\n",
        "    entities = sparql.query().convert()\n",
        "    entities_df = pd.json_normalize(entities['results']['bindings'])\n",
        "    label = entities_df[\"label.value\"].iloc[0]\n",
        "    finalLabels.append(label)\n",
        "\n",
        "  return finalLabels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gK0zkvbXxRpU"
      },
      "source": [
        "#function to get all backlinks to wikipage and saves backlinks >= 1000 in array\n",
        "#source: https://www.mediawiki.org/wiki/API:Backlinks#Response\n",
        "\n",
        "import requests\n",
        "\n",
        "def getBacklinks(entities):\n",
        "  S = requests.Session()\n",
        "\n",
        "  URL = \"https://en.wikipedia.org/w/api.php\"\n",
        "\n",
        "  listWithBacklink500 = []\n",
        "  dictWithAllEntries = {}\n",
        "\n",
        "  for val in entities:\n",
        "    #print(row[\"itemLabel.value\"])\n",
        "    PARAMS = {\n",
        "        \"action\": \"query\",\n",
        "        \"format\": \"json\",\n",
        "        \"list\": \"backlinks\",\n",
        "        \"bltitle\": val,\n",
        "        \"bllimit\": \"max\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "      R = S.get(url=URL, params=PARAMS)\n",
        "      DATA = R.json()\n",
        "\n",
        "\n",
        "      BACKLINKS = DATA[\"query\"][\"backlinks\"]\n",
        "\n",
        "      count = 0\n",
        "      for links in BACKLINKS:\n",
        "        count +=1\n",
        "\n",
        "\n",
        "      while \"continue\" in DATA:\n",
        "        if (count >= 1000):\n",
        "          break;\n",
        "        blcontinue = DATA[\"continue\"][\"blcontinue\"]\n",
        "        PARAMS[\"blcontinue\"] = blcontinue\n",
        "\n",
        "        R = S.get(url=URL, params=PARAMS)\n",
        "        DATA = R.json()\n",
        "        BACKLINKS = DATA[\"query\"][\"backlinks\"]\n",
        "\n",
        "        for links in BACKLINKS:\n",
        "          count += 1\n",
        "\n",
        "      dictWithAllEntries[val] = count\n",
        "      #print(val + \": \" + str(count))\n",
        "\n",
        "    except ValueError:\n",
        "      print(\"Value Error\")\n",
        "\n",
        "    if (count >= 1000):\n",
        "      listWithBacklink500.append(val)\n",
        "\n",
        "  if (len(listWithBacklink500) > 0):\n",
        "    #return getPageViews(listWithBacklink500)\n",
        "    return listWithBacklink500\n",
        "\n",
        "  else:\n",
        "    listMostBacklinks = []\n",
        "    sortedDict = dict(sorted(dictWithAllEntries.items(), key=lambda item:item[1]))\n",
        "\n",
        "    for k, v in sortedDict.items():\n",
        "      listMostBacklinks.append(k)\n",
        "\n",
        "    numberToPass = len(listMostBacklinks)//3\n",
        "\n",
        "    return listMostBacklinks[-numberToPass:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-0fPfmxUZSW"
      },
      "source": [
        "#function to get all backlinks to wikipage and saves backlink == 500 in array\n",
        "#source: https://www.mediawiki.org/wiki/API:Backlinks#Response\n",
        "\n",
        "import requests\n",
        "\n",
        "def getBacklinksDict(entities):\n",
        "  S = requests.Session()\n",
        "\n",
        "  URL = \"https://en.wikipedia.org/w/api.php\"\n",
        "\n",
        "  dictWithBacklink500 = {}\n",
        "  dictWithAllEntries = {}\n",
        "\n",
        "  for key, val in entities.items():\n",
        "    print(key, val)\n",
        "\n",
        "    PARAMS = {\n",
        "        \"action\": \"query\",\n",
        "        \"format\": \"json\",\n",
        "        \"list\": \"backlinks\",\n",
        "        \"bltitle\": key,\n",
        "        \"bllimit\": \"max\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "      R = S.get(url=URL, params=PARAMS)\n",
        "      DATA = R.json()\n",
        "\n",
        "\n",
        "      BACKLINKS = DATA[\"query\"][\"backlinks\"]\n",
        "\n",
        "      count = 0\n",
        "      for links in BACKLINKS:\n",
        "        count +=1\n",
        "\n",
        "\n",
        "      while \"continue\" in DATA:\n",
        "        if (count >= 1000):\n",
        "          break;\n",
        "        blcontinue = DATA[\"continue\"][\"blcontinue\"]\n",
        "        PARAMS[\"blcontinue\"] = blcontinue\n",
        "\n",
        "        R = S.get(url=URL, params=PARAMS)\n",
        "        DATA = R.json()\n",
        "        BACKLINKS = DATA[\"query\"][\"backlinks\"]\n",
        "\n",
        "        for links in BACKLINKS:\n",
        "          count += 1\n",
        "\n",
        "      dictWithAllEntries[key] = count\n",
        "      #print(val + \": \" + str(count))\n",
        "\n",
        "    except ValueError:\n",
        "      print(\"Value Error\")\n",
        "    except KeyError:\n",
        "      print(\"Key Error\")\n",
        "\n",
        "    if (count >= 1000):\n",
        "      dictWithBacklink500.update({key: val})\n",
        "\n",
        "  if (len(dictWithBacklink500) > 0):\n",
        "    #return getPageViews(listWithBacklink500)\n",
        "    return dictWithBacklink500\n",
        "\n",
        "    #print(listMostBacklinks[-numberToPass:])\n",
        "\n",
        "\n",
        "    return listMostBacklinks[-numberToPass:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hz4SZXwl1j7d"
      },
      "source": [
        "#get avg pageViews of entities\n",
        "\n",
        "def getPageViewsList(listEntities):\n",
        "  avgDict = {}\n",
        "\n",
        "  for val in listEntities:\n",
        "    try:\n",
        "      resp = pageviewapi.per_article('en.wikipedia', val, '20191106', '20201120', access='all-access', agent='all-agents', granularity='monthly')\n",
        "    #print(resp)\n",
        "      avgViews = 0\n",
        "      count = 0\n",
        "      for i in resp.get(\"items\"):\n",
        "        #print(i.get(\"views\"))\n",
        "        avgViews += i.get(\"views\")\n",
        "        count += 1\n",
        "      avgDict.update({val: avgViews//count})\n",
        "    except:\n",
        "      print(\"Page not found to check pageViews for page: \" + val)\n",
        "\n",
        "  #return (max(avgDict, key=avgDict.get))\n",
        "  return avgDict\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhL5jdCN-NdY"
      },
      "source": [
        "#get avg pageViews of entity\n",
        "\n",
        "#import pageviewapi\n",
        "\n",
        "\n",
        "def getPageViews(entity):\n",
        "  avgDict = {}\n",
        "\n",
        "  try:\n",
        "    resp = pageviewapi.per_article('en.wikipedia', entity, '20191106', '20201120', access='all-access', agent='all-agents', granularity='monthly')\n",
        "\n",
        "    avgViews = 0\n",
        "    count = 0\n",
        "    for i in resp.get(\"items\"):\n",
        "      #print(i.get(\"views\"))\n",
        "      avgViews += i.get(\"views\")\n",
        "      count += 1\n",
        "\n",
        "    pageViews = avgViews//count\n",
        "    #avgDict.update({val: avgViews//count})\n",
        "\n",
        "    #return (max(avgDict, key=avgDict.get))\n",
        "    return pageViews\n",
        "  #print(resp)\n",
        "  except:\n",
        "    print(\"Page not found to check pageViews for page: \" + entity)\n",
        "    return -1\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check if entity is a person\n",
        "#return list of persons\n",
        "\n",
        "def checkIfHuman(entity):\n",
        "\n",
        "  #listHumans = []\n",
        "\n",
        "  #for test in entity:\n",
        "    #print(item)\n",
        "  searched = \"'\"+entity+\"'\"\n",
        "\n",
        "  #print(searched)\n",
        "\n",
        "  try:\n",
        "\n",
        "    sparql.setQuery('''\n",
        "        SELECT distinct ?item ?itemLabel\n",
        "        WHERE{\n",
        "          ?item ?label '''+searched+'''.\n",
        "          ?item wdt:P31 wd:Q5.\n",
        "          ?article schema:about ?item .\n",
        "          ?article schema:inLanguage \"en\" .\n",
        "          ?article schema:isPartOf <https://en.wikipedia.org/>.\n",
        "          SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
        "        }\n",
        "    ''')\n",
        "    sparql.setReturnFormat(JSON)\n",
        "    entities = sparql.query().convert()\n",
        "    entities_df = pd.json_normalize(entities['results']['bindings'])\n",
        "\n",
        "    #print(entities)\n",
        "    #print(entities_df[\"itemLabel.value\"].item())\n",
        "\n",
        "    #for index, row in entities_df.iterrows():\n",
        "    return entities_df[\"itemLabel.value\"].item()\n",
        "\n",
        "\n",
        "  except:\n",
        "    return None\n"
      ],
      "metadata": {
        "id": "QHdp21OCwtgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOJJtksOgQYk"
      },
      "source": [
        "#if parantheses in entity -> remove\n",
        "\n",
        "def removeParanthesesDict(inputDict):\n",
        "\n",
        "  newDict = {}\n",
        "  for key, val in inputDict.items():\n",
        "    if ('(' in key):\n",
        "      newElem = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", key).rstrip()\n",
        "      newDict.update({newElem: val})\n",
        "    else:\n",
        "      newDict.update({key: val})\n",
        "\n",
        "  return newDict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lhmuuUNQ__8"
      },
      "source": [
        "###Hints if answer is a Location"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-T2fLcTRD54"
      },
      "source": [
        "#Query with all properties to get data\n",
        "\n",
        "def allHintsLocation(entity):\n",
        "  sparql.setQuery('''\n",
        "  SELECT ?itemLabel ?property\n",
        "  WHERE\n",
        "  {\n",
        "    VALUES ?property {wdt:P35 wdt:P30 wdt:P36 wdt:P610 wdt:P1082 wdt:P421 wdt:P38 wdt:P17 wdt:P1376 wdt:P131 wdt:P6 wdt:P1830 wdt:P47 wdt:P206 wdt:P37 wdt:P463 wdt:793}\n",
        "    wd:'''+entity+''' ?property ?item.\n",
        "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
        "  }\n",
        "  ''')\n",
        "  sparql.setReturnFormat(JSON)\n",
        "  hintsLocation = sparql.query().convert()\n",
        "\n",
        "  hintsLocation_df = pd.json_normalize(hintsLocation['results']['bindings'])\n",
        "\n",
        "  return hintsLocation_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "co1A0iK-sery"
      },
      "source": [
        "#create template based hints\n",
        "\n",
        "def getFormatedHintsLocation(queryResults_df):\n",
        "  timeZone = []\n",
        "  isLocatedIn = []\n",
        "  ownerOf = []\n",
        "  shareBorder = []\n",
        "  bodyOfWater = []\n",
        "  languages = []\n",
        "  memberOf = []\n",
        "  significantEvents = []\n",
        "\n",
        "  listHintsLocation = []\n",
        "\n",
        "  searchString = \"http://www.wikidata.org/prop/direct/\"\n",
        "\n",
        "  for index, row in queryResults_df.iterrows():\n",
        "    if(not location in row[\"itemLabel.value\"]):\n",
        "      if(row[\"property.value\"] == searchString + \"P35\"):\n",
        "        listHintsLocation.append(\"Head of state of searched country is \" + row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P30\"):\n",
        "        listHintsLocation.append(\"The searched location is on continent \" + row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P36\"):\n",
        "        listHintsLocation.append(\"The capital of searched country is \" + row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"\"):\n",
        "        listHintsLocation.append(\"The highest point in searched location is \" + row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P1082\"):\n",
        "        listHintsLocation.append(\"The searched location has a population of \" + row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P38\"):\n",
        "        listHintsLocation.append(\"Currency in searched location is \" + row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P17\"):\n",
        "        if(not location in row[\"itemLabel.value\"]):\n",
        "          listHintsLocation.append(\"The searched location is in country \"+ row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P1376\"):\n",
        "        listHintsLocation.append(\"The searched city is capital of \" + row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P131\"):\n",
        "        listHintsLocation.append(\"The searched location is located in \" + row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P6\"):\n",
        "        listHintsLocation.append(\"Head of government of searched location is \" + row[\"itemLabel.value\"])\n",
        "\n",
        "      #if get multiple values save all in list and pick one afterwards\n",
        "      elif(row[\"property.value\"] == searchString + \"P1830\"):\n",
        "        if(not location in row[\"itemLabel.value\"]):\n",
        "          ownerOf.append(row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P47\"):\n",
        "        shareBorder.append(row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P206\"):\n",
        "        bodyOfWater.append(row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P37\"):\n",
        "        if(not location in row[\"itemLabel.value\"]):\n",
        "          languages.append(row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P421\"):\n",
        "        timeZone.append(row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P463\"):\n",
        "        memberOf.append(row[\"itemLabel.value\"])\n",
        "      elif(row[\"property.value\"] == searchString + \"P793\"):\n",
        "        if(not location in row[\"itemLabel.value\"]):\n",
        "          significantEvents.append(row[\"itemLabel.value\"])\n",
        "\n",
        "  #getting more than one result with one query, pick one for output\n",
        "  if(len(ownerOf) > 0):\n",
        "    listHintsLocation.append(\"The searched location is owner of \" + ownerOf[0])\n",
        "  if(len(shareBorder) > 0):\n",
        "    listHintsLocation.append(\"The searched location shares border with \" + shareBorder[0])\n",
        "  if(len(bodyOfWater) > 0):\n",
        "    listHintsLocation.append(\"The next body of water of searched location is \" + bodyOfWater[0])\n",
        "  if(len(memberOf) > 0):\n",
        "    listBacklinks = getBacklinks(memberOf)\n",
        "    dictPageViews = getPageViewsList(listBacklinks)\n",
        "    listHintsLocation.append(\"Searched location is member of \" + (max(dictPageViews, key=dictPageViews.get)))\n",
        "  if(len(significantEvents) > 0):\n",
        "    listBacklinks = getBacklinks(significantEvents)\n",
        "    dictPageViews = getPageViewsList(listBacklinks)\n",
        "    listHintsLocation.append(\"A significant event happened in this location was \" + (max(dictPageViews, key=dictPageViews.get)))\n",
        "\n",
        "  #if getting more than one language concatenate all\n",
        "  commaCounter = len(languages)-1\n",
        "  stringLanguages = \"\"\n",
        "  for language in languages:\n",
        "    if(commaCounter > 0):\n",
        "      stringLanguages += language + \", \"\n",
        "      commaCounter -= 1\n",
        "    else:\n",
        "      stringLanguages += language\n",
        "  if(stringLanguages != \"\"):\n",
        "    listHintsLocation.append(\"Spoken language(s) in searched location is/are \" + stringLanguages)\n",
        "  if(len(timeZone) > 0):\n",
        "    listHintsLocation.append(\"The searched location is in time Zone \" + timeZone[0])\n",
        "\n",
        "\n",
        "  return listHintsLocation\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g65hUdVTRbXU"
      },
      "source": [
        "def writeLocationHintsInFile(listHintsLocation):\n",
        "  randomHintsList = random.sample(listHintsLocation, len(listHintsLocation))\n",
        "\n",
        "  for item in randomHintsList:\n",
        "    with open(\"hintsLocation.txt\", 'a') as writefile:\n",
        "      writefile.write(item + \"\\n\")\n",
        "      writefile.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Yf6Fi_34ye5"
      },
      "source": [
        "###Hints if answer is a Person"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Query with all properties to get data\n",
        "\n",
        "def allHintsPerson(entity):\n",
        "  sparql.setQuery('''\n",
        "  SELECT ?itemLabel ?property\n",
        "  WHERE\n",
        "  {\n",
        "    VALUES ?property {wdt:P21 wdt:P27 wdt:P569 wdt:P19 wdt:P1971 wdt:P106 wdt:P1340 wdt:P1884 wdt:P2048 wdt:P39 wdt:P69 wdt:P512 wdt:P102 wdt:P3602 wdt:P800 wdt:P166 wdt:P3373 wdt:P1412 wdt:P413 wdt:P118 wdt:P54}\n",
        "    wd:'''+entity+''' ?property ?item.\n",
        "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
        "  }\n",
        "  ''')\n",
        "  sparql.setReturnFormat(JSON)\n",
        "  hintsPerson = sparql.query().convert()\n",
        "\n",
        "  hintsPerson_df = pd.json_normalize(hintsPerson['results']['bindings'])\n",
        "\n",
        "  return hintsPerson_df"
      ],
      "metadata": {
        "id": "Lm8zYu5C3xSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qS1ZtKwL5hM-"
      },
      "source": [
        "#create temolate based hints\n",
        "\n",
        "def getFormatedHintsPerson(queryResults_df):\n",
        "\n",
        "  listOccupation = [\"Searched person was occuupied as \"]\n",
        "  listPositionHeld = [\"Searched person held the position(s) \"]\n",
        "  listEducatedAt = [\"Searched person was educated at \"]\n",
        "  listAcademicDegrees = [\"Searched person has academic degrees \"]\n",
        "  listPoliticParty = [\"Searched was/is member of politic party \"]\n",
        "  listCandidacyElection = [\"Searched person was candidacy at election(s) \"]\n",
        "  listNotableWork = [\"Searched person did notable work \"]\n",
        "  listAwardsReceived = [\"Searched person received awards \"]\n",
        "  listLanguages = [\"Searched person speaks language(s) \"]\n",
        "  listPlayedPositions = [\"Searched person played position \"]\n",
        "  listLeague = [\"Searched person played in leagues \"]\n",
        "  listTeams = [\"Searched person played in teams \"]\n",
        "  listCitizenship = [\"Searched person has following citizenships \"]\n",
        "  listSiblings = []\n",
        "  listHair = []\n",
        "\n",
        "  listHintsPerson = []\n",
        "\n",
        "  for index, row in queryResults_df.iterrows():\n",
        "    if(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P21\"):\n",
        "      listHintsPerson.append(\"Gender of searched person is \" + row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P569\"):\n",
        "      listHintsPerson.append(\"Searched person was born in \" + row[\"itemLabel.value\"][:4])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P19\"):\n",
        "      listHintsPerson.append(\"Searched person was born in \" + row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P1971\"):\n",
        "      listHintsPerson.append(\"Number of children of seared person is \" + row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P1340\"):\n",
        "      listHintsPerson.append(\"Searched person has \" + row[\"itemLabel.value\"] + \" eyes\")\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P2048\"):\n",
        "      listHintsPerson.append(\"Searched persons height is \" + row[\"itemLabel.value\"])\n",
        "\n",
        "\n",
        "    #if get multiple values save all in list and pick one afterwards\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P27\"):\n",
        "      listCitizenship.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P106\"):\n",
        "      listOccupation.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P39\"):\n",
        "      listPositionHeld.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P69\"):\n",
        "      listEducatedAt.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P512\"):\n",
        "      listAcademicDegrees.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P102\"):\n",
        "      listPoliticParty.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P3602\"):\n",
        "      listCandidacyElection.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P800\"):\n",
        "      listNotableWork.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P166\"):\n",
        "      listAwardsReceived.append(row[\"itemLabel.value\"])\n",
        "\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P1412\"):\n",
        "      listLanguages.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P413\"):\n",
        "      listPlayedPositions.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P118\"):\n",
        "      listLeague.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P54\"):\n",
        "      listTeams.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P3373\"):\n",
        "      listSiblings.append(row[\"itemLabel.value\"])\n",
        "    elif(row[\"property.value\"] == \"http://www.wikidata.org/prop/direct/P1884\"):\n",
        "      listHair.append(row[\"itemLabel.value\"])\n",
        "\n",
        "  #if too many results could be return check\n",
        "  #if more thane 3 -> cut\n",
        "  listResultsCut = [listPositionHeld, listAcademicDegrees, listPoliticParty,\n",
        "                 listCandidacyElection, listNotableWork, listLanguages,\n",
        "                 listPlayedPositions, listLeague, listTeams, listCitizenship, listAwardsReceived, listEducatedAt, listOccupation]\n",
        "  for singleList in listResultsCut:\n",
        "    cutResults(singleList, listHintsPerson)\n",
        "\n",
        "  #just get one hair color\n",
        "  if(len(listHair) > 0):\n",
        "    listHintsPerson.append(\"The searched person has \" + listHair[0])\n",
        "\n",
        "  #get number of siblings\n",
        "  if(len(listSiblings) > 0):\n",
        "    listHintsPerson.append(\"The searched person has \" + str(len(listSiblings)) + \" siblings\")\n",
        "\n",
        "  return listHintsPerson"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#if more results than 3 -> print 3 results + \"and x others\"\n",
        "\n",
        "def cutResults(listResults, listHintsPerson):\n",
        "\n",
        "  numResults = len(listResults)-1\n",
        "  resultsLeft = numResults-3\n",
        "\n",
        "  if (len(listResults) > 1):\n",
        "    if(resultsLeft > 0):\n",
        "      listHintsPerson.append(getConcatenationMultipleResults(listResults[0:4])+ \" and \" + str(resultsLeft) + \" others\")\n",
        "    else:\n",
        "      listHintsPerson.append(getConcatenationMultipleResults(listResults))"
      ],
      "metadata": {
        "id": "KDP4yoRpB_Sq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIOq3XcwIr2a"
      },
      "source": [
        "#concatenate multiple results with comma\n",
        "\n",
        "def getConcatenationMultipleResults(resultList):\n",
        "  concatenation = \"\"\n",
        "  commaCounter = len(resultList)-1\n",
        "  for item in resultList:\n",
        "    if(commaCounter == len(resultList)-1):\n",
        "      concatenation += item\n",
        "      commaCounter-=1\n",
        "    elif(commaCounter > 0):\n",
        "      concatenation += item + \", \"\n",
        "      commaCounter-=1\n",
        "    elif(commaCounter == 0):\n",
        "      concatenation += item\n",
        "\n",
        "  return concatenation\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B74PSzscAzl8"
      },
      "source": [
        "def writePersonHintsInFile(listHintsPerson):\n",
        "  with open(\"hintsPerson.txt\", 'a') as writefile:\n",
        "    #writefile.write(\"\\n\\n---Hints for Person \" + person + \"---\\n\\n\")\n",
        "    randomHintsList = random.sample(listHintsPerson, len(listHintsPerson))\n",
        "    for item in randomHintsList:\n",
        "      writefile.write(item + \"\\n\")\n",
        "    writefile.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LORwHLog-t_"
      },
      "source": [
        "### Hints if answer is a Year"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dil5f183hhHJ"
      },
      "source": [
        "**Get all outlinks of wikipedia year page and pass to getBacklinks. Than look for return values on page.content and print line**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQFplu-7AF1L"
      },
      "source": [
        "import spacy\n",
        "def hintsYear(year):\n",
        "\n",
        "  page = wikipedia.page(year, auto_suggest=False)\n",
        "  pageContent = page.content\n",
        "\n",
        "  #get all links from year page\n",
        "  dictPageLinks = getOutlinks2(year)\n",
        "  #print(dictPageLinks)\n",
        "\n",
        "  #get rid of unnesercery links\n",
        "  filteredPageLinks = filterDict(dictPageLinks)\n",
        "\n",
        "\n",
        "  #get backlinks of all links\n",
        "  dictBacklink = getBacklinksDict(filteredPageLinks)\n",
        "  #first200 = {k: filteredPageLinks[k] for k in list(filteredPageLinks)[:200]}\n",
        "  #dictBacklink = getBacklinksDict(first200)\n",
        "\n",
        "  #remove parantheses of links\n",
        "  dictAdjusted = removeParanthesesDict(dictBacklink)\n",
        "\n",
        "  #get all possible hint (all lines with a entity #backlinks > 1000)\n",
        "  dictPossibleHints = findLineInTextEvent(dictAdjusted, year)\n",
        "\n",
        "  with open('./resultsEvents.txt', 'a') as writefile:\n",
        "    writefile.write(\"Question: \" + questionYear + \"(\"+ str(year) + \")---\\n\")\n",
        "\n",
        "  #check if entity because of which line was found is subject\n",
        "  #if it is not the subject -> drop\n",
        "  dictFinalHints = getFinalHints(dictPossibleHints)\n",
        "\n",
        "  #get rid of number of foundevents at end of line\n",
        "  #need to put number at the and because it is dict and would be overwritten\n",
        "  dictFinalHintsAdjusted = {}\n",
        "  for key, val in dictFinalHints.items():\n",
        "    dictFinalHintsAdjusted.update({key.split('///', 1)[0]: val})\n",
        "\n",
        "  #returns dict sorted after PageViews\n",
        "  dictSortedPV = sortHintsNumberPVSubject(dictFinalHintsAdjusted)\n",
        "\n",
        "  #get highest PageView\n",
        "  maxPV = list(dictSortedPV.values())[0]\n",
        "\n",
        "  #sort hints PageViews and Simscore combined\n",
        "  sortHintsUtilityScore(questionYear, dictSortedPV, maxPV)\n",
        "\n",
        "  listHumansNames = []\n",
        "\n",
        "  for key, val in dictAdjusted.items():\n",
        "    result = checkIfHuman(key)\n",
        "    if(result != None):\n",
        "      listHumansNames.append(result)\n",
        "\n",
        "  #findLineInTextBirthDeath(listHumansNames, year)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-dJk5vrhq6M"
      },
      "source": [
        "**Function to find line of most important link in year content**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0GonOTeikJm"
      },
      "source": [
        "import re\n",
        "\n",
        "def findLineInTextEvent(entitiesEvent, year):\n",
        "\n",
        "  page = wikipedia.page(year, auto_suggest=False)\n",
        "  pageText = page.content\n",
        "  splitText = []\n",
        "\n",
        "  splitText = (re.split(\"== Events ==|== Births ==\", pageText ))\n",
        "\n",
        "  contentEvents = splitText[1]\n",
        "\n",
        "  foundEvent = 0\n",
        "\n",
        "  dictHintsEntity = {}\n",
        "\n",
        "  for key, val in entitiesEvent.items():\n",
        "    #if (foundEvent == 20):\n",
        "      #break;\n",
        "\n",
        "    for line in contentEvents.split(\"\\n\"):\n",
        "      #if (foundEvent == 20):\n",
        "        #break;\n",
        "\n",
        "      if val in line:\n",
        "        words = line.split()\n",
        "        #counterEntitiesInLine = 0\n",
        "        if(len(words) < 15):\n",
        "          dictHintsEntity.update({line + \"///\" + str(foundEvent): val})\n",
        "          #listHints.append(line)\n",
        "          foundEvent += 1\n",
        "          #break;\n",
        "  return dictHintsEntity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZAM4MxB-8By"
      },
      "source": [
        "def findLineInTextBirthDeath(entitiesHuman, year):\n",
        "\n",
        "  page = wikipedia.page(year, auto_suggest=False)\n",
        "  pageText = page.content\n",
        "  splitText = []\n",
        "\n",
        "  splitText = (re.split(\"== Births ==|== Deaths ==\", pageText ))\n",
        "\n",
        "  print(splitText)\n",
        "\n",
        "  contentBirths = splitText[1]\n",
        "  contentDeaths = splitText[2]\n",
        "\n",
        "  foundBirth = 0\n",
        "  foundDeath = 0\n",
        "\n",
        "  #open file and append results\n",
        "  with open('./resultsBirthDeath.txt', 'a') as writefile:\n",
        "    writefile.write(\"---Birth/Death in year \" + str(year) + \"---\")\n",
        "\n",
        "    for entity in entitiesHuman:\n",
        "      #if (foundBirth == 3 and foundDeath == 3):\n",
        "        #writefile.write(\"\\n\")\n",
        "        #break;\n",
        "\n",
        "      for line in contentBirths.split(\"\\n\"):\n",
        "        #if (foundBirth == 3):\n",
        "          #break;\n",
        "\n",
        "        if entity in line:\n",
        "          writefile.write(\"Found with entity \" + entity + \"\\n\")\n",
        "          writefile.write(\"Birth: \" + line + \"\\n\")\n",
        "          writefile.write(\"\\n\")\n",
        "          foundBirth +=1\n",
        "          break;\n",
        "\n",
        "      for line in contentDeaths.split(\"\\n\"):\n",
        "        #if (foundDeath == 3):\n",
        "          #break;\n",
        "        if entity in line:\n",
        "          writefile.write(\"Found with entity \" + entity + \"\\n\")\n",
        "          writefile.write(\"Death: \" + line + \"\\n\")\n",
        "          writefile.write(\"\\n\")\n",
        "          foundDeath += 1\n",
        "          break;\n",
        "    writefile.write(\"\\n\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUy44eeb0EKg"
      },
      "source": [
        "def sortHintsNumberPVSubject(dictHints):\n",
        "\n",
        "  with open('./resultsEvents.txt', 'a') as writefile:\n",
        "\n",
        "    dictFinalHintsPV = {}\n",
        "\n",
        "    for key, val in dictHints.items():\n",
        "      dictFinalHintsPV.update({key: getPageViews(val)})\n",
        "\n",
        "    dictFinalHintsSortedPV = dict(sorted(dictFinalHintsPV.items(), key=lambda item: item[1], reverse=True))\n",
        "    \"\"\"writefile.write(\"\\n--Final Hints sorted #PV---\\n\")\n",
        "    for key, val in dictFinalHintsSortedPV.items():\n",
        "      writefile.write(key + \"(\" + str(val) + \" PageViews)\\n\")\"\"\"\n",
        "\n",
        "  return dictFinalHintsSortedPV\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4YpAjm5B6CM"
      },
      "source": [
        "\n",
        "\n",
        "def sortHintsUtilityScore(question, dictFinalHintsSortedPV, maxPV):\n",
        "  model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "  question_embedding = model.encode(question)\n",
        "\n",
        "  alpha = 0.5\n",
        "\n",
        "  #sentence_embeddings = model.encode(hints)\n",
        "\n",
        "  dictHintsBERT = {}\n",
        "\n",
        "  for key, val in dictFinalHintsSortedPV.items():\n",
        "    sentence_embedding = model.encode(key)\n",
        "    simScore = cosine_similarity([question_embedding], [sentence_embedding]).item()\n",
        "    impScore = alpha * (val/maxPV) + (1-alpha) * simScore\n",
        "    dictHintsBERT.update({key: impScore})\n",
        "\n",
        "  with open('./resultsEvents.txt', 'a') as writefile:\n",
        "\n",
        "    dictHintsBERT = dict(sorted(dictHintsBERT.items(), key=lambda item: item[1], reverse=True))\n",
        "    writefile.write(\"\\n--Final Hints sorted Utility Score---\\n\")\n",
        "    for key, val in dictHintsBERT.items():\n",
        "      writefile.write(key + \"(\" + str(val) + \" Utility Score)\\n\")\n",
        "\n",
        "  return dictHintsBERT\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxuRQ-FsZdQ_"
      },
      "source": [
        "def get_subject_phrase(doc):\n",
        "    for token in doc:\n",
        "        if (\"subj\" in token.dep_):\n",
        "            subtree = list(token.subtree)\n",
        "            start = subtree[0].i\n",
        "            end = subtree[-1].i + 1\n",
        "            return doc[start:end]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBif9Lc8_sg3"
      },
      "source": [
        "def getFinalHints(dictPossibleHints):\n",
        "\n",
        "  #nlp = spacy.load(\"en\")\n",
        "  nlp = spacy.load(\"en_core_web_sm\")\n",
        "  dictFinalHintsSubject = {}\n",
        "\n",
        "  for key, val in dictPossibleHints.items():\n",
        "    doc = nlp(key)\n",
        "    subject = get_subject_phrase(doc)\n",
        "    if (val in str(subject)):\n",
        "      dictFinalHintsSubject.update({key: val})\n",
        "      #listFinalHints.append(key)\n",
        "\n",
        "  return dictFinalHintsSubject"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JUt_0x6jD-x"
      },
      "source": [
        "\n",
        "def getOutlinks2(year):\n",
        "\n",
        "  URL = 'https://en.wikipedia.org/wiki/' + str(year)\n",
        "\n",
        "  # Fetch all the HTML source from the url\n",
        "  response = requests.get(URL)\n",
        "\n",
        "\n",
        "  soup = bs4.BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "  listTextOfLinks = []\n",
        "  listLinks = []\n",
        "\n",
        "  dictLinkName = {}\n",
        "\n",
        "\n",
        "  for link in soup.find_all(\"a\"):\n",
        "    listTextOfLinks.append(link.get_text())\n",
        "    dictLinkName.update({str(link.get(\"href\")).split('/')[-1].replace('_', ' '): link.get_text()})\n",
        "\n",
        "  #newDict = filterDict(dictLinkName)\n",
        "\n",
        "  #print(len(dictLinkName))\n",
        "  #print(len(newDict))\n",
        "  #print(newDict)\n",
        "\n",
        "  return dictLinkName"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAuFgVXomCdf"
      },
      "source": [
        "def filterDict(dictNameLink):\n",
        "\n",
        "  patternYear = r\"[1-3][0-9]{3}\"\n",
        "  patternCentury = r\"[1-2][0-9]\\w+.century\"\n",
        "  patternAd = r\"AD.\\d\\d\\d\\d\"\n",
        "  patternMonth = r\"[a-zA-Z]+\\s\\d+\"\n",
        "  patternMillennium = r\".+millennium\"\n",
        "  patternHashtag = r\"#\"\n",
        "  patternCalendar = r\"calendar\"\n",
        "  patternCategory = r\"category\"\n",
        "  patternList = r\"list\"\n",
        "  patternTemplate = r\"template\"\n",
        "  patternWikipedia = r\"wikipedia\"\n",
        "  patternIndex = r\"index\"\n",
        "  patternHtml = r\"html\"\n",
        "  patternPercent = r\"%\"\n",
        "\n",
        "  newDict = {}\n",
        "\n",
        "  for key, val in dictNameLink.items():\n",
        "    if(not re.match(patternYear, key) and\n",
        "       not re.match(patternCentury, key) and\n",
        "       not re.match(patternAd, key) and\n",
        "       not re.match(patternMonth, key) and\n",
        "       not re.search(patternHashtag, key) and\n",
        "       not re.search(patternCalendar, key, re.IGNORECASE) and\n",
        "       not re.search(patternCategory, key, re.IGNORECASE) and\n",
        "       not re.search(patternList, key, re.IGNORECASE) and\n",
        "       not re.search(patternTemplate, key, re.IGNORECASE) and\n",
        "       not re.search(patternWikipedia, key, re.IGNORECASE) and\n",
        "       not re.search(patternIndex, key, re.IGNORECASE) and\n",
        "       not re.search(patternHtml, key, re.IGNORECASE) and\n",
        "       not re.search(patternPercent, key) and\n",
        "       not re.match(patternMillennium, key) and\n",
        "       not val == \"\" and not val == \" \"):\n",
        "\n",
        "\n",
        "      newDict.update({key: val})\n",
        "\n",
        "\n",
        "  print(newDict)\n",
        "\n",
        "  return newDict\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Call Functions\n"
      ],
      "metadata": {
        "id": "cxaEE5DFfr_v"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bA1ZDHtbAkJJ"
      },
      "source": [
        "**Call hint function for all questions where answer is Location**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kHh8TAEt6OS"
      },
      "source": [
        "for index, row in location_df.iterrows():\n",
        "  location = row[\"Answer\"]\n",
        "  questionLocation = row[\"Question\"]\n",
        "  with open(\"hintsLocation.txt\", 'a') as writefile:\n",
        "    writefile.write(\"\\n\\n\" + questionLocation + \" (\" + location + \")\\n\\n\")\n",
        "    writefile.close()\n",
        "\n",
        "\n",
        "  #get Wikidata ID for SPARQL-Query\n",
        "  locationQitem = getQitemOfName(location)\n",
        "\n",
        "  #execute SPARQL-Query\n",
        "  queryResults_df = allHintsLocation(locationQitem)\n",
        "\n",
        "  #getFinalHints\n",
        "  listHintsLocation = getFormatedHintsLocation(queryResults_df)\n",
        "\n",
        "  #write hints in file in random order\n",
        "  writeLocationHintsInFile(listHintsLocation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYzskrP0AtKs"
      },
      "source": [
        "**Call hint function for all questions where answer is Person**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B__QLqKMAX51"
      },
      "source": [
        "\n",
        "for index, row in person_df.iterrows():\n",
        "  person = row[\"Answer\"]\n",
        "  questionPerson = row[\"Question\"]\n",
        "\n",
        "  with open(\"hintsPerson.txt\", 'a') as writefile:\n",
        "    writefile.write(\"\\n\\n\" + questionPerson + \" (\"+ person +\")\\n\\n\")\n",
        "    writefile.close()\n",
        "\n",
        "  #get Wikipedia ID for SPARQL-Query\n",
        "  personQitem = getQitemOfName(person)\n",
        "\n",
        "  #execute SPARQL-Query\n",
        "  queryResults_df = allHintsPerson(personQitem)\n",
        "\n",
        "  #get Final Hints\n",
        "  listHintsPerson = getFormatedHintsPerson(queryResults_df)\n",
        "\n",
        "  #write hints in file in random order\n",
        "  writePersonHintsInFile(listHintsPerson)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjsplvG5DUJn"
      },
      "source": [
        "**Call hint function for all question where answer is Year**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z5x7mjMDY-b"
      },
      "source": [
        "for index, row in year_df.iterrows():\n",
        "  year = row[\"Answer\"]\n",
        "  questionYear = row[\"Question\"]\n",
        "  hintsYear(row[\"Answer\"])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}